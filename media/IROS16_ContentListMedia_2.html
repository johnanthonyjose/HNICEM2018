<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml2/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
 <head>
  <title>IROS16</title>
  <link href="style.css" rel="stylesheet" type="text/css" media="screen" />


<script language="JavaScript">

function viewAbstract(number){
   var box = document.getElementById('Ab' + number);
   if (box.style.display == 'block'){
      box.style.display = 'none';
   }
   else if (box && box.style.display == 'none'){
      box.style.display = 'block';
   }
}
function openAllAbstracts(){
   var d = document.getElementsByTagName('div');
   var count = d.length;
   if (count == 0){return;}
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab' && d[i].style.display == 'none'){
         d[i].style.display = 'block';
      }
   }
}
function closeAllAbstracts(){
   var d = document.getElementsByTagName('div');
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab'){
         d[i].style.display = 'none';
      }
   }
}
</script>

</head>

<body leftmargin="0" topmargin="0" marginwidth="0" marginheight="0">

<div class="c" id="TheTop">
</div>
<table border="0" cellspacing="0" cellpadding="1" width="85%" nowrap style="margin: auto">
<tr><td>
<h2>Technical Program for Tuesday October 11, 2016</h2>
</td></tr></table>

<p class="c"></p>
<div class="c">

                  <span style="color:gray ">To show or hide the keywords and abstract of a paper (if available), click on the paper title</span><br>
                  <a href="javascript:openAllAbstracts()" title="Click to open all abstracts">Open all abstracts</a>&nbsp;&nbsp;
                  <a href="javascript:closeAllAbstracts()" title="Click to close all abstracts">Close all abstracts</a>
               
</div>

<div class="c">
<table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tupl"><b>TuPL</b></a></td>
               <td class="r">Grand Ballroom</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tupl" title="Click to go to the Program at a Glance"><b>Plenary Talk 1. Manuela M. Veloso: Autonomous Intelligent Service Robots:
<br>Learning and Explanations in Human-Robot Interaction</b></a></td>
               <td class="r">Plenary session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#101162" title="Click to go to the Author Index">Fukuda, Toshio</a></td><td class="r">Meijo Univ</td></tr>

</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuor"><b>TuOR</b></a></td>
               <td class="r">Grand Ballroom</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuor" title="Click to go to the Program at a Glance"><b>Opening Ceremony</b></a></td>
               <td class="r">Opening Ceremony</td>
             </tr>
            


</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuh1"><b>TuH1</b></a></td>
               <td class="r">#111</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuh1" title="Click to go to the Program at a Glance"><b>Highlight 1: Robot Vision/Learning in Robotics</b></a></td>
               <td class="r">Highlight Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#102662" title="Click to go to the Author Index">Amato, Nancy</a></td><td class="r">Texas A&M Univ</td></tr>

<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuh1_01">10:15-10:20, Paper TuH1.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0042.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('42'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Probabilistic Multi-Class Segmentation for the Amazon Picking Challenge</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158597" title="Click to go to the Author Index">Jonschkowski, Rico</a></td><td class="r">Tech. Univ. Berlin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122977" title="Click to go to the Author Index">Eppner, Clemens</a></td><td class="r">Tech. Univ. Berlin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158601" title="Click to go to the Author Index">Höfer, Sebastian</a></td><td class="r">Tech. Univ. Berlin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142950" title="Click to go to the Author Index">Martín-Martín, Roberto</a></td><td class="r">Tech. Univ. Berlin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101767" title="Click to go to the Author Index">Brock, Oliver</a></td><td class="r">Tech. Univ. Berlin</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab42" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0042.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a></span><br>
                           <strong>Abstract:</strong> We present a method for multi-class segmentation from RGB-D data in a realistic warehouse picking setting. The method computes pixel-wise probabilities and combines them to find a coherent object segmentation. It reliably segments objects in cluttered scenarios, even when objects are translucent, reflective, highly deformable, have fuzzy surfaces, or consist of loosely coupled components. The robust performance results from the exploitation of problem structure inherent to the warehouse setting. The proposed method proved its capabilities as part of our winning entry to the 2015 Amazon Picking Challenge. We present a detailed experimental analysis of the contribution of different information sources, compare our method to standard segmentation techniques, and assess possible extensions that further enhance the algorithm's capabilities. We release our software and data sets as open source.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuh1_02">10:20-10:25, Paper TuH1.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1013.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1013'); return false" title="Click to show or hide the keywords and abstract">Robust Material Classification with a Tactile Skin Using Deep Learning</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196419" title="Click to go to the Author Index">Baishya, Shiv Sankar</a></td><td class="r">Deutschen Zentrums Für Luft Und Raumfahrt (DLR)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105517" title="Click to go to the Author Index">Bäuml, Berthold</a></td><td class="r">German Aerospace Center (DLR)</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1013" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Tactile_Sensing" title="Click to go to the Keyword Index">Tactile Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Learning" title="Click to go to the Keyword Index">Robot Learning</a></span><br>
                           <strong>Abstract:</strong> Attaching a flexible tactile skin to an existing robotic system is relatively easy compared to integrating most other tactile sensor designs. In this paper we show that material classification purely based on the spatio-temporal signal of a flexible tactile skin can be robustly performed in a real world setting. We compare different classification algorithms and feature sets, including features adopted and extended from previous works in tactile material classification and that are based on the signal's Fourier spectrum.<p>Our convolutional deep learning network architecture, which we also present here, is directly fed with the raw 24000 dimensional sensor signal and performs best by a large margin, reaching a classification accuracy of up to 97.3%.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuh1_03">10:25-10:30, Paper TuH1.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1053.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1053'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Low-Latency Visual Odometry Using Event-Based Feature Tracks</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196008" title="Click to go to the Author Index">Küng, Beat</a></td><td class="r">Univ. of Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#157228" title="Click to go to the Author Index">Mueggler, Elias</a></td><td class="r">Univ. of Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179731" title="Click to go to the Author Index">Gallego, Guillermo</a></td><td class="r">Univ. of Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105662" title="Click to go to the Author Index">Scaramuzza, Davide</a></td><td class="r">Univ. of Zurich</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1053" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1053.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a></span><br>
                           <strong>Abstract:</strong> New vision sensors, such as the Dynamic and Active-pixel Vision sensor (DAVIS), incorporate a conventional camera and an event-based sensor in the same pixel array. These sensors have great potential for robotics because they allow us to combine the benefits of conventional cameras with those of event-based sensors: low latency, high temporal resolution, and high dynamic range. However, new algorithms are required to exploit the sensor characteristics and cope with its unconventional output, which consists of a stream of asynchronous brightness changes (called “events”) and synchronous grayscale frames. In this paper, we present a low-latency visual odometry algorithm for the DAVIS sensor using event-based feature tracks. Features are first detected in the grayscale frames and then tracked asynchronously using the stream of events. The features are then fed to an event-based visual odometry algorithm that tightly interleaves robust pose optimization and probabilistic mapping. We show that our method successfully tracks the 6-DOF motion of the sensor in natural scenes. This is the first work on event-based visual odometry with the DAVIS sensor using feature tracks.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuh1_04">10:30-10:35, Paper TuH1.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1056.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1056'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Point-To-Hyperplane RGB-D Pose Estimation: Fusing Photometric and Geometric Measurements</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183411" title="Click to go to the Author Index">Ireta Muñoz, Fernando Israel</a></td><td class="r">Univ. De Nice Sophia Antipolis</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106099" title="Click to go to the Author Index">Comport, Andrew Ian</a></td><td class="r">CNRS-I3S/UNS</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1056" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1056.VI.mpg" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Visual_Tracking" title="Click to go to the Keyword Index">Visual Tracking</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Navigation" title="Click to go to the Keyword Index">Visual Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a></span><br>
                           <strong>Abstract:</strong> The objective of this paper is to investigate the problem of how to best combine and fuse color and depth measurements for incremental pose estimation or 3D tracking. Subsequently a framework will be proposed that allows to formulate the problem with a unique measurement vector and not to combine them in an ad-hoc manner. In particular, the full color and depth measurement will be defined as a 4-vector (by combining 3D Euclidean points + image intensities) and an optimal error for pose estimation will be derived from this. As will be shown, this will lead to designing an iterative closest point approach in 4 dimensional space. A kd-tree is used to find the closest point in 4D-space, therefore simultaneously accounting for color and depth. Based on this unified framework a novel Point-to-hyperplane approach will be introduced which has the advantages of classic Point-to-plane ICP but in 4D-space. By doing this it will be shown that there is no longer any need to provide or estimate a scale factor between different measurement types. Consequently, this allows to increase the convergence domain and speed up the alignment, whilst maintaining the robust and accurate properties. Results on both simulated and real environments will be provided along with benchmark comparisons.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuh1_05">10:35-10:40, Paper TuH1.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1563.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1563'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>More Than a Million Ways to Be Pushed. a High-Fidelity Experimental Dataset of Planar Pushing</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#146991" title="Click to go to the Author Index">Yu, Kuan-Ting</a></td><td class="r">MIT</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196487" title="Click to go to the Author Index">Bauza Villalonga, Maria</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191587" title="Click to go to the Author Index">Fazeli, Nima</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105121" title="Click to go to the Author Index">Rodriguez, Alberto</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1563" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1563.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Contact_Modelling" title="Click to go to the Keyword Index">Contact Modelling</a>, <a href="IROS16_KeywordIndexMedia.html#Model_Learning" title="Click to go to the Keyword Index">Model Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Manipulation_Planning_and_Control" title="Click to go to the Keyword Index">Manipulation Planning and Control</a></span><br>
                           <strong>Abstract:</strong> Pushing is a motion primitive useful to handle objects that are too large, too heavy, or too cluttered to be grasped. It is at the core of much of robotic manipulation, in particular when physical interaction is involved. It seems reasonable then to wish for robots to understand how pushed objects move. <p>In reality, however, robots often rely on approximations which yield models that are computable, but also restricted and inaccurate. Just how close are those models? How reasonable are the assumptions they are based on? To help answer these questions, and to get a better experimental understanding of pushing, we present a comprehensive and high-fidelity dataset of planar pushing experiments. The dataset contains timestamped poses of a circular pusher and a pushed object, as well as forces at the interaction. We vary the push interaction in 6 dimensions: surface material, shape of the pushed object, contact position, pushing direction, pushing speed, and pushing acceleration. An industrial robot automates the data capturing along precisely controlled position-velocity-acceleration trajectories of the pusher, which give dense samples of positions and forces of uniform quality. <p>We finish the paper by characterizing the variability of friction, and evaluating the most common assumptions and simplifications made by models of frictional pushing in robotics.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuh2"><b>TuH2</b></a></td>
               <td class="r">#112</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuh2" title="Click to go to the Program at a Glance"><b>Highlight 2: Robot Applications</b></a></td>
               <td class="r">Highlight Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#100084" title="Click to go to the Author Index">Hamel, William R.</a></td><td class="r">Univ. of Tennessee</td></tr>

<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuh2_01">10:15-10:20, Paper TuH2.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0723.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('723'); return false" title="Click to show or hide the keywords and abstract">ANYmal - a Highly Mobile and Dynamic Quadrupedal Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114045" title="Click to go to the Author Index">Hutter, Marco</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158977" title="Click to go to the Author Index">Gehring, Christian</a></td><td class="r">ETH Zurich, Disney Res. Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196206" title="Click to go to the Author Index">Jud, Dominic</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196190" title="Click to go to the Author Index">Lauber, Andreas</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#193489" title="Click to go to the Author Index">Bellicoso, Carmine Dario</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#174373" title="Click to go to the Author Index">Tsounis, Vassilios</a></td><td class="r">National Tech. Univ. of Athens</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#172467" title="Click to go to the Author Index">Hwangbo, Jemin</a></td><td class="r">Swiss Federal Inst. of Tech. Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196195" title="Click to go to the Author Index">Bodie, Karen</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#146641" title="Click to go to the Author Index">Fankhauser, Péter</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#133249" title="Click to go to the Author Index">Bloesch, Michael</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#185810" title="Click to go to the Author Index">Diethelm, Remo</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196199" title="Click to go to the Author Index">Bachmann, Samuel</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179324" title="Click to go to the Author Index">Melzer, Amir</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#133700" title="Click to go to the Author Index">Hoepflinger, Mark</a></td><td class="r">ETH Zurich</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab723" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Legged_Robots" title="Click to go to the Keyword Index">Legged Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Smart_Actuators" title="Click to go to the Keyword Index">Smart Actuators</a></span><br>
                           <strong>Abstract:</strong> This paper introduces ANYmal, a quadrupedal robot that features outstanding mobility and dynamic motion capability. Thanks to novel, compliant joint modules with integrated electronics, the 30kg, 0.5m tall robotic dog is torque controllable and very robust against impulsive loads during running or jumping. The presented machine was designed with a focus on outdoor suitability, simple maintenance, and user-friendly handling to enable future operation in real world scenarios. Performance tests with the joint actuators indicated a torque control bandwidth of more than 70Hz, high disturbance rejection capability, as well as impact robustness when moving with maximal velocity. It is demonstrated in a series of experiments that ANYmal can execute walking gaits, dynamically trot at moderate speed, and is able to perform special maneuvers to stand up or crawl very steep stairs. Detailed measurements unveil that even full-speed running requires less than 280W, resulting in an autonomy of more than 2h.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuh2_02">10:20-10:25, Paper TuH2.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1103.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1103'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Design and Characterization of the EP-Face Connector</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169018" title="Click to go to the Author Index">Tosun, Tarik</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#155626" title="Click to go to the Author Index">Davey, Jay</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196497" title="Click to go to the Author Index">Liu, Chao</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101787" title="Click to go to the Author Index">Yim, Mark</a></td><td class="r">Univ. of Pennsylvania</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1103" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1103.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Cellular_and_Modular_Robots" title="Click to go to the Keyword Index">Cellular and Modular Robots</a></span><br>
                           <strong>Abstract:</strong> We present the EP-Face connector, a novel connector for hybrid chain-lattice type modular robots that is high-strength (88.4N), compact, fast, power efficient, and robust to position errors.<p>The connector consists of an array of electro-permanent magnets (EP magnets) embedded in a planar face. EP magnets are solid-state magnets that can be turned on and off and require power only when changing state.<p>In this paper, we present the design of the connector, manufacturing process, detailed experimental characterization of the connector strength under different loading conditions, and compare its performance to existing magnetic and mechanical connectors. We also illustrate the functional benefits of the EP-Face by demonstrating reconfiguration with the SMORES-EP robot.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuh2_03">10:25-10:30, Paper TuH2.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1283.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1283'); return false" title="Click to show or hide the keywords and abstract">A Palm for a Rock Climbing Robot Based on Dense Arrays of Micro-Spines</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168434" title="Click to go to the Author Index">Wang, Shiquan</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#167579" title="Click to go to the Author Index">Jiang, Hao</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101950" title="Click to go to the Author Index">Cutkosky, Mark</a></td><td class="r">Stanford Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1283" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Climbing_Robots" title="Click to go to the Keyword Index">Climbing Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> We present a new palm design that features a dense array of micro-spines for the JPL Robosimian human-scale climbing robot. A linearly-constrained spine mechanism is introduced and analyzed using adhesion and stiffness models. This mechanism achieves a spine density of 19/cm2 and a mean adhesion of 67N (207kPa) on coarse concrete surfaces. The models are validated on two different surfaces with two sets of experiments. A 120x100mm palm consisting of 12 spine tiles and a pulley differential system for load sharing are designed and tested on 9 different surfaces. The mean shear adhesion goes up to 710N (183kPa) on concrete blocks. Design considerations include scaling efficiency and maximal single-spine force. Desirable properties for load sharing in the palm design are discussed.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuh2_04">10:30-10:35, Paper TuH2.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1560.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1560'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Haptic Skin Stretch on a Steering Wheel for Displaying Preview Information in Autonomous Cars</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196601" title="Click to go to the Author Index">Ploch, Christopher</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#162072" title="Click to go to the Author Index">Bae, Jung Hwa</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101950" title="Click to go to the Author Index">Cutkosky, Mark</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#124149" title="Click to go to the Author Index">Ju, Wendy</a></td><td class="r">Stanford Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1560" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1560.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> Lateral skin stretch is a promising technology for haptic display of information between an autonomous or semi- autonomous car and a driver. We present the design of a steering wheel with an embedded lateral skin stretch display and report on the results of tests conducted in a driving vehicle in suburban traffic. Results are generally consistent with previous results utilizing skin stretch in stationary applications, but a slightly higher, and particularly a faster rate of stretch application is preferred for accurate detection of direction and approximate magnitude.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuh2_05">10:35-10:40, Paper TuH2.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1634.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1634'); return false" title="Click to show or hide the keywords and abstract">Information Gathering Actions Over Human Internal State</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195462" title="Click to go to the Author Index">Sadigh, Dorsa</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104586" title="Click to go to the Author Index">Sastry, Shankar</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#172753" title="Click to go to the Author Index">Seshia, Sanjit A.</a></td><td class="r">Univ. of California Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#137063" title="Click to go to the Author Index">Dragan, Anca</a></td><td class="r">Univ. of California Berkeley</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1634" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Cognitive_Human_Robot_Interaction" title="Click to go to the Keyword Index">Cognitive Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Centered_Robotics" title="Click to go to the Keyword Index">Human Centered Robotics</a></span><br>
                           <strong>Abstract:</strong> Much of estimation of human internal state (goal, intentions, activities, preferences, etc.) is passive: an algorithm observes human actions and updates its estimate of human state. In this work, we embrace the fact that robot actions affect what humans do, and leverage it to improve state estimation. We enable robots to do active information gathering, by planing actions that probe the user in order to clarify their internal state. For instance, an autonomous car will plan to nudge into a human driver's lane to test their driving style. Results in simulation and in a user study suggest that active information gathering significantly outperforms passive state estimation.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tut11"><b>TuT11</b></a></td>
               <td class="r">#111</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tut11" title="Click to go to the Program at a Glance"><b>Sensing</b></a></td>
               <td class="r">Teaser Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#102922" title="Click to go to the Author Index">Asfour, Tamim</a></td><td class="r">Karlsruhe Inst. of Tech. (KIT)</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#102001" title="Click to go to the Author Index">Merlet, Jean-Pierre</a></td><td class="r">INRIA</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_01">10:45-10:46, Paper TuT11.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0132.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('132'); return false" title="Click to show or hide the keywords and abstract">Heuristic 3D Object Shape Completion Based on Symmetry and Scene Context</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#132279" title="Click to go to the Author Index">Schiebener, David</a></td><td class="r">Karlsruhe Inst. of Tech. (KIT)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#189955" title="Click to go to the Author Index">Schmidt, Andreas</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110388" title="Click to go to the Author Index">Vahrenkamp, Nikolaus</a></td><td class="r">Karlsruhe Inst. of Tech. (KIT)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102922" title="Click to go to the Author Index">Asfour, Tamim</a></td><td class="r">Karlsruhe Inst. of Tech. (KIT)</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab132" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a></span><br>
                           <strong>Abstract:</strong> Object shape information is essential for robot manipulation tasks, in particular for grasp planning and collision-free motion planning. But in general a complete object model is not available, in particular when dealing with unknown objects. We propose a method for completing shapes that are only partially known, which is a common situation when a robot perceives a new object only from one direction.<p>Our approach is based on the assumption that most objects used in service robotic setups have symmetries. We determine and rate symmetry plane candidates to estimate the hidden parts of the object. By finding possible supporting planes based on its immediate neighborhood, the search space for symmetry planes is restricted, and the bottom part of the object is added. Gaps along the sides in the direction of the view axis are closed by linear interpolation. We evaluate our approach with real-world experiments using the YCB object and model set.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_02">10:46-10:47, Paper TuT11.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0190.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('190'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>3D Contour Following for a Cylindrical End-Effector Using Capacitive Proximity Sensors</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#133244" title="Click to go to the Author Index">Escaida Navarro, Stefan</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179755" title="Click to go to the Author Index">Koch, Stefan</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103218" title="Click to go to the Author Index">Hein, Björn</a></td><td class="r">Karlsruhe Inst. of Tech. (KIT)</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab190" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0190.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Collision_Detection_and_Avoidance" title="Click to go to the Keyword Index">Collision Detection and Avoidance</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a></span><br>
                           <strong>Abstract:</strong> In this paper we've equipped a cylindrical end-effector with an array of capacitive sensors in order to implement 3D contour following. During the task, using proximity servoing, the sensors are aligned parallel to the surface and kept at a target distance. In addition, due to the spatial resolution, it's possible to estimate the surface's curvature in two dimensions along the rows and columns of the array. We show how a compound movement can be derived from both curvatures that pre-aligns the end-effector in each step, yielding a predictive component for the control scheme. We evaluate our approach with different geometries and show that the curvature information produces smooth contour following paths. We show that the system can handle speeds up to 150 mm/s.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_03">10:47-10:48, Paper TuT11.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0219.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('219'); return false" title="Click to show or hide the keywords and abstract">Pose Estimation of a Rigid Body and Its Supporting Moving Platform Using Two Gyroscopes and Relative Complementary Measurements</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#135120" title="Click to go to the Author Index">Zhang, Yizhai</a></td><td class="r">Northwestern Pol. Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191464" title="Click to go to the Author Index">Song, Kehao</a></td><td class="r">Northwestern Pol. Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102943" title="Click to go to the Author Index">Yi, Jingang</a></td><td class="r">Rutgers Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#130523" title="Click to go to the Author Index">Duan, Zhansheng</a></td><td class="r">Xi'an Jiaotong Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191452" title="Click to go to the Author Index">Pan, Quan</a></td><td class="r">Northwestern Pol. Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102722" title="Click to go to the Author Index">Huang, Panfeng</a></td><td class="r">Northwestern Pol. Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab219" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a>, <a href="IROS16_KeywordIndexMedia.html#Wearable_Robots" title="Click to go to the Keyword Index">Wearable Robots</a></span><br>
                           <strong>Abstract:</strong> We present a drift-free pose estimation scheme for rigid body and its supporting platform by fusing only two gyroscopes and the relative complementary measurements. The fusion design not only provides robust relative attitude estimation between the rigid body and the platform, but also is capable of identifying partial global absolute attitudes without capturing any absolute attitude information. The pose estimation is built on a special design of the coupled kinematic model with the relative measurements between the rigid body and its supporting platform. We compare the fusion design with an alternative kinematic model and the posterior Cramer-Rao bound analyses are presented to show the completely different estimation performances. An extended Kalman filter (EKF) implementation of the fusion design is presented for the bicycle riding applicatio through experiments.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_04">10:48-10:49, Paper TuT11.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0256.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('256'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Sparse Sensing for Resource-Constrained Depth Reconstruction</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195580" title="Click to go to the Author Index">Ma, Fangchang</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#119426" title="Click to go to the Author Index">Carlone, Luca</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195582" title="Click to go to the Author Index">Ayaz, Ulas</a></td><td class="r">MIT</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#124153" title="Click to go to the Author Index">Karaman, Sertac</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab256" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0256.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a>, <a href="IROS16_KeywordIndexMedia.html#SLAM" title="Click to go to the Keyword Index">SLAM</a></span><br>
                           <strong>Abstract:</strong> We address the following question: is it possible to reconstruct the geometry of an unknown environment using sparse and incomplete depth measurements? This problem is relevant for a resource-constrained robot that has to navigate and map an environment, but does not have enough on-board power or payload to carry a traditional depth sensor (e.g., a 3D lidar) and can only acquire few (point-wise) depth measurements. In general, reconstruction from incomplete data is not possible, but when the robot operates in man-made environments, the depth exhibits some regularity (e.g., many planar surfaces with few edges); we leverage this regularity to infer depth from incomplete measurements. Our formulation bridges robotic perception with the compressive sensing literature in signal processing. We exploit this connection to provide formal results on exact depth recovery in 2D and 3D problems. Taking advantage of our specific sensing modality, we also prove novel and more powerful results to completely characterize the geometry of the signals that we can reconstruct. Our results directly translate to practical algorithms for depth reconstruction; these algorithms are simple (they reduce to solving a linear program), and robust to noise. We test our algorithms on real and simulated data, and show that they enable accurate depth reconstruction from a handful of measurements, and perform well even when the assumption of structured environment is violated.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_05">10:49-10:50, Paper TuT11.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0267.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('267'); return false" title="Click to show or hide the keywords and abstract">Adaptive Patrolling by Mobile Robot for Changing Visitor Trends</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102021" title="Click to go to the Author Index">Hoshino, Satoshi</a></td><td class="r">Utsunomiya Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#182126" title="Click to go to the Author Index">Ugajin, Shingo</a></td><td class="r">Utsunomiya Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab267" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Surveillance_Systems" title="Click to go to the Keyword Index">Surveillance Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Service_Robots" title="Click to go to the Keyword Index">Service Robots</a></span><br>
                           <strong>Abstract:</strong> In this paper, a single mobile robot is used to patrol an indoor environment composed of rooms. The robot is required to monitor the rooms and detect visitors as many as possible. A key idea in the patrolling mission is to allow the robot to utilize information about visitor trends. Thus far, we have focused on a visiting probability as the trend. However, the visitor trends usually change over time. Therefore, the robot is further required to detect the changes and identify the correct visitor trends in each room. For this challenge, we present a novel change-point detection method. In this method, the Jensen-Shannon divergence is used as a dissimilarity measure between two distributions of the visiting probability. The change-point detection method is activated in the Bayesian learning for estimating the changed visiting probability. Consequently, the robot is allowed to adaptively patrol the changing environment. Through the simulation experiments, we discuss the effectiveness of the adaptive patrolling for the changes in the visitor trends.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_06">10:50-10:51, Paper TuT11.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0328.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('328'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Development of a Low-Cost Ultra-Tiny Line Laser Range Sensor</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#182682" title="Click to go to the Author Index">Chen, Xiangyu</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#164284" title="Click to go to the Author Index">Zhao, Moju</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183349" title="Click to go to the Author Index">Xiang, Lingzhu</a></td><td class="r">Univ. of Toronto</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#149938" title="Click to go to the Author Index">Sugai, Fumihito</a></td><td class="r">Tokyo Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117269" title="Click to go to the Author Index">Yaguchi, Hiroaki</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106350" title="Click to go to the Author Index">Okada, Kei</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106348" title="Click to go to the Author Index">Inaba, Masayuki</a></td><td class="r">The Univ. of Tokyo</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab328" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0328.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Calibration_and_Identification" title="Click to go to the Keyword Index">Calibration and Identification</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a></span><br>
                           <strong>Abstract:</strong> Abstract— To enable robotic sensing for tasks with requirements on weight, size, and cost, we develop an ultra-tiny line laser range sensor based on the Time-of-Flight (TOF) principle. With delicate circuit design and optical attachments, we create a sensor as small as 35[mm] × 27[mm] × 30[mm] and as light as 20[g]. The line sensor samples 272 pixels (256 effective pixels) uniformly distributed within the measurement field of view customizable using different laser lenses. The optimal measurement range of the sensor is 0.05[m] &#8764; 2[m]. Higher sampling rates can be achieved with a shorter range. The sensor can also extend its range to 3[m] with reduced accuracy. We model the overall errors of the sensor and formulate calibration methods, achieving repeatable accuracy and measurement bias both within 2[cm] with our tested ambient lighting conditions and measurement ranges. The sensor is applicable to range sensing tasks including humanoid hand-eye measurement, UAV safe landing, tiny robot range sensing, and object detection.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_07">10:51-10:52, Paper TuT11.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0740.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('740'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Precise and Efficient Model-Based Vehicle Tracking Method Using Rao-Blackwellized and Scaling Series Particle Filters</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#141630" title="Click to go to the Author Index">He, Mengwen</a></td><td class="r">Nagoya Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110550" title="Click to go to the Author Index">Takeuchi, Eijiro</a></td><td class="r">Nagoya Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#115476" title="Click to go to the Author Index">Ninomiya, Yoshiki</a></td><td class="r">Toyota Central R & D Labs., Inc</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184739" title="Click to go to the Author Index">Kato, Shinpei</a></td><td class="r">Nagoya Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab740" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0740.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Intelligent_Transportation_Systems" title="Click to go to the Keyword Index">Intelligent Transportation Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Tracking" title="Click to go to the Keyword Index">Visual Tracking</a></span><br>
                           <strong>Abstract:</strong> A precise and efficient tracking technique is essential for an intelligent vehicle to interact with the surrounding vehicles on the road. In this paper, we propose a model-based vehicle tracking method using the Rao-Blackwellized particle filter (RBPF) and scaling series particle filter (SSPF).<p>To ensure that the accuracy and speed of the proposed method are better than those standard particle-based tracking methods, we introduce the latest sensor measurement into the motion estimate. A dynamic Bayesian network and its theoretical base are derived to support this improvement. To track the high-dimensional states of the target vehicle including its position, orientation, motion, and geometry in the proposed general tracking method, we use the SSPF for the motion estimate and geometry fitting and the RBPF for the geometry estimate.<p>To demonstrate the effectiveness of our tracking method, we conduct an evaluation experiment using two intelligent vehicles around Nagoya University, Japan. The total distance traveled by the vehicles in this experiment is approximately 10 km, and the experimental environment contains flat roads, narrow streets, and steep ramps. To demonstrate the accuracy and speed of our tracking method, we extract the ground-truth from our intelligent vehicles and compare its performance with that of state-of-art RBPF-based approach.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_08">10:52-10:53, Paper TuT11.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0763.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('763'); return false" title="Click to show or hide the keywords and abstract">Unifying Consensus and Covariance Intersection for Decentralized State Estimation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#116913" title="Click to go to the Author Index">Tamjidi, Amir Hossein</a></td><td class="r">Texas A&M Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100973" title="Click to go to the Author Index">Chakravorty, Suman</a></td><td class="r">Texas A&M Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111653" title="Click to go to the Author Index">Shell, Dylan</a></td><td class="r">Texas A&M Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab763" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Sensor_Networks" title="Click to go to the Keyword Index">Sensor Networks</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a>, <a href="IROS16_KeywordIndexMedia.html#Networked_Robots" title="Click to go to the Keyword Index">Networked Robots</a></span><br>
                           <strong>Abstract:</strong> This paper presents a new recursive information consensus filter for decentralized dynamic-state estimation. No structure is assumed about the topology of the network and local estimators are assumed to have access only to local information. The network need not be connected at all times. Consensus over priors which might become correlated is performed through Iterative Covariance Intersection (ICI) and consensus over new information is handled using weights based on a Metropolis Hastings Markov Chain. We establish bounds for estimation performance and show that our method produces unbiased conservative estimates that are better than CI. The performance of the proposed method is evaluated and compared with competing algorithms on an atmospheric dispersion problem.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_09">10:53-10:54, Paper TuT11.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0840.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('840'); return false" title="Click to show or hide the keywords and abstract">Towards Occupational Health Improvement in Foundries through Dense Dust and Pollution Monitoring Using a Complementary Approach with Mobile and Stationary Sensing Nodes</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158887" title="Click to go to the Author Index">Hernandez Bennetts, Victor Manuel</a></td><td class="r">Örebro Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#166253" title="Click to go to the Author Index">Kucner, Tomasz</a></td><td class="r">Örebro Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#116069" title="Click to go to the Author Index">Schaffernicht, Erik</a></td><td class="r">Örebro Univ. AASS Res. Center</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191443" title="Click to go to the Author Index">Fan, Han</a></td><td class="r">Örebro Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104036" title="Click to go to the Author Index">Lilienthal, Achim J.</a></td><td class="r">Örebro Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191465" title="Click to go to the Author Index">Andersson, Lena</a></td><td class="r">Örebro Univ. Hospital</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191492" title="Click to go to the Author Index">Johansson, Anders</a></td><td class="r">Örebro Univ. Hospital</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab840" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Service_Robots" title="Click to go to the Keyword Index">Service Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Environment_Monitoring_and_Management" title="Click to go to the Keyword Index">Environment Monitoring and Management</a>, <a href="IROS16_KeywordIndexMedia.html#Surveillance_Systems" title="Click to go to the Keyword Index">Surveillance Systems</a></span><br>
                           <strong>Abstract:</strong> In industrial environments, such as metallurgic facilities, human operators are exposed to harsh conditions where ambient air is often polluted with quartz, dust, lead debris and toxic fumes. Constant exposure to respirable particles can cause irreversible health damages and thus it is of high interest for occupational health experts to monitor the air quality on a regular basis. However, current monitoring procedures are carried out sparsely, with data collected in single day campaigns limited to few measurement locations. In this paper we explore the use and present first experimental results of a novel heterogeneous approach that uses a mobile robot and a network of low cost sensing nodes. The proposed system aims to address the spatial and temporal limitations of current monitoring techniques. The mobile robot, along with standard localization and mapping algorithms, allows to produce short term, spatially dense representations of the environment where dust, gas, ambient temperature and airflow information can be modelled. The sensing nodes on the other hand, can collect temporally dense (and usually spatially sparse) information during long periods of time, allowing in this way to register for example, daily variations in the pollution levels. Using data collected with the proposed system in an steel foundry, we show that a heterogeneous approach provides dense spatio-temporal information that can be used to improve the working conditions in industrial facilities.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_10">10:54-10:55, Paper TuT11.10</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0857.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('857'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Scene Recognition for Mobile Robots by Relational Object Search Using Next-Best-View Estimates from Hierarchical Implicit Shape Models</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142094" title="Click to go to the Author Index">Meißner, Pascal</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191483" title="Click to go to the Author Index">Schleicher, Ralf</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191490" title="Click to go to the Author Index">Hutmacher, Robin</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#109666" title="Click to go to the Author Index">Schmidt-Rohr, Sven R.</a></td><td class="r">Karlsruhe Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101397" title="Click to go to the Author Index">Dillmann, Rüdiger</a></td><td class="r">Karlsruhe Inst. of Tech. (KIT)</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab857" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0857.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Learning" title="Click to go to the Keyword Index">Visual Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a></span><br>
                           <strong>Abstract:</strong> We present an approach for recognizing indoor scenes in object constellations that require object search by a mobile robot, as they cannot be captured from a single viewpoint. In our approach that we call Active Scene Recognition (ASR), robots predict object poses from learnt spatial relations that they combine with their estimates about present scenes. Our models for estimating scenes and predicting poses are Implicit Shape Model (ISM) trees from prior work. ISMs model scenes as sets of objects with spatial relations in-between and are learnt from observations. In prior work, we presented a realization of ASR, limited to choosing orientations for a fixed robot head with an approach to search objects that uses positions and ignores types. In this paper, we introduce an integrated system that extends ASR to selecting positions and orientations of camera views for a mobile robot with a pivoting head. We contribute an approach for Next-Best-View estimation in object search on predicted object poses. It is defined on 6 DoF viewing frustums and optimizes the searched view, together with the objects to be searched in it, based on 6 DoF pose predictions. To prevent combinatorial explosion when searching camera pose space, we introduce a hierarchical approach to sample robot positions with increasing resolution.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_11">10:55-10:56, Paper TuT11.11</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0916.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('916'); return false" title="Click to show or hide the keywords and abstract">Environment-Aware Proximity Detection with Capacitive Sensors for Human-Robot-Interaction</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118803" title="Click to go to the Author Index">Hoffmann, Alwin</a></td><td class="r">Univ. of Augsburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195343" title="Click to go to the Author Index">Poeppel, Alexander</a></td><td class="r">Univ. of Augsburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#131662" title="Click to go to the Author Index">Schierl, Andreas</a></td><td class="r">Univ. of Augsburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#123791" title="Click to go to the Author Index">Reif, Wolfgang</a></td><td class="r">Univ. of Augsburg</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab916" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Collision_Detection_and_Avoidance" title="Click to go to the Keyword Index">Collision Detection and Avoidance</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a></span><br>
                           <strong>Abstract:</strong> Recently, the need for safe human-robot-interaction has become increasingly important, and with it the requirement to reliably detect persons in the workspace of a robot. Capacitive sensors mounted to the robot structure can be used to measure the presence of conductive objects and, hence, allow the detection of persons. However, various objects in the workspace can influence capacitive sensor measurements. Thus, we propose to record an environment model containing the expected sensor values for relevant robot poses. Using this model, distance estimation and real-time reaction can be performed even in the presence of additional conductive objects in the workspace. A demonstration of our approach was shown successfully at the Hannover Messe 2015.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_12">10:56-10:57, Paper TuT11.12</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1014.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1014'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in Changing Environments</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191594" title="Click to go to the Author Index">Mees, Oier</a></td><td class="r">Albert-Ludwigs-Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#177071" title="Click to go to the Author Index">Eitel, Andreas</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101785" title="Click to go to the Author Index">Burgard, Wolfram</a></td><td class="r">Univ. of Freiburg</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1014" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1014.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Detection_and_Tracking" title="Click to go to the Keyword Index">Human Detection and Tracking</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a></span><br>
                           <strong>Abstract:</strong> Object detection is an essential task for autonomous robots operating in dynamic and changing environments. A robot should be able to detect objects in the presence of sensor noise that can be induced by changing lighting conditions for cameras and false depth readings for range sensors, specially RGB-D cameras. To tackle these challenges, we propose a novel adaptive fusion approach for object detection that learns weighting the predictions of different sensor modalities in an online manner. Our approach is based on a mixture of convolutional neural network (CNN) experts and incorporates multiple modalities including appearance, depth and motion. We test our method in extensive robot experiments, in which we detect people in a combined indoor and outdoor scenario from RGB-D data, where we show that our method can adapt during harsh dark to bright lighting changes and severe camera motion blur. Furthermore, we present a new RGB-D dataset for people detection in mixed in- and outdoor environments, recorded from a mobile robot.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_13">10:57-10:58, Paper TuT11.13</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1055.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1055'); return false" title="Click to show or hide the keywords and abstract">Non-Field-Of-View Sound Source Localization Using Diffraction and Reflection Signals</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#161837" title="Click to go to the Author Index">Takami, Kuya</a></td><td class="r">Virginia Pol. Inst. and State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196468" title="Click to go to the Author Index">Liu, Hangxin</a></td><td class="r">Virginia Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107671" title="Click to go to the Author Index">Furukawa, Tomonari</a></td><td class="r">Virginia Pol. Inst. and State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110645" title="Click to go to the Author Index">Kumon, Makoto</a></td><td class="r">Graduate School of Science and Tech. Kumamoto</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106485" title="Click to go to the Author Index">Dissanayake, Gamini</a></td><td class="r">Univ. of Tech. Sydney</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1055" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a></span><br>
                           <strong>Abstract:</strong> This paper describes a non-field-of-view (NFOV) localization approach for a mobile robot in an unknown environment based on an acoustic signal combined with the geometrical information from an optical sensor. The approach estimates the location of a target through the mobile robot's sensor observation frame, which consists of a combination of diffraction and reflection acoustic signals and a 3-D environment geometrical description. This fusion of audio-visual sensor observation likelihoods allows the robot to estimate the NFOV target. The diffraction and reflection observations from the microphone array generate the acoustic joint observation likelihood. The observed geometry also determines far-field or near-field acoustic conditions to improve the estimation of the sound direction of arrival. A mobile robot equipped with a microphone array and an RGB-D sensor was tested in a controlled environment, an anechoic chamber, to demonstrate the NFOV localization capabilities. This resulted in +/- 18 degrees, and less than 0.75 m error in angle and distance estimation, respectively.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_14">10:58-10:59, Paper TuT11.14</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1098.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1098'); return false" title="Click to show or hide the keywords and abstract">Fast Range Image-Based Segmentation of Sparse 3D Laser Scans for Online Operation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#165809" title="Click to go to the Author Index">Bogoslavskyi, Igor</a></td><td class="r">Univ. of Bonn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101642" title="Click to go to the Author Index">Stachniss, Cyrill</a></td><td class="r">Univ. of Bonn</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1098" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a></span><br>
                           <strong>Abstract:</strong> Object segmentation from 3D range data is an important topic in mobile robotics. A robot navigating in a dynamic environment needs to be aware of objects that might change or move. A segmentation of the laser scans into individual objects is typically the first processing step before a further analysis is performed. In this paper, we present a fast method that segments 3D range data into different objects, runs online, and has small computational demands. Our approach avoids the explicit computation of the 3D point cloud and performs all computations directly on a 2D range image, which enables a fast segmentation for each scan. A further relevant aspect of our method is that we can segment objects even if the 3D data is sparse. This is important for scanners such as the new Velodyne Puck. We implemented our approach in C++ and ROS and thoroughly tested it using different 3D scanners. Our method can operate at over 100,Hz for the 64-beam Velodyne scanner on a single core of a mobile CPU while producing high quality segmentation results. In addition to this, we make the source code for the approach available.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_15">10:59-11:00, Paper TuT11.15</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1188.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1188'); return false" title="Click to show or hide the keywords and abstract">Fingertip Proximity Sensor with Realtime Visual-Based Calibration</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#151502" title="Click to go to the Author Index">Konstantinova, Jelizaveta</a></td><td class="r">King's Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#172565" title="Click to go to the Author Index">Stilli, Agostino</a></td><td class="r">King's Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#159678" title="Click to go to the Author Index">Faragasso, Angela</a></td><td class="r">King's Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101975" title="Click to go to the Author Index">Althoefer, Kaspar</a></td><td class="r">King's Coll. London</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1188" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a></span><br>
                           <strong>Abstract:</strong> Proximity and distance estimation sensors are broadly used in robotic hands to enhance the quality of grasping during grasp planning, grasp correction and in-hand manipulation. This paper presents a fiber optical proximity sensor that is integrated with a tactile sensing fingertip of a robotic hand of a mobile robot. The distance estimation of proximity sensors are typically influenced by the reflective properties of an object, such as color or surface roughness. With the approach proposed in this paper, the accuracy of the proximity sensor is enhanced using the information collected by the vision system of the robot. A camera is employed to obtain RGB values of the object to be grasped. Further on, the data obtained from the camera is used to obtain the correct calibration for the proximity sensor. Based on the experimental evidence, it is shown that our approach can be effectively used to reduce the distance estimation error.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_16">11:00-11:01, Paper TuT11.16</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1195.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1195'); return false" title="Click to show or hide the keywords and abstract">Estimating Perturbations from Experience Using Neural Networks and Information Transfer</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158774" title="Click to go to the Author Index">Berger, Erik</a></td><td class="r">TU Bergakademie Freiberg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158775" title="Click to go to the Author Index">Vogt, David</a></td><td class="r">TU Bergakademie Freiberg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192209" title="Click to go to the Author Index">Grehl, Steve</a></td><td class="r">TU Bergakademie Freiberg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#130055" title="Click to go to the Author Index">Jung, Bernhard</a></td><td class="r">TU Bergakademie Freiberg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#130054" title="Click to go to the Author Index">Ben Amor, Heni</a></td><td class="r">Arizona State Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1195" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Learning_from_Demonstration" title="Click to go to the Keyword Index">Learning from Demonstration</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Force_and_Tactile_Sensing" title="Click to go to the Keyword Index">Force and Tactile Sensing</a></span><br>
                           <strong>Abstract:</strong> In order to ensure safe operation, robots must be able to reliably detect behavior perturbations that result from unexpected physical interactions with their environment and human co-workers. While some robots provide firmware force sensors that generate rough force estimates, more accurate force measurements are usually achieved with dedicated force-torque sensors. However, such sensors are often heavy, expensive and require an additional power supply. In the case of light-weight manipulators, the already limited payload capabilities may be reduced in a significant way. This paper presents an experience-based approach for accurately estimating external forces being applied to a robot without the need for a force-torque sensor. Using Information Transfer, a subset of sensors relevant to the executed behavior are identified from a larger set of internal sensors. Models mapping robot sensor data to force-torque measurements are learned using a neural network. These models can be used to predict the magnitude and direction of perturbations from affordable, proprioceptive sensors only. Experiments with a UR5 robot show that our method yields force estimates with accuracy comparable to a dedicated force-torque sensor. Moreover, our method yields a substantial improvement in accuracy over force-torque values provided by the robot firmware.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_17">11:01-11:02, Paper TuT11.17</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1225.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1225'); return false" title="Click to show or hide the keywords and abstract">Structure-Based Vision-Laser Matching</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191825" title="Click to go to the Author Index">Gawel, Abel Roman</a></td><td class="r">Autonomous Systems Lab, ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178462" title="Click to go to the Author Index">Cieslewski, Titus</a></td><td class="r">Univ. of Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192683" title="Click to go to the Author Index">Dubé, Renaud</a></td><td class="r">ETH Zürich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101748" title="Click to go to the Author Index">Bosse, Michael</a></td><td class="r">ETH Zürich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100088" title="Click to go to the Author Index">Siegwart, Roland</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101699" title="Click to go to the Author Index">Nieto, Juan</a></td><td class="r">ETH Zürich</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1225" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a></span><br>
                           <strong>Abstract:</strong> Persistent merging of maps created by different sensor modalities is an insufficiently addressed problem. Current approaches either rely on appearance-based features which may suffer from lighting and viewpoint changes or require pre-registration between all sensor modalities used. This work presents a framework using structural descriptors for matching LIDAR point-cloud maps and sparse vision keypoint maps. The matching algorithm works independently of the sensors’ viewpoint and varying lighting and does not require pre-registration between the sensors used. Furthermore, we employ the approach in a novel vision-laser map-merging algorithm. We analyse a range of structural descriptors and present results of the method integrated within a full mapping framework. Despite the fact that we match between the visual and laser domains, we can successfully perform map-merging using structural descriptors. The effectiveness of the presented structure-based vision-laser matching is evaluated on the public KITTI dataset and furthermore demonstrated on a map merging problem in an industrial site.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_18">11:02-11:03, Paper TuT11.18</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1258.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1258'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Omni-Directional Person Tracking on a Flying Robot Using Occlusion-Robust Ultra-Wideband Signals</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196278" title="Click to go to the Author Index">Hepp, Benjamin</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171324" title="Click to go to the Author Index">Naegeli, Tobias</a></td><td class="r">ETH</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#170316" title="Click to go to the Author Index">Hilliges, Otmar</a></td><td class="r">ETH Zurich</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1258" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1258.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Detection_and_Tracking" title="Click to go to the Keyword Index">Human Detection and Tracking</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> We present a tracking system based on ultra-wideband (UWB) radio tranceivers mounted on a robot and a target. In comparison to typical UWB localization systems with fixed UWB tranceivers in the environment we only require instrumentation of the target with a single UWB tranceiver. Our system works in GPS-denied environments and does not suffer from long-term drift and limited fields of view.<p>This paper reports the localization algorithm and implementation details. Additionally, we demonstrate a quantitative evaluation of the accuracy (10 text{cm} average position error for a square with side-length of 4 text{m}) and application scenarios with a quadrotor flying in close proximity to a person and handling occlusion of the target.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_19">11:03-11:04, Paper TuT11.19</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1278.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1278'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Contact Localization through Spatially Overlapping Piezoresistive Signals</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#187281" title="Click to go to the Author Index">Piacenza, Pedro</a></td><td class="r">Columbia Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192694" title="Click to go to the Author Index">Xiao, Yuchen</a></td><td class="r">Columbia Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195909" title="Click to go to the Author Index">Park, Steve</a></td><td class="r">Korea Advanced Inst. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195865" title="Click to go to the Author Index">Kymissis, Ioannis</a></td><td class="r">Columbia Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110328" title="Click to go to the Author Index">Ciocarlie, Matei</a></td><td class="r">Columbia Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1278" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1278.VI.mpg" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Tactile_Sensing" title="Click to go to the Keyword Index">Tactile Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Dexterous_Manipulation" title="Click to go to the Keyword Index">Dexterous Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a></span><br>
                           <strong>Abstract:</strong> Achieving high spatial resolution in contact sensing for robotic manipulation often comes at the price of increased complexity in fabrication and integration. One traditional approach is to fabricate a large number of taxels, each delivering an individual, isolated response to a stimulus. In contrast, we propose a method where the sensor simply consists of a continuous volume of piezoresistive elastomer with a number of electrodes embedded inside. We measure piezoresistive effects between all pairs of electrodes in the set, and count on this rich signal set containing the information needed to pinpoint contact location with high accuracy using regression algorithms. In our validation experiments, we demonstrate submillimeter median accuracy in locating contact on a 10mm by 16mm sensor using only four electrodes (creating six unique pairs). In addition to extracting more information from fewer wires, this approach lends itself to simple fabrication methods and makes no assumptions about the underlying geometry, simplifying future integration on robot fingers.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_20">11:04-11:05, Paper TuT11.20</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1308.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1308'); return false" title="Click to show or hide the keywords and abstract">A New Miniaturised Multi-Axis Force/Torque Sensors Based on Optoelectronic Technology and Simply-Supported Beam</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105797" title="Click to go to the Author Index">Noh, Yohan</a></td><td class="r">King's Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#147556" title="Click to go to the Author Index">Bimbo, Joao</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#172565" title="Click to go to the Author Index">Stilli, Agostino</a></td><td class="r">King's Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142097" title="Click to go to the Author Index">Wurdemann, Helge Arne</a></td><td class="r">Univ. Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104575" title="Click to go to the Author Index">Liu, Hongbin</a></td><td class="r">Department of Informatics</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#193114" title="Click to go to the Author Index">Housden, Richard James</a></td><td class="r">King's Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178727" title="Click to go to the Author Index">Rhode, Kawal</a></td><td class="r">King's Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101975" title="Click to go to the Author Index">Althoefer, Kaspar</a></td><td class="r">King's Coll. London</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1308" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Force_and_Tactile_Sensing" title="Click to go to the Keyword Index">Force and Tactile Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Tactile_Sensing" title="Click to go to the Keyword Index">Tactile Sensing</a></span><br>
                           <strong>Abstract:</strong> This paper presents a methodology for the development of a multi-axis force/torque sensor based on optoelectronic technology. The advantages of using this sensing principle are the low manufacturing costs, the simple fabrication, and the immunity to electrical noise. The force/ torque sensor makes use of six optical sensors: each sensor measures the displacement of a reflective surface that moves integrally with a simply-supported beam. The proposed mechanical structure allows for a variety of shapes on the mechanical structure to be easily adaptable to many robot applications. In this paper, we present a five-axis force/torque sensor based on this optoelectronic principle. To measure force/torque components, two identical three-DoF force/torque sensor structures (comprised of three beams) are mounted on top of each other. Photo sensors and mirrors are fixed inside the structure to measure the six beam deflections. In this paper, we describe the sensor structure, design, fabrication, calibration, and verify our sensor development methodology.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_21">11:05-11:06, Paper TuT11.21</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1542.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1542'); return false" title="Click to show or hide the keywords and abstract">Measurement Object Hardness with a GelSight Touch Sensor</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#172670" title="Click to go to the Author Index">Yuan, Wenzhen</a></td><td class="r">MIT</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#124784" title="Click to go to the Author Index">Srinivasan, Mandayam</a></td><td class="r">MIT</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#162234" title="Click to go to the Author Index">Adelson, Edward</a></td><td class="r">MIT</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1542" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Tactile_Sensing" title="Click to go to the Keyword Index">Tactile Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Contact_Modelling" title="Click to go to the Keyword Index">Contact Modelling</a></span><br>
                           <strong>Abstract:</strong> Hardness sensing is a valuable capability for a robot touch sensor. We describe a novel method of hardness sensing that does not require accurate control of contact conditions. A GelSight sensor is a tactile sensor that provides high resolution tactile images, which enables a robot to infer object properties such as geometry and fine texture, as well as contact force and slip conditions. 	The sensor is pressed on silicone samples by a human or a robot and we measure the sample hardness only with data from the sensor, without a separate force sensor and without precise knowledge of the contact trajectory. We describe the features that show object hardness. For hemispherical objects, we develop a model to measure the sample hardness, and the estimation error is about 4% in the range of 8 Shore 00 to 45 Shore A. With this technology, a robot is able to more easily infer the hardness of the touched objects, thereby improving its object recognition as well as manipulation strategy.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_22">11:06-11:07, Paper TuT11.22</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1578.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1578'); return false" title="Click to show or hide the keywords and abstract">Contact Localization on Grasped Objects Using Tactile Sensing</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179495" title="Click to go to the Author Index">Molchanov, Artem</a></td><td class="r">Univ. of Southern California</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#124019" title="Click to go to the Author Index">Kroemer, Oliver</a></td><td class="r">Univ. of Southern California</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140358" title="Click to go to the Author Index">Su, Zhe</a></td><td class="r">Univ. of Southern California</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101856" title="Click to go to the Author Index">Sukhatme, Gaurav</a></td><td class="r">Univ. of Southern California</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1578" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Tactile_Sensing" title="Click to go to the Keyword Index">Tactile Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Biomimetics" title="Click to go to the Keyword Index">Biomimetics</a></span><br>
                           <strong>Abstract:</strong> Manipulation tasks often require robots to make contact between a grasped tool and another object in the robot's environment. The ability to detect and estimate the positions and directions of these contact points is crucial for monitoring the progress of the task, and detecting failures. In this paper, we present a data-driven approach for detecting and localizing contacts between a grasped object and the environment using tactile sensing. We explore framing the contact localization as both a regression and a classification problem and train neural networks accordingly to estimate the contact parameters. We also compare the neural networks with Gaussian process regression and support vector machine classification with spatio-temporal hierarchical matching pursuit feature learning. We evaluate the presented approach using hundreds of contact events on eighteen objects with different shapes, sizes and material properties. The experiments show that the neural network approach can learn to localize contact events for individual objects with a mean absolute error of less than 2.5 cm for the positions and less than 10 deg for the directions.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_23">11:07-11:08, Paper TuT11.23</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1585.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1585'); return false" title="Click to show or hide the keywords and abstract">Thermal Image Enhancement Using Convolutional Neural Network</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111307" title="Click to go to the Author Index">Choi, Yukyung</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192830" title="Click to go to the Author Index">Kim, Namil</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192842" title="Click to go to the Author Index">Hwang, Soonmin</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101760" title="Click to go to the Author Index">Kweon, In So</a></td><td class="r">KAIST</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1585" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Detection_and_Tracking" title="Click to go to the Keyword Index">Human Detection and Tracking</a></span><br>
                           <strong>Abstract:</strong> With the advent of commodity autonomous mobiles, it is becoming increasingly prevalent to recognize under extreme conditions such as night, erratic illumination conditions. This need has caused the approaches using multi-modal sensors, which could be complementary to each other. The choice for the thermal camera provides a rich source of temperature information, less affected by changing illumination or background clutters. However, existing thermal cameras have a relatively smaller resolution than RGB cameras that has trouble for fully utilizing the information in recognition tasks. To mitigate this, we aim to enhance the low-resolution thermal image according to the extensive analysis of existing approaches. To this end, we introduce Thermal Image Enhancement using Convolutional Neural Network (CNN), called in TEN, which directly learns an end-to-end mapping a single low resolution image to the desired high resolution image. In addition, we examine various image domains to find the best representative of the thermal enhancement. Overall, we propose the first thermal image enhancement method based on CNN guided on RGB data. We provide extensive experiments designed to evaluate the quality of image and the performance of several object recognition tasks such as pedestrian detection, visual odometry, and image registration.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_24">11:08-11:09, Paper TuT11.24</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0260.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('260'); return false" title="Click to show or hide the keywords and abstract">M2DP: A Novel 3D Point Cloud Descriptor and Its Application in Loop Closure Detection</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195587" title="Click to go to the Author Index">He, Li</a></td><td class="r">Univ. of Alberta</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184110" title="Click to go to the Author Index">Wang, Xiaolong</a></td><td class="r">Northwestern Pol. Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100155" title="Click to go to the Author Index">Zhang, Hong</a></td><td class="r">Univ. of Alberta</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab260" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Visual_Navigation" title="Click to go to the Keyword Index">Visual Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#SLAM" title="Click to go to the Keyword Index">SLAM</a></span><br>
                           <strong>Abstract:</strong> In this paper, we present a novel global descriptor M2DP for 3D point clouds, and apply it to the problem of loop closure detection. In M2DP, we project a 3D point cloud to multiple 2D planes and generate a density signature for points for each of the planes. We then use the principal component of these signatures as the descriptor of the 3D point cloud. Our experimental results show that the proposed algorithm outperforms state-of-the-art global 3D descriptors in both accuracy and efficiency.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut11_25">11:09-11:10, Paper TuT11.25</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0305.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('305'); return false" title="Click to show or hide the keywords and abstract">Fabric Interface with Proximity and Tactile Sensation for Human-Robot Interaction</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122439" title="Click to go to the Author Index">Ho, Van</a></td><td class="r">Ryukoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102549" title="Click to go to the Author Index">Hirai, Shinichi</a></td><td class="r">Ritsumeikan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#201888" title="Click to go to the Author Index">Naraki, Koki</a></td><td class="r">Ryukoku Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab305" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Force_and_Tactile_Sensing" title="Click to go to the Keyword Index">Force and Tactile Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a>, <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a></span><br>
                           <strong>Abstract:</strong> Human-in-the-loop task involving soft contact has become&#12288;common in robotic&#12288;application, especially in physical human-robot interaction. In this task, it&#12288;is required that robot would sense interactions with human by touching, as&#12288;well as assess possibility of human approaching by proximity sensation. In&#12288;addition, it is also essential to fabricate an interface so that human does&#12288;not feel uncomfortable during physical interaction with robot. This paper&#12288;presents an attempt on fabrication of sensing elements that can be utilized&#12288;for construction of a soft interface (or a robotic skin). Each element is made&#12288;from fabrics and soft materials that can sense both proximity and applied&#12288;force from human's touch. In addition, each sensing element can sense the&#12288;relative distance of conductive object (or human body) that is approaching the&#12288;sensing element's surface, and the 2times2 contact force distribution when&#12288;the object makes contact with the sensing element. By exploiting simultaneous&#12288;measurement of capacitance, each fabric sensing element can smoothly switch&#12288;the proximity mode and tactile mode based on position of the object. We also&#12288;constructed a model that can predict variation of capacitance measurement of&#12288;proximity and tactile modes during operation for further analysis. The methods&#12288;and results presented in this paper can be extended to construct a larger&#12288;scale of robotic skin for robot's body, and act as a platform for study&#12288;human-robot interaction.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tut12"><b>TuT12</b></a></td>
               <td class="r">#112</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tut12" title="Click to go to the Program at a Glance"><b>Mechanism Design and Control</b></a></td>
               <td class="r">Teaser Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#120739" title="Click to go to the Author Index">Kajikawa, Shinya</a></td><td class="r">Tohoku Gakuin Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#152176" title="Click to go to the Author Index">Li, Zheng</a></td><td class="r">The Chinese Univ. of Hong Kong</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_01">10:45-10:46, Paper TuT12.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0047.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('47'); return false" title="Click to show or hide the keywords and abstract">A New VSJ Mechanism for Multi-Directional Passivity and Quick Response</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#120739" title="Click to go to the Author Index">Kajikawa, Shinya</a></td><td class="r">Tohoku Gakuin Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab47" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a>, <a href="IROS16_KeywordIndexMedia.html#Flexible_Arms" title="Click to go to the Keyword Index">Flexible Arms</a>, <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a></span><br>
                           <strong>Abstract:</strong> We propose a novel variable stiffness joint (VSJ) mechanism that achieves the desired stiffness by using elastic deformation of silicone rubber cushions (SRCs) . This mechanism can absorb the forces from multiple direction by the flexibility of SRCs and guarantee high safety. To react adequately against unpredictable collision or to manipulate an unstable object skillfully, a joint should be designed to have a stiffness adjustment capability in a short time. The proposed mechanism adjusts stiffness by controlling the position of an elliptical cam that pushes or pulls the support wall of the SRCs to surround them. The elastic deformation of the SRCs can be restricted according to the degree of the surrounded region of the SRCs. As a result, we can control the joint stiffness easily. Furthermore, the rotation of the cam from 0 deg. to 90 deg. accomodates the entire range of stiffness;, as a result, we can expect a fast adjustment of the stiffness. In this paper, we describe in detail the mechanism and the theoretical model for the stiffness adjustment. Moreover fundamental performances on the stiffness range, adjustment speed, and movability of the joint are shown through several experiments, and the problems in actual use are discussed.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_02">10:46-10:47, Paper TuT12.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0107.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('107'); return false" title="Click to show or hide the keywords and abstract">Machining with Serial and Quasi-Serial Industrial Robots: Comparison Analysis and Architecture Limitations</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#144194" title="Click to go to the Author Index">Klimchik, Alexandr</a></td><td class="r">Innopolis Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#137288" title="Click to go to the Author Index">Magid, Evgeni</a></td><td class="r">Innopolis Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105193" title="Click to go to the Author Index">Pashkevich, Anatol</a></td><td class="r">Ec. Des Mines De Nantes</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab107" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Industrial_Robots" title="Click to go to the Keyword Index">Industrial Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Performance_Evaluation_and_Benchmarking" title="Click to go to the Keyword Index">Performance Evaluation and Benchmarking</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> The paper deals with comparison study of serial and quasi-serial industrial robots used for machining operations. It pro-poses a new methodology for robot ranking, which is based on estimation of the end-effector resistance to cutting forces for several machining tasks, which are optimally located within the robot workspace. To cover wide range of applications, a set of isotropic, quasi-isotropic and extended benchmark tasks are considered. It is shown that regardless of the benchmark problem, seral manipulators are preferable for small and me-dium tasks while quasi-serial ones’ better suit large-dimensional tasks. The proposed technique was applied for the comparison analysis of 10 industrial robots of both serial and quasi-serial architectures with similar working radius and payload of about 200 kg.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_03">10:47-10:48, Paper TuT12.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0162.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('162'); return false" title="Click to show or hide the keywords and abstract">Twisted String Actuation with Sliding Surfaces</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103582" title="Click to go to the Author Index">Palli, Gianluca</a></td><td class="r">Univ. of Bologna</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180251" title="Click to go to the Author Index">Hosseini, Mohssen</a></td><td class="r">Univ. of Bologna</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103935" title="Click to go to the Author Index">Melchiorri, Claudio</a></td><td class="r">Univ. of Bologna</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab162" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#New_Actuators" title="Click to go to the Keyword Index">New Actuators</a>, <a href="IROS16_KeywordIndexMedia.html#Tendon_Wire_Mechanisms" title="Click to go to the Keyword Index">Tendon/Wire Mechanisms</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> In this paper, an ongoing work for verifying the behavior of a twisted string actuator in contact with a sliding surface or guided through a sheath is presented. The twisted string actuation system is particularly suitable for very compact and light-weight robotic devices, like artificial limbs and exoskeletons, since it allows the implementation of powerful tendon-based driving systems, based on small-size DC motors characterized by high speed, low torque and very limited inertia. One of the major limitations of this actuation system is by now related to the fact that the string should not be in contact with any obstacle, because this contact will alter the twisting angle propagation along the string and, eventually, completely stop the string twisting. This design constraint imposes a straight path between the motor and the linear load attached to the other string end. After the presentation of the basic properties of the twisted string actuation system, the model of the twisted string in contact with a sliding surface is discussed. The behavior of the system has been then experimentally verified and discussed. A preliminary evaluation of control strategies for compensating the side effects generated by the contact of the twisted string with the sliding surface is also presented.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_04">10:48-10:49, Paper TuT12.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0202.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('202'); return false" title="Click to show or hide the keywords and abstract">An Active Disturbance Rejection Controller Design for the Robust Position Control of Series Elastic Actuators</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#126838" title="Click to go to the Author Index">Sariyildiz, Emre</a></td><td class="r">National Univ. of Singapore</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#159808" title="Click to go to the Author Index">Chen, Gong</a></td><td class="r">National Univ. of Singapore</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107709" title="Click to go to the Author Index">Yu, Haoyong</a></td><td class="r">National Univ. of Singapore</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab202" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Variable_Stiffness_Actuator_Design_and_Control" title="Click to go to the Keyword Index">Variable Stiffness Actuator Design and Control</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a></span><br>
                           <strong>Abstract:</strong> Series Elastic Actuators (SEAs) have several superiorities over conventional stiff and non-back-drivable actuators in force control, e.g., lower reflected inertia, low cost force measurement, high force fidelity, safety, and so on. However, their position control applications significantly suffer from low performance and disturbances due to insufficient controller designs. In this paper, a new Active Disturbance Rejection (ADR) controller is proposed for the robust position control problem of SEAs by combining Differential Flatness (DF) and Disturbance Observer (DOb) in state space. The trajectory of the actuator is generated by using DF and is tracked by using a conventional state feed-back controller. The state and control input references of a DF-based trajectory tracking controller are modified by using estimated disturbances so that the robustness is achieved. The proposed controller provides high performance position control for SEAs when they suffer from plant uncertainties and external disturbances such as inertia variation, backlash, friction and external load. Experimental results are given to validate the proposal.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_05">10:49-10:50, Paper TuT12.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0277.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('277'); return false" title="Click to show or hide the keywords and abstract">An Asymmetric Cable-Driven Mechanism for Force Control of Exoskeleton Systems</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#163180" title="Click to go to the Author Index">Jung, Yeongtae</a></td><td class="r">UNIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#109405" title="Click to go to the Author Index">Bae, Joonbum</a></td><td class="r">UNIST</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab277" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Tendon_Wire_Mechanisms" title="Click to go to the Keyword Index">Tendon/Wire Mechanisms</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a></span><br>
                           <strong>Abstract:</strong> In this paper, an asymmetric cable-driven mechanism is proposed for accurate force control of exoskeleton systems with a compact structure. Inspired by the fact that the required forces in human motions are not symmetric in many cases, a spring-actuator type cable-drive mechanism is adopted, which enables a compact cable routing structure. The joint is connected with the exoskeleton frame through a rotary series elastic mechanism to transmit the desired force to the human user. High performance in force control is achieved by advanced control algorithms, which combine a proportional and differential (PD) controller optimized with a linear quadratic (LQ) method with a disturbance observer (DOB) and a zero phase error tracking (ZPET) feedforward filter. The proposed system was tested for the elbow joint. Experimental results confirmed that the proposed system was able to generate and deliver accurate force to the human user even with external disturbances and modeling uncertainties introduced by human motions.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_06">10:50-10:51, Paper TuT12.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0285.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('285'); return false" title="Click to show or hide the keywords and abstract">Constrained Robot Control Using Control Barrier Functions</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#194625" title="Click to go to the Author Index">Rauscher, Manuel</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#155746" title="Click to go to the Author Index">Kimmel, Melanie</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107646" title="Click to go to the Author Index">Hirche, Sandra</a></td><td class="r">Tech. Univ. München</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab285" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Safety" title="Click to go to the Keyword Index">Robot Safety</a>, <a href="IROS16_KeywordIndexMedia.html#Collision_Detection_and_Avoidance" title="Click to go to the Keyword Index">Collision Detection and Avoidance</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> Many robotic applications, especially if humans are involved, require the robot to adhere to certain joint, workspace, velocity or force limits while simultaneously executing a task. In this paper, we introduce a control structure, which merges an arbitrary desired robot behavior with given constraints. Using a quadratic program (QP), control barrier functions (CBFs) are combined with an arbitrary nominal control law, which determines the desired behavior. The CBFs enforce the constraints, overruling nominal control whenever necessary. We show that the concept is applicable with arbitrary numbers of constraints and any nominal control law. In order to illustrate the capabilities of the approach, the control scheme is applied to an anthropomorphic manipulator, which is constrained by static as well as moving constraints.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_07">10:51-10:52, Paper TuT12.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0517.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('517'); return false" title="Click to show or hide the keywords and abstract">Nonlinear Disturbance Observer Based Torque Control for Series Elastic Actuator</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#189905" title="Click to go to the Author Index">Wang, Meng</a></td><td class="r">Nankai Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158201" title="Click to go to the Author Index">Sun, Lei</a></td><td class="r">Nankai Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190960" title="Click to go to the Author Index">Yin, Wei</a></td><td class="r">Nankai Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190970" title="Click to go to the Author Index">Dong, Shuai</a></td><td class="r">Nankai Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106992" title="Click to go to the Author Index">Liu, Jingtai</a></td><td class="r">Nankai Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab517" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Variable_Stiffness_Actuator_Design_and_Control" title="Click to go to the Keyword Index">Variable Stiffness Actuator Design and Control</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> This paper presents a practical control approach for series elastic actuators(SEAs) to generate the desired torque. Specifically, the controller is applicable to both linear and nonlinear SEAs and it works well even in the presence of unknown payload parameters and external disturbances. Via the analysis and transformation of the SEA dynamics, a lumped disturbance signal is constructed and a form convenient for controller design is developed. Then, a nonlinear disturbance observer(NDOB) and a sliding-mode control scheme are introduced to synthesize the control law. The performance of the proposed controller is theoretically ensured by Lyapunov analysis. Taking a nonlinear SEA for instance, a series of simulations and hardware experiments are carried out. The results suggest the effectiveness and superior performance of the proposed method for SEA torque control by comparing it with the cascade-PID controller.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_08">10:52-10:53, Paper TuT12.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0528.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('528'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Autonomous Decentralized Control for Soft-Bodied Caterpillar-Like Modular Robot Exploiting Large and Continuum Deformation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110201" title="Click to go to the Author Index">Umedachi, Takuya</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#108942" title="Click to go to the Author Index">Trimmer, Barry</a></td><td class="r">Tufts Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab528" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0528.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Cellular_and_Modular_Robots" title="Click to go to the Keyword Index">Cellular and Modular Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Biologically_Inspired_Robots" title="Click to go to the Keyword Index">Biologically-Inspired Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Soft_bodied_Robots" title="Click to go to the Keyword Index">Soft-bodied Robots</a></span><br>
                           <strong>Abstract:</strong> Building robots from soft materials provides opportunities to create more robust and adaptive designs: soft bodies can conform to complex shapes in the environment and they are able to cushion shocks and store elastic energy. The challenge, however, remains to control highly deformable moveable structures effectively. We have proposed that one useful approach is through an autonomous decentralized control using physically coupled mechano-sensory oscillators. We have developed a highly deformable 3-D printed soft robot (PS robot) as a platform to explore the validity of the control strategy. Based on this platform we introduce a caterpillar-like soft-bodied modular-robot that is controlled in a fully decentralized manner. This paper focuses on one of the advantageous characteristics of autonomous decentralized control, i.e., the extensibility/contractibility of the modularity structures. The numerical and experimental results demonstrate that simple oscillators (controllers) can interact with each other by deforming the soft-bodied structure leading to a phase gradient that propagates crawling locomotion. The PS robots provide a powerful platform for exploring this strategy and they can be generalized for use with different robot shapes and material properties.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_09">10:53-10:54, Paper TuT12.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0545.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('545'); return false" title="Click to show or hide the keywords and abstract">Backstepping Trajectory Tracking Control for a Spherical Rolling Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179798" title="Click to go to the Author Index">Bai, Yang</a></td><td class="r">Kyushu Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105095" title="Click to go to the Author Index">Svinin, Mikhail</a></td><td class="r">Kyushu Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106559" title="Click to go to the Author Index">Yamamoto, Motoji</a></td><td class="r">Kyushu Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab545" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Nonholonomic_Motion_Planning" title="Click to go to the Keyword Index">Nonholonomic Motion Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Integrated_Planning_and_Control" title="Click to go to the Keyword Index">Integrated Planning and Control</a></span><br>
                           <strong>Abstract:</strong> This paper deals with a trajectory tracking problem for the ball-pendulum system, a spherical rolling robot driven by a two degree of freedom pendulum. The backstepping technique is applied and first tested on the hoop-pendulum system, a planar case of the ball-pendulum. By mimicking the backstepping process of the planar case, a feedback controller for the ball-pendulum system is then proposed, tracking motion trajectories for both the position and orientation of the spherical shell of the rolling robot. The validity of the constructed tracking controller is demonstrated by establishing the asymptotic stability of the error dynamics for the closed-loop system. The performance of the controller is verified under simulations for tracking linear and circular motions.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_10">10:54-10:55, Paper TuT12.10</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0603.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('603'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Complete Methodology to Design a Safety Mechanism for Prismatic Joint Implementation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195758" title="Click to go to the Author Index">Ayoubi, Younsse</a></td><td class="r">Univ. of Poitiers</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#154544" title="Click to go to the Author Index">Laribi, Med Amine</a></td><td class="r">Inst. Pprime, CNRS, Univ. De Poitiers, ENSMA</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117663" title="Click to go to the Author Index">Courreges, Fabien</a></td><td class="r">Limoges Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117018" title="Click to go to the Author Index">Zeghloul, Said</a></td><td class="r">Inst. Pprime, CNRS, Univ. De Poitiers, ENSMA</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118500" title="Click to go to the Author Index">Arsicault, Marc</a></td><td class="r">PPRIME Inst. Univ. Poitiers</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab603" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0603.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Safety" title="Click to go to the Keyword Index">Robot Safety</a>, <a href="IROS16_KeywordIndexMedia.html#Cooperating_Robots" title="Click to go to the Keyword Index">Cooperating Robots</a></span><br>
                           <strong>Abstract:</strong> The majority of recent researches have been focused on developing compliant joint for rotary motion. Few authors contributed to the problematic of safety in pure linear motion,i.e. prismatic joint. The contribution of this work is to present a new design capable of achieving, passively, a nonlinear elastic behavior for prismatic joint implementation, the so-called Prismatic Compliant Joint (PCJ). This new device is based on a six-bar mechanism equipped with a linear spring. Hence, this structure generates the desired nonlinear stiffness behavior under a specified external force. The elastic characteristic will comply with force safety criteria of physical Human/Robot interaction (pHRI); a Hunt-Crossley model based one. In order to fit the PCJ response curve to the established safety measures, an optimization based on genetic algorithm method tunes PCJ intrinsic parameters subject to the chosen constraints.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_11">10:55-10:56, Paper TuT12.11</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0634.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('634'); return false" title="Click to show or hide the keywords and abstract">Design and Kinematic Modeling of a Concentric Wire-Driven Mechanism Targeted for Minimally Invasive Surgery</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#152176" title="Click to go to the Author Index">Li, Zheng</a></td><td class="r">The Chinese Univ. of Hong Kong</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#176430" title="Click to go to the Author Index">Chiu, WAI, YAN Philip</a></td><td class="r">Chinese Univ. of Hong Kong</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118191" title="Click to go to the Author Index">Du, Ruxu</a></td><td class="r">The Chinese Univ. of Hong Kong</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab634" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a>, <a href="IROS16_KeywordIndexMedia.html#Flexible_Arms" title="Click to go to the Keyword Index">Flexible Arms</a>, <a href="IROS16_KeywordIndexMedia.html#Tendon_Wire_Mechanisms" title="Click to go to the Keyword Index">Tendon/Wire Mechanisms</a></span><br>
                           <strong>Abstract:</strong> In this paper, a concentric wire-driven mechanism (CWM) is presented. The CWM comprises of two nested wire-driven mechanisms (WDM). Each WDM contains a flexible backbone and a set of wires. The backbone bending is controlled by pulling the wires. The stiffness of the outer WDM is controllable and dominants that of the overlapped section. Therefore, in the overlapped section the inner WDM conforms to the shape of the outer WDM and the tip of the outer WDM serves as the base of the distal separate section of the inner WDM. Compared with conventional flexible mechanisms, i.e., tendon/wire/cable-driven mechanism and concentric tube mechanism, the CWM has a much wider workspace. This advantage is demonstrated by simulations based on kinematic modeling. A prototype is developed and the preliminary experimental results validate the design concept.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_12">10:56-10:57, Paper TuT12.12</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0665.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('665'); return false" title="Click to show or hide the keywords and abstract">Closed-Form Solutions for the Inverse Kinematics of the Agile Eye with Constraint Errors on the Revolute Joint Axes</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104235" title="Click to go to the Author Index">Cammarata, Alessandro</a></td><td class="r">Univ. Di Catania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104796" title="Click to go to the Author Index">Sinatra, Rosario</a></td><td class="r">Univ. Di Catania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196110" title="Click to go to the Author Index">Lacagnina, Michele</a></td><td class="r">Univ. of Catania, Department of Industrial Engineering</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab665" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Kinematics" title="Click to go to the Keyword Index">Kinematics</a>, <a href="IROS16_KeywordIndexMedia.html#Parallel_Robots" title="Click to go to the Keyword Index">Parallel Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Manufacturing_and_Automation" title="Click to go to the Keyword Index">Manufacturing and Automation</a></span><br>
                           <strong>Abstract:</strong> Manufacturing process yield errors that change the kinematics of parallel robots. In this paper we describe how the constraint errors due to the manufacturing of the revolute axes can influence the inverse kinematic model of the Agile Eye, a Spherical Parallel Robot largely studied in the literature. Starting from the nominal inverse kinematic model a complete model to take into account geometric errors on the axes of the revolute joints is obtained in closed form. Numerical simulations compare the results from the nominal and real design to show relevant changes and to confirm the statistical nonlinear nature of the problem. Finally, an application to the elastodynamics is provided to verify the influence of the constraint errors on the frequency range.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_13">10:57-10:58, Paper TuT12.13</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0694.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('694'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Modeling, Design & Characterization of a Novel Passive Variable Stiffness Joint (pVSJ)</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190390" title="Click to go to the Author Index">Awad, Mohammad I.</a></td><td class="r">Khalifa Univ. of Science Tech. and Res</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140522" title="Click to go to the Author Index">Gan, Dongming</a></td><td class="r">Khalifa Univ. of Science, Tech. and Res</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#153671" title="Click to go to the Author Index">Cempini, Marco</a></td><td class="r">Rehabilitation Inst. of Chicago</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#159922" title="Click to go to the Author Index">Cortese, Mario</a></td><td class="r">Scuola Superiore Sant'Anna</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110598" title="Click to go to the Author Index">Vitiello, Nicola</a></td><td class="r">Scuola Superiore Sant Anna</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101631" title="Click to go to the Author Index">Dias, Jorge</a></td><td class="r">Univ. of Coimbra</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101839" title="Click to go to the Author Index">Dario, Paolo</a></td><td class="r">Scuola Superiore Sant'Anna</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179559" title="Click to go to the Author Index">Seneviratne, Lakmal</a></td><td class="r">L. D. Seneviratne Is with Kings Coll. London, UK, and Robotics</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab694" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0694.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Variable_Stiffness_Actuator_Design_and_Control" title="Click to go to the Keyword Index">Variable Stiffness Actuator Design and Control</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a></span><br>
                           <strong>Abstract:</strong> In this paper we present the design and characterization of a novel Passive Variable Stiffness Joint (pVSJ). pVSJ is the proof of concept of a passive revolute joint with controllable variable stiffness. The current design is intended to be a bench-test for future development towards applications in haptic teleoperation purposed exoskeletons. The main feature of the pVSJ is its capability of varying the stiffness with infinite range based on a simple mechanical system. Moreover, the joint can rotate freely at the zero stiffness case without any limitation. The stiffness varying mechanism consists of two torsional springs, mounted with an offset from the pVSJ rotation center and coupled with the joint shaft by an idle roller. The position of the roller between the pVSJ rotation center and the spring’s center is controlled by a linear sliding actuator fitted on the chassis of the joint. The variation of the output stiffness is obtained by changing the distance from the roller-springs contact point to the joint rotation center (effective arm). If this effective arm is null, the stiffness of the joint will be zero. The stiffness increases to reach high stiffness values when the effective arm approaches its maximum value, bringing the roller close to the torsional springs’ center. The experimental results matched with the physical-based modeling of the pVSJ in terms of stiffness variation curve, stiffness dependency upon the springs’ elasticity, joint deflection and the spring’s deflection.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_14">10:58-10:59, Paper TuT12.14</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0759.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('759'); return false" title="Click to show or hide the keywords and abstract">The Waterbug Sub-Surface Sampler: Design, Control and Analysis</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196245" title="Click to go to the Author Index">Higgins, James</a></td><td class="r">Univ. of Nebraska-Lincoln</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106284" title="Click to go to the Author Index">Detweiler, Carrick</a></td><td class="r">Univ. of Nebraska-Lincoln</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab759" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Environment_Monitoring_and_Management" title="Click to go to the Keyword Index">Environment Monitoring and Management</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a>, <a href="IROS16_KeywordIndexMedia.html#Joint_Mechanism_Design" title="Click to go to the Keyword Index">Joint/Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> Monitoring and predicting water quality poses significant challenges. Collecting enough information to char- acterize bodies of water is a critical bottleneck. Collecting data and samples from the surface all the way to the bottom over a short period of time would give water scientists the best spatio-temporal picture. In this paper, we present a small, light-weight, inexpensive water sensing and sampling robot, the “Waterbug”, capable of descending to depths up to 10m, collecting sensor information and a water sample, and returning to the surface. The water sampler also has limited capability to adjust buoyancy to hold depth for the purpose of measuring environmental conditions at specific locations in the water column. It is small enough that a single scientist could carry several in a backpack or it could be deployed by other robotic systems. The low cost of the node makes it feasible for blanket deployment. No tools are required for field servicing and the sample collection chamber is a common syringe that can be swapped quickly for redeployment. The main challenge was developing the system model and algorithm for achieving neutral buoyancy in the presence of system and initial condition variance. Over a range of conditions, we were able to achieve an 80% success rate for meeting the neutral buoyancy criteria and a 100% success rate in capturing a sample and returning to the surface.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_15">10:59-11:00, Paper TuT12.15</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0810.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('810'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Gravity Compensation Mechanism for Roll-Pitch Rotation of a Robotic Arm</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180755" title="Click to go to the Author Index">Chung, Deok Gyoon</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100050" title="Click to go to the Author Index">Kwon, Dong-Soo</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168042" title="Click to go to the Author Index">Won, Jongseok</a></td><td class="r">Seoul Natl Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#145094" title="Click to go to the Author Index">Hwang, Minho</a></td><td class="r">KAIST</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab810" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0810.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a>, <a href="IROS16_KeywordIndexMedia.html#Surgical_Robotics" title="Click to go to the Keyword Index">Surgical Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Formal_Methods_in_Robotics_and_Automation" title="Click to go to the Keyword Index">Formal Methods in Robotics and Automation</a></span><br>
                           <strong>Abstract:</strong> In robotics, robotic arms should be smaller, less expensive, and able to demonstrate a better performance, in order to be utilized in various fields. However, expensive actuators and speed reducers are required to generate sufficient force at the end effector after overcoming the gravitational torque of robotic arm. Robotic arms become compact and cost effective if the gravitational torque can be passively compensated. Especially in the field of robotic laparoscopic surgery, the static equilibrium state of the robot arm in the entire workspace is essential to achieve both safety and ease of use. In this study, we propose a novel passive gravity compensation mechanism based on springs and wires. This mechanism utilizes a scotch yoke mechanism to compensate the gravitational torque changed by two rotational degrees of freedom (DOF). Furthermore, we applied to a surgical robotic arm which was constructed using a roll-pitch joint. According to the experiments, it was proved that the new mechanism effectively compensate the gravitational torque. As a result, compact and cost-effective robotic arms can be developed satisfying safety and ease of use.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_16">11:00-11:01, Paper TuT12.16</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1137.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1137'); return false" title="Click to show or hide the keywords and abstract">High-Speed and Compact Depalletizing Robot Capable of Handling Packages Stacked Complicatedly</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110638" title="Click to go to the Author Index">Nakamoto, Hideichi</a></td><td class="r">Toshiba Corp</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195850" title="Click to go to the Author Index">Eto, Haruna</a></td><td class="r">Toshiba Corp</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#130026" title="Click to go to the Author Index">Sonoura, Takafumi</a></td><td class="r">Toshiba Corp</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195563" title="Click to go to the Author Index">Tanaka, Junya</a></td><td class="r">Toshiba Corp</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195323" title="Click to go to the Author Index">Ogawa, Akihito</a></td><td class="r">TOSHIBA Corp</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1137" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a>, <a href="IROS16_KeywordIndexMedia.html#Manufacturing_and_Automation" title="Click to go to the Keyword Index">Manufacturing and Automation</a>, <a href="IROS16_KeywordIndexMedia.html#Gripper_and_Hand_Design" title="Click to go to the Keyword Index">Gripper and Hand Design</a></span><br>
                           <strong>Abstract:</strong> For a depalletizing robot to be practical, it has to be able to work skillfully in a limited space with a depalletizinging speed equal to or exceeding that of a human. Our new robot has the following advantages. A gantry robot and a telescopic arm mechanism are adopted so that the robot, although space-saving, has a large workspace. Using camera image and depth image, the robot recognizes the various package stacked complicatedly and decides the handling order of the package. The robot generates the most suitable handling path and performs handling control such that stacks do not collapse. The robot achieves fall prevention of the package and stable conveyance by supporting beneath the package with another arm, the conveyor arm. Moreover, in order to accelerate processing of packages, we thought it was important for the robot to be equipped with a wide end effector capable holding multiple packages simultaneously. Therefore we developed a depalletizing simulator and performed the optimal design of the end effector by using a simulator. This, we were able to realize a compact depalletizing robot with a depalletizing speed equal to that of a human.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_17">11:01-11:02, Paper TuT12.17</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1252.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1252'); return false" title="Click to show or hide the keywords and abstract">Design Optimisation and Performance Evaluation of a Toroidal Magnetorheological Hydraulic Piston Head</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#167448" title="Click to go to the Author Index">Aguirre Dominguez, Gonzalo</a></td><td class="r">Waseda Univ. Sugano Lab</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114910" title="Click to go to the Author Index">Kamezaki, Mitsuhiro</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#193010" title="Click to go to the Author Index">He, Shan</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178219" title="Click to go to the Author Index">Somlor, Sophon</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#108304" title="Click to go to the Author Index">Schmitz, Alexander</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100151" title="Click to go to the Author Index">Sugano, Shigeki</a></td><td class="r">Waseda Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1252" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#New_Actuators" title="Click to go to the Keyword Index">New Actuators</a>, <a href="IROS16_KeywordIndexMedia.html#Hydraulic_Pneumatic_Actuators" title="Click to go to the Keyword Index">Hydraulic/Pneumatic Actuators</a>, <a href="IROS16_KeywordIndexMedia.html#Compliance_and_Impedance_Control" title="Click to go to the Keyword Index">Compliance and Impedance Control</a></span><br>
                           <strong>Abstract:</strong> The advantages of mechanical compliance have driven the development of devices using new smart materials. A new kind of magnetorheological piston based on a toroidal array of magnetorheological valves, has been previously tested to prove its feasibility. However, being an initial prototype its potential was still limited by its complex design, and low output force. This study presents the revisions done to the design with several improvements targeting key performance parameters. An improved annular piston design is also introduced as comparison with conventional devices. The toroidal and annular piston head prototypes are built and tested, and their force performance compared with the previous iteration. The experimental results show an overall performance improvement of the toroidal assembly. However, the force model used in the study still fails to accurately predict the magnetic flux at the gaps of the piston head. This deviation is later verify and corrected using a FEM analysis. The force performance of the new toroidal assembly is on par with the commonplace annular design. It also displays a more linear behaviour, at the expense of lower energy efficiency. Finally, it also shows potential for a greater degree of customisation to meet different system requirements.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_18">11:02-11:03, Paper TuT12.18</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1333.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1333'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Kinematic Modeling, Analysis, and Load Distribution Algorithm for a Redundantly Actuated 4-DOF Parallel Mechanism</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169322" title="Click to go to the Author Index">Kang, Long</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106327" title="Click to go to the Author Index">Kim, Whee Kuk</a></td><td class="r">Korea Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106263" title="Click to go to the Author Index">Yi, Byung-Ju</a></td><td class="r">Hanyang Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1333" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1333.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Parallel_Robots" title="Click to go to the Keyword Index">Parallel Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Kinematics" title="Click to go to the Keyword Index">Kinematics</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> This paper presents the kinematic modeling and analysis of a gravity balanced 3T1R parallel mechanism with actuation redundancy. The screw theory is employed for Jacobian derivation and singularity analysis. Actuation redundancy is introduced to eliminate forward kinematic singularities. Moreover, three load redistribution algorithms employing redundant actuation to avoid torque saturation are presented and the comparison analysis shows through simulation study that the newly suggested algorithm considering a relaxed form of torque limit has several advantageous features in comparison to other load distribution algorithms.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_19">11:03-11:04, Paper TuT12.19</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1381.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1381'); return false" title="Click to show or hide the keywords and abstract">Impulse Modeling and Analysis of Dual Arm Hammering Task: Human-Like Manipulator</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#189664" title="Click to go to the Author Index">Imran, Abid</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106263" title="Click to go to the Author Index">Yi, Byung-Ju</a></td><td class="r">Hanyang Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1381" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Biomimetics" title="Click to go to the Keyword Index">Biomimetics</a>, <a href="IROS16_KeywordIndexMedia.html#Dynamics" title="Click to go to the Keyword Index">Dynamics</a>, <a href="IROS16_KeywordIndexMedia.html#Biologically_Inspired_Robots" title="Click to go to the Keyword Index">Biologically-Inspired Robots</a></span><br>
                           <strong>Abstract:</strong> This paper presents the robotic hammering task performed by single arm and dual arm. In order to analyze optimal hammering task, the analytical model of external and internal impulses are taken as the measure. Desired hammering task implies that the hammer simultaneously exerts maximum external impulse on nail and minimum internal impulses on robot or human joints. The advantages of dual arm manipulator with muscles over dual arm without muscles and single arm model with muscle and without muscle are presented. Moreover, the effective mass in nailing operation is experimentally obtained for two different materials and it is integrated into the external impulse model of the hammering task. Furthermore, the hammering workspace is optimized by considering the minimum internal and maximum external impulses.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_20">11:04-11:05, Paper TuT12.20</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1436.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1436'); return false" title="Click to show or hide the keywords and abstract">Joint Torque Servo Control of Electro-Hydrostatic Actuators for High Torque-To-Weight Ratio Robot Control</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#172750" title="Click to go to the Author Index">Lee, Woongyong</a></td><td class="r">POSTECH</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#149446" title="Click to go to the Author Index">Kim, Min Jun</a></td><td class="r">POSTECH</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100095" title="Click to go to the Author Index">Chung, Wan Kyun</a></td><td class="r">POSTECH</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1436" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Hydraulic_Pneumatic_Actuators" title="Click to go to the Keyword Index">Hydraulic/Pneumatic Actuators</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a></span><br>
                           <strong>Abstract:</strong> This paper proposes joint torque servo control of an electro-hydrostatic actuator (EHA) to achieve effective dynamic control of robot manipulators. Particularly, in control design, fluid parameters are not considered when developing a servo controller for multi degree-of-freedom (DOF) robot manipulators. Instead, the mechanical impedance, which represents the effect of robot joint velocity on joint torque, is reduced using a disturbance observer (DOB). Then, a simple PID control is applied to the modified system to improve the joint torque tracking performance. The proposed controller was validated on two 1-DOF EHAs. It guarantees the robustness to external disturbances and to fluid parameter variations even though fluid parameters are not considered in control design.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_21">11:05-11:06, Paper TuT12.21</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1493.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1493'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Mechanical Implementation of a Variable-Stiffness Actuator for a Softly Strummed Ukulele</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178331" title="Click to go to the Author Index">Lawrence, Austin</a></td><td class="r">Northwestern Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140093" title="Click to go to the Author Index">Alspach, Alexander</a></td><td class="r">Disney Res</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114671" title="Click to go to the Author Index">Bentivegna, Darrin</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1493" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1493.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Variable_Stiffness_Actuator_Design_and_Control" title="Click to go to the Keyword Index">Variable Stiffness Actuator Design and Control</a>, <a href="IROS16_KeywordIndexMedia.html#Soft_bodied_Robots" title="Click to go to the Keyword Index">Soft-bodied Robots</a>, <a href="IROS16_KeywordIndexMedia.html#New_Actuators" title="Click to go to the Keyword Index">New Actuators</a></span><br>
                           <strong>Abstract:</strong> This research illustrates the design, implementation, and evaluation of pneumatic variable-stiffness actuator (VSA) used to strum a four-stringed ukulele with audio variability. A guitar pick is antagonistically loaded with two inflatable polydimethylsiloxane (PDMS) actuators, allowing for the independent control of both position or stiffness through the utilization of one and two PDMS solenoids, respectively. To generate smooth analog pressure signals, the bellows incorporate a controlled leak to atmospheric pressure, having synonymous properties to a low-pass filter circuit when fed a coarse pressure signal through PWM control. Experimental results illustrate a minimum to maximum stiffness range of X to Y Nm/rad, a peak stiffness change of X Nm/rad/s, and a stiffness change rate of X Nm/rad/pa. Additional to the VSA, alternative PDMS fabrication methods are presented to allow for complex, precise manufacturing of silicone bodies in a low-cost manner.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_22">11:06-11:07, Paper TuT12.22</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0460.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('460'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>HPP: A New Software for Constrained Motion Planning</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184043" title="Click to go to the Author Index">Mirabel, Joseph</a></td><td class="r">LAAS-CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#182535" title="Click to go to the Author Index">Tonneau, Steve</a></td><td class="r">Cnrs - Laas</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195551" title="Click to go to the Author Index">Seppälä, Anna-Kaarina</a></td><td class="r">LAAS-CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195550" title="Click to go to the Author Index">Fernbach, Pierre</a></td><td class="r">Cnrs - Laas</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184359" title="Click to go to the Author Index">Campana, Mylène</a></td><td class="r">LAAS-CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103132" title="Click to go to the Author Index">Mansard, Nicolas</a></td><td class="r">CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101933" title="Click to go to the Author Index">Lamiraux, Florent</a></td><td class="r">CNRS</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab460" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0460.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Planning_for_Manipulators" title="Click to go to the Keyword Index">Motion Planning for Manipulators</a>, <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a></span><br>
                           <strong>Abstract:</strong> We present HPP, a software designed for complex classes of motion planning problems, such as navigation among movable objects, manipulation, contact-rich multiped locomotion, or elastic rods in cluttered environments. HPP is an open-source answer to the lack of a standard framework for these important issues for robotics and graphics communities.<p>HPP adopts a clear object oriented architecture, which makes it easy to implement parts of an existing planning algorithm, or entirely new algorithms. Python bindings and a visualization tool allow for fast problem setting and prototyping: a new algorithm can be implemented in just a few lines of code.<p>HPP can be used for classic planning problems such as pick and place for mobile robots, but is specifically designed to solve problems where the motion of the robot is constrained. Examples of behaviors produced by HPP thanks to a generic constraint formulation include: maintaining a relative orientation between bodies, enforcing the static equilibrium of the robot, or automatically inferring that an object must be moved to allow locomotion. Constraints are tied to a custom representation of the kinematic chain, compatible with the Unified Robot Description format (URDF).<p>To illustrate the possibilities of HPP, we present several recent scientific contributions implemented with HPP, most of which are provided with Python tutorials. HPP aims at being seamlessly integrated within a global robot control loop: Pinocchio, the fast multi-body dynamics library, is currently being integrated in HPP, thus bridging the gap between the planning and control communities.	
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_23">11:07-11:08, Paper TuT12.23</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1293.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1293'); return false" title="Click to show or hide the keywords and abstract">Accurate Torque Control of Finger Joints with UT Hand Exoskeleton through Bowden Cable SEA</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114430" title="Click to go to the Author Index">Yun, Youngmok</a></td><td class="r">The Univ. of Texas at Austin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#147533" title="Click to go to the Author Index">Agarwal, Priyanshu</a></td><td class="r">Univ. of Texas at Austin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169783" title="Click to go to the Author Index">Fox, Jonas</a></td><td class="r">The Univ. of Texas at Austin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#186541" title="Click to go to the Author Index">Madden, Kaci</a></td><td class="r">Univ. of Texas</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106060" title="Click to go to the Author Index">Deshpande, Ashish</a></td><td class="r">Univ. of Texas</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1293" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Wearable_Robots" title="Click to go to the Keyword Index">Wearable Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Rehabilitation_Robotics" title="Click to go to the Keyword Index">Rehabilitation Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Tendon_Wire_Mechanisms" title="Click to go to the Keyword Index">Tendon/Wire Mechanisms</a></span><br>
                           <strong>Abstract:</strong> The torque control of finger joints is important for effective hand rehabilitation after neural disorders such as stroke. This paper presents an approach for accurate torque control of finger joints with UT hand exoskeleton. We present 1) how we obtained an accurate kinematics model, 2) how we built the torque actuation model with Bowden cable SEA, and 3) how we controlled the torque of finger joints with the kinematic model and the Bowden cable SEA. We have validated our approach with a testbed finger and results show that the UT hand exoskeleton accurately controls the torque of finger joints
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_24">11:08-11:09, Paper TuT12.24</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1392.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1392'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Toward Autonomous Aircraft Piloting by a Humanoid Robot: Hardware and Control Algorithm Design</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184926" title="Click to go to the Author Index">Song, Hanjun</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196704" title="Click to go to the Author Index">Shin, Heemin</a></td><td class="r">Korea Advanced Inst. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196705" title="Click to go to the Author Index">You, Haram</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#202565" title="Click to go to the Author Index">Hong, Jun</a></td><td class="r">Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104593" title="Click to go to the Author Index">Shim, David Hyunchul</a></td><td class="r">KAIST</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1392" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1392.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robotics_in_Hazardous_Fields" title="Click to go to the Keyword Index">Robotics in Hazardous Fields</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a></span><br>
                           <strong>Abstract:</strong> Unmanned aerial vehicles (UAVs) are now very popular for many applications such as surveillance and transport and they are typically constructed starting from the design process. However, it is very time consuming and require all new airworthiness process. In this paper, we aim to provide a novel framework to automate an existing aircraft for unmanned operations using a humanoid robot, which is capable of manipulating the yoke, levers, switches and pedals to fly the airplane while operating various components such as lights and landing gears just as a human pilot would do. In this manner, the airplane can be converted for automated mode in a very short time without losing airworthiness. For testing, due to the complication of operating the robot in a real airplane, it is validated using a flight simulator, which provides the flight data over the network. The simulator is installed on a motion platform, which makes the robot move around due to the airplane’s motion for realism. The proposed method is successfully validated in a series of flight simulations with various scenarios to show the feasibility of humanoid robot operation of an aircraft under various conditions.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut12_25">11:09-11:10, Paper TuT12.25</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1464.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1464'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>One DoF Robotic Hand That Makes Human Laugh by Tickling through Rubbing Underarm</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140462" title="Click to go to the Author Index">Kishi, Tatsuhiro</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#166310" title="Click to go to the Author Index">Nozawa, Takashi</a></td><td class="r">Mejiro Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196769" title="Click to go to the Author Index">Nibori, Ai</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#170186" title="Click to go to the Author Index">Futaki, Hajime</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196775" title="Click to go to the Author Index">Miura, Yusaku</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196779" title="Click to go to the Author Index">Shina, Megumi</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196780" title="Click to go to the Author Index">Matsuki, Kei</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180490" title="Click to go to the Author Index">Yanagino, Hiroshi</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#163373" title="Click to go to the Author Index">Cosentino, Sarah</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101936" title="Click to go to the Author Index">Hashimoto, Kenji</a></td><td class="r">Waseda Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102287" title="Click to go to the Author Index">Takanishi, Atsuo</a></td><td class="r">Waseda Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1464" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1464.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Entertainment_Robotics" title="Click to go to the Keyword Index">Entertainment Robotics</a></span><br>
                           <strong>Abstract:</strong> This paper describes the development of one DoF robotic hand that makes human laugh by tickling through rubbing underarm. Laughter is attracting research attention because it enhances health by treating or preventing mental diseases. However, laughter has not been used effectively in healthcare because the mechanism of laughter is complicated and is yet to be fully understood. The development of a robot capable of making humans laugh is useful for clarifying the mechanism of laughter because the stimuli by the robot is quantitative and reproductive. Especially, tickling matches to this purpose because the relationship between stimuli and reaction is simpler compared to other techniques. Therefore, this research aimed to develop a robotic hand that can output quantitative and reproductive tickling stimuli for clarifying the mechanism of laughter. Rubbing underarm is selected as a target motion of robot because previous research suggests that this is the best way for making humans feel ticklish. In order to achieve the tickling motion by robots as humans, the required specifications were determined through experimental method. In order to develop a robot that achieves the required fingertip trajectory by simple mechanisms as much as possible, mechanism with crank and link driven by single motor was developed. The result of experimental evaluation shows that the developed robot could make humans laugh by its rubbing motion. In addition, the quantitative tickling motion by developed robotic hand was suggested to be effective for clarifying the mechanism of laughter.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuai1"><b>TuAI1</b></a></td>
               <td class="r">#111</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuai1" title="Click to go to the Program at a Glance"><b>Interactive Session: Sensing</b></a></td>
               <td class="r">Interactive Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#102922" title="Click to go to the Author Index">Asfour, Tamim</a></td><td class="r">Karlsruhe Inst. of Tech. (KIT)</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#102001" title="Click to go to the Author Index">Merlet, Jean-Pierre</a></td><td class="r">INRIA</td></tr>
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuai2"><b>TuAI2</b></a></td>
               <td class="r">#112</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuai2" title="Click to go to the Program at a Glance"><b>Interactive Session: Mechanism Design and Control</b></a></td>
               <td class="r">Interactive Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#120739" title="Click to go to the Author Index">Kajikawa, Shinya</a></td><td class="r">Tohoku Gakuin Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#152176" title="Click to go to the Author Index">Li, Zheng</a></td><td class="r">The Chinese Univ. of Hong Kong</td></tr>
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuat1"><b>TuAT1</b></a></td>
               <td class="r">#101</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuat1" title="Click to go to the Program at a Glance"><b>Robot Control Systems</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#105608" title="Click to go to the Author Index">Knoll, Alois</a></td><td class="r">Tech. Univ. Muenchen TUM</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#179107" title="Click to go to the Author Index">Balachandran, Ribin</a></td><td class="r">DLR</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat1_01">11:15-11:30, Paper TuAT1.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0239.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('239'); return false" title="Click to show or hide the keywords and abstract">Performance Comparison of Wave Variable Transformation and Time Domain Passivity Approaches for Time-Delayed Teleoperation: Preliminary Results</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179107" title="Click to go to the Author Index">Balachandran, Ribin</a></td><td class="r">DLR</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111257" title="Click to go to the Author Index">Artigas, Jordi</a></td><td class="r">DLR - German Aerospace Center</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102369" title="Click to go to the Author Index">Ryu, Jee-Hwan</a></td><td class="r">Korea Univ. of Tech. and Education</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179770" title="Click to go to the Author Index">Mehmood, Usman</a></td><td class="r">Korea Univ. of Tech. and Education</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab239" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Telerobotics" title="Click to go to the Keyword Index">Telerobotics</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Dexterous_Manipulation" title="Click to go to the Keyword Index">Dexterous Manipulation</a></span><br>
                           <strong>Abstract:</strong> This paper presents initial results of a set of experiments to compare time-delayed teleoperation performance of selected architectures based on two popular passivity based methods, namely, Wave Variables Transformation (WVT) and Time Domain Passivity Approach (TDPA). Five different architectures widely used and available in literature are selected: 2-channel and 3-channel for both WV and TDPA, and 4-channel based on TDPA. An empirical approach for measuring transparency is proposed, which presents three main qualities: 1) Controller independency 2) Frequency dependency of performance and 3) Removal of the human operator textit{out} of the loop. In order to achieve this, the human operator is replaced by a linear actuator equipped with a force sensor. Deterministic interactions between the simulated operator and the rest of the system can thus be easily implemented and registered. Therefore, this approach allows to isolate pure effects of any control method and perform a proper analysis in terms of selected attributes for a desired metrics. In this article, these are chosen to be effective impedance and position tracking error, both in the frequency domain. In addition to the quantitative results, a qualitative analysis is presented which underlines practical implications of each architecture and control method. The comparison sample presented contains five different delays, varying from 0 to 200 milliseconds, five different environmental impedances and 5 bilateral control architectures along with an analysis of the results obtained.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat1_02">11:30-11:45, Paper TuAT1.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0435.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('435'); return false" title="Click to show or hide the keywords and abstract">Sensor-Based Control Using Finite Time Observer of Visual Signatures: Application to Corridor Following</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192232" title="Click to go to the Author Index">Ben Said, Hela</a></td><td class="r">XLIM Lab. UMR CNRS 7252, Univ. of Limoges</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#159458" title="Click to go to the Author Index">Stephant, Joanny</a></td><td class="r">XLIM UMR CNRS 7252 Limoges Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110001" title="Click to go to the Author Index">Labbani-Igbida, Ouiddad</a></td><td class="r">Univ. of Limoges -- ENSIL Engineering School -- XLIM Inst</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab435" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Control_Architectures_and_Programming" title="Click to go to the Keyword Index">Control Architectures and Programming</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Servoing" title="Click to go to the Keyword Index">Visual Servoing</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_based_Planning" title="Click to go to the Keyword Index">Sensor-based Planning</a></span><br>
                           <strong>Abstract:</strong> In this paper, we describe an approach to autonomous navigation using a state observer for sensor based control. In particular, we investigate the task of corridor following in the case of sensory failure (unreliable data) and/or loss of measurement. Our approach uses a virtual sensory frame to project laser range&#64257;nder scans, and builds visual features (namely the position of the vanishing point and the orientation of the corridor median line) to be used in the control law. To insure a safe and smooth navigation even when those parameters cannot be extracted, we design a &#64257;nite time state observer to estimate the visual features with the objective of maintaining an ef&#64257;cient control of the robot. Simulation and experimental tests validate the proposed approach.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat1_03">11:45-12:00, Paper TuAT1.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0685.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('685'); return false" title="Click to show or hide the keywords and abstract">How Behavior Trees Generalize the Teleo-Reactive Paradigm and And-Or-Trees</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#164882" title="Click to go to the Author Index">Colledanchise, Michele</a></td><td class="r">KTH - the Royal Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104031" title="Click to go to the Author Index">Ogren, Petter</a></td><td class="r">Royal Inst. of Tech. (KTH)</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab685" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Control_Architectures_and_Programming" title="Click to go to the Keyword Index">Control Architectures and Programming</a>, <a href="IROS16_KeywordIndexMedia.html#Behaviour_Based_Systems" title="Click to go to the Keyword Index">Behaviour-Based Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Agent_Based_Systems" title="Click to go to the Keyword Index">Agent-Based Systems</a></span><br>
                           <strong>Abstract:</strong> Behavior Trees (BTs) is a way of organizing the switching structure of a control system, that was originally developed in the computer gaming industry but is now also being used in robotics. The Teleo-Reactive programs (TRs) is a highly cited reactive hierarchical robot control approach suggested by Nilsson and And-Or-Trees are trees used for heuristic problems solving.<p>In this paper, we show that BTs generalize TRs as well as And-Or-Trees, even though the two concepts are quite different. And-Or-Trees are trees of conditions, and we show that they transform into a feedback execution plan when written as a BT. TRs are hierarchical control structures, and we show how every TR can be written as a BT. Furthermore, we show that so-called Universal TRs, guaranteeing that the goal will be reached, are a special case of so-called Finite Time Successful BTs. This implies that many designs and theoretical results developed for TRs can be applied to BTs.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat1_04">12:00-12:15, Paper TuAT1.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1089.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1089'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Task Level Robot Programming Using Prioritized Non-Linear Inequality Constraints</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#165403" title="Click to go to the Author Index">Somani, Nikhil</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114560" title="Click to go to the Author Index">Rickert, Markus</a></td><td class="r">Fortiss GmbH</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#145149" title="Click to go to the Author Index">Gaschler, Andre K.</a></td><td class="r">Fortiss Tech. Univ. Muenchen</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#159503" title="Click to go to the Author Index">Cai, Caixia</a></td><td class="r">Tech. Univ. München (TUM)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142250" title="Click to go to the Author Index">Perzylo, Alexander Clifford</a></td><td class="r">Fortiss GmbH - An-Inst. Tech. Univ. Muenchen</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105608" title="Click to go to the Author Index">Knoll, Alois</a></td><td class="r">Tech. Univ. Muenchen TUM</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1089" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1089.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Control_Architectures_and_Programming" title="Click to go to the Keyword Index">Control Architectures and Programming</a>, <a href="IROS16_KeywordIndexMedia.html#Industrial_Robots" title="Click to go to the Keyword Index">Industrial Robots</a></span><br>
                           <strong>Abstract:</strong> In this paper, we propose a framework for prioritized constraint-based specification of robot tasks. This framework is integrated with a cognitive robotic system based on semantic models of processes, objects, and workcells. The target is to enable intuitive (re-)programming of robot tasks, in a way that is suitable for non-expert users typically found in SMEs. Using CAD semantics, robot tasks are specified as geometric inter-relational constraints. During execution, these are combined with constraints from the environment and the workcell, and solved in real-time. Our constraint model and solving approach supports a variety of constraint functions that can be non-linear and also include bounds in the form of inequalities, e.g., geometric inter-relations, distance, collision avoidance and posture constraints. It is a hierarchical approach where priority levels can be specified for the constraints, and the nullspace of higher priority constraints is exploited to optimize the lower priority constraints. The presented approach has been applied to several typical industrial robotic use-cases to highlight its advantages compared to other state-of-the-art approaches.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuat2"><b>TuAT2</b></a></td>
               <td class="r">#102</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuat2" title="Click to go to the Program at a Glance"><b>Tracking</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#122966" title="Click to go to the Author Index">Liu, Yong</a></td><td class="r">Zhejiang Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#104345" title="Click to go to the Author Index">Bhattacharya, Sourabh</a></td><td class="r">Iowa State Univ</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat2_01">11:15-11:30, Paper TuAT2.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0653.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('653'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Robust Object Tracking with a Hierarchical Ensemble Framework</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191367" title="Click to go to the Author Index">Wang, Mengmeng</a></td><td class="r">Zhejiang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122966" title="Click to go to the Author Index">Liu, Yong</a></td><td class="r">Zhejiang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#113216" title="Click to go to the Author Index">Xiong, Rong</a></td><td class="r">Zhejiang Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab653" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0653.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Visual_Tracking" title="Click to go to the Keyword Index">Visual Tracking</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a></span><br>
                           <strong>Abstract:</strong> Autonomous robots enjoy a wide popularity nowadays and have been applied in many applications, such as home security, entertainment, delivery, navigation and guidance. It is vital for robots to track objects accurately in real time in these applications, so it is necessary to focus on tracking algorithms to improve the robustness, speed and accuracy. In this paper, we propose a real-time robust object tracking algorithm based on a hierarchical ensemble framework which incorporates information including individual pixel features, local patches and holistic target models. The framework combines multiple ensemble models simultaneously instead of using a single ensemble model individually. A discriminative model which accounts for the matching degree of local patches is adopted via a bottom ensemble layer, and a generative model which exploits holistic templates is used to search for the object based on the middle ensemble layer as well as an adaptive Kalman filter. We test the proposed tracker on challenging benchmark image sequences. The experimental results demonstrate that the proposed tracker performs superiorly against several state-of-the-art algorithms, especially when the appearance changes dramatically and the occlusions occur.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat2_02">11:30-11:45, Paper TuAT2.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0951.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('951'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Tracking a Moving Target in Cluttered Environments Using a Quadrotor</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#189048" title="Click to go to the Author Index">Chen, Jing</a></td><td class="r">Hong Kong Univ. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192354" title="Click to go to the Author Index">Liu, Tianbo</a></td><td class="r">Hong Kong Univ. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142354" title="Click to go to the Author Index">Shen, Shaojie</a></td><td class="r">Hong Kong Univ. of Science and Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab951" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0951.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Tracking" title="Click to go to the Keyword Index">Visual Tracking</a></span><br>
                           <strong>Abstract:</strong> We address the challenging problem of tracking a moving target in cluttered environments using a quadrotor. Our online trajectory planning method generates smooth, dynamically feasible, and collision-free polynomial trajectories that follow a visually-tracked moving target. As visual observations of the target are obtained, the target trajectory can be estimated and used to predict the target motion for a short time horizon. We propose a formulation to embed both limited horizon tracking error and quadrotor control costs in the cost function for a quadratic programming (QP), while encoding both collision avoidance and dynamical feasibility as linear inequality constraints for the QP. Our method generates tracking trajectories in the order of milliseconds and is therefore suitable for online target tracking with a limited sensing range. We implement our approach on-board a quadrotor testbed equipped with cameras, a laser range finder, an IMU, and onboard computing. Statistical analysis, simulation, and real-world experiments are conducted to demonstrate the effectiveness of our approach.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat2_03">11:45-12:00, Paper TuAT2.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1141.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1141'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Model-Based Tracking of Miniaturized Grippers Using Particle Swarm Optimization</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#123340" title="Click to go to the Author Index">Scheggi, Stefano</a></td><td class="r">Univ. of Twente</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190906" title="Click to go to the Author Index">ChangKyu, Yoon</a></td><td class="r">Johns Hopkins Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190910" title="Click to go to the Author Index">Gracias, David H.</a></td><td class="r">Department of Chemical and Biomolecular Engineering, the Johns H</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#120094" title="Click to go to the Author Index">Misra, Sarthak</a></td><td class="r">Univ. of Twente</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1141" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1141.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Visual_Tracking" title="Click to go to the Keyword Index">Visual Tracking</a></span><br>
                           <strong>Abstract:</strong> Micro-sized agents can benefit robotic minimally invasive surgery since they can be inserted into the human body and use natural pathways such as arteries and veins or the gastrointestinal tract, to reach their target for drug delivery or diagnosis. Recently, miniaturized agents with shape-changing and gripping capabilities have provided significant advantages in performing grasping, transportation, and manipulation tasks. In order to robustly perform such tasks, it is of utmost importance to properly estimate their overall configuration. This paper presents a novel solution to the problem of estimating and tracking the 3D position, orientation and configuration of the tips of miniaturized grippers from RGB marker-less visual observations obtained by a microscope. We consider this as an optimization problem, seeking for the gripper model parameters that minimize the discrepancy between hypothesized instances of the gripper model and actual observations of the miniaturized gripper. This optimization problem is solved using a variant of the Particle Swarm Optimization algorithm. The proposed approach has been evaluated on several image sequences showing the grippers moving, rotating, opening/closing and grasping biological material.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat2_04">12:00-12:15, Paper TuAT2.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1145.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1145'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Target Tracking on Triangulation Graphs</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#167267" title="Click to go to the Author Index">Laguna, Guillermo</a></td><td class="r">Iowa State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#165829" title="Click to go to the Author Index">Zou, Rui</a></td><td class="r">Iowa State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104345" title="Click to go to the Author Index">Bhattacharya, Sourabh</a></td><td class="r">Iowa State Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1145" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1145.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Surveillance_Systems" title="Click to go to the Keyword Index">Surveillance Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Tracking" title="Click to go to the Keyword Index">Visual Tracking</a></span><br>
                           <strong>Abstract:</strong> We investigate a variation of the art gallery problem in which a team of mobile guards tries to track an unpredictable intruder in a simply-connected polygonal environment. The guards are confined to move along the diagonals of a polygon, and are deployed according to the strategy proposed in [1] that provides an upper bound of <i>n/4</i> mobile guards to cover a simply-connected polygonal environment. We introduce the concept of critical regions to generate event-triggered strategies for the guards. Based on these strategies, we present sufficient conditions for <i>n/4</i> guards to track an unpredictable mobile intruder forever.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuat3"><b>TuAT3</b></a></td>
               <td class="r">#103</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuat3" title="Click to go to the Program at a Glance"><b>Recognition</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#137474" title="Click to go to the Author Index">Mertsching, Bärbel</a></td><td class="r">Univ. of Paderborn</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#159480" title="Click to go to the Author Index">Ghalamzan Esfahani, Amir Masoud</a></td><td class="r">Univ. of Birmingham</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat3_01">11:15-11:30, Paper TuAT3.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0495.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('495'); return false" title="Click to show or hide the keywords and abstract">Illumination Invariant Representation of Natural Images for Visual Place Recognition</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160503" title="Click to go to the Author Index">Shakeri, Moein</a></td><td class="r">Univ. of Alberta</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100155" title="Click to go to the Author Index">Zhang, Hong</a></td><td class="r">Univ. of Alberta</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab495" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Navigation" title="Click to go to the Keyword Index">Visual Navigation</a></span><br>
                           <strong>Abstract:</strong> Illumination changes are a typical problem for many outdoor long-term applications such as visual place recognition. Keypoints may fail to match between images taken at the same location but different times of the day. Although recently some methods are presented for creating shadow-free image representations, all of them have the limitation in terms of dealing with night images and non-Planckian source of lighting. In this paper we present a new method to creating illumination invariant image representation using a combination of two existing methods based on natural image statistics that address the issue of illumination invariance. Unlike previous attempts at solving the problem of illumination invariant representation, the proposed method does not assume the ideal narrow-band color camera nor a calibration step for each environment. We evaluate our method on real datasets to establish its accuracy and efficiency. Experimental results show that our method outperforms competing methods for illumination invariant image representation.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat3_02">11:30-11:45, Paper TuAT3.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0963.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('963'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>3D Graph Based Stairway Detection and Localization for Mobile Robots</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196014" title="Click to go to the Author Index">Westfechtel, Thomas</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107167" title="Click to go to the Author Index">Ohno, Kazunori</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#137474" title="Click to go to the Author Index">Mertsching, Bärbel</a></td><td class="r">Univ. of Paderborn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196114" title="Click to go to the Author Index">Nickchen, Daniel</a></td><td class="r">Paderborn Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196290" title="Click to go to the Author Index">Kojima, Shotaro</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100118" title="Click to go to the Author Index">Tadokoro, Satoshi</a></td><td class="r">Tohoku Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab963" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0963.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Climbing_Robots" title="Click to go to the Keyword Index">Climbing Robots</a></span><br>
                           <strong>Abstract:</strong> Perception is the main key to enable robots to react to and interact with its environment. Especially for multi-floor operations, the robot has to robustly detect and localize stairs in order to allow for a safe climbing. In this paper, we develop a graph based stairway detection method for point cloud data, that is able to detect a large variety of stairways. Our approach first segments planar regions and extracts the stair tread and stair riser shaped segments. Using these segments a dynamic graph model is initialized that is used to detect stairs including the railing system in the surroundings. We show that our system can accurately detect and localize different stairways from a variety of different positions, including descending stairs. Our system's accuracy is higher than most state-of-the-art stairway detection methods even in case of sparse point cloud data.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat3_03">11:45-12:00, Paper TuAT3.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1018.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1018'); return false" title="Click to show or hide the keywords and abstract">User-Adaptive Fall Detection for Patients Using Wristband</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#182254" title="Click to go to the Author Index">Nho, Young-Hoon</a></td><td class="r">KAIST, HRI Res. Center</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118328" title="Click to go to the Author Index">Lim, Jong Gwan</a></td><td class="r">Univ. of Trento</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#194771" title="Click to go to the Author Index">Kim, Dae-Eon</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100050" title="Click to go to the Author Index">Kwon, Dong-Soo</a></td><td class="r">KAIST</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1018" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a></span><br>
                           <strong>Abstract:</strong> Fall detection systems have been proposed to prevent additional injuries following fall accidents. This paper introduces an easily learnable fall detection system based on the data of an individual patient in a hospital room. The improvement of low performance using a single accelerometer at wrists and the inconvenience of sensor attached to a waist in the conventional approach was concentrated on by integrating heart rate signals to the conventional acceleration approach and changing the sensor location from a waist to wrists. As for the optimal heart rate feature selection, we proposed a four-feature vector combination (root mean square of successive differences, standard deviation of successive differences, normal to normal 50, normal to normal 20) with correlation and mutual information analysis in addition to mean absolute deviation selected as an accelerometer feature. To easily acquire and train the patients’ fall data, our system was based on unsupervised learning approaches using Gaussian mixture models for optimal classifiers with the optimal cluster number decided by cluster validation index of square error sum. A 10-fold cross validation was applied for a final performance evaluation where each threshold for separating fall state from non-fall state was automatically decided in several comparison groups, which were created on the basis of fusion timing and used sensors. As a result, despite sensors attached to the wrist, the wearable inconvenience of the conventional is overcome using the feature-level fused approach between heart rates and accelerations with the accuracy up to 98.39 %, which is closest to 99.34 % of the case using a single accelerometer located at the waist.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat3_04">12:00-12:15, Paper TuAT3.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1260.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1260'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Effective Place Scene Clustering Using Straight Lines</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184262" title="Click to go to the Author Index">Moon, Hyewon</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#154706" title="Click to go to the Author Index">Lee, Jin Han</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#150665" title="Click to go to the Author Index">Lee, Sehyung</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102631" title="Click to go to the Author Index">Suh, Il Hong</a></td><td class="r">Hanyang Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1260" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1260.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Navigation" title="Click to go to the Keyword Index">Visual Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a></span><br>
                           <strong>Abstract:</strong> In this paper, we propose a scene clustering algorithm which uses straight line features. Scenes are represented as nodes in the graph, and each connectivity between nodes is calculated by a pre-trained vocabulary tree. By applying a spectral clustering algorithm to the constructed graph, the scenes are partitioned into k groups where k is determined by the proposed estimation method. Instead of using the standard eigenvalue analysis, the optimal k is computed so that the partitioned graph becomes to have strong intra-class correlations while inter-class correlations are relatively weak. As a result, scenes are adaptively clustered according to the environmental changes. The clustering performance of the proposed method is quantitatively evaluated with three image sequences captured in challenging environments. Experimental comparisons demonstrate that our line-based algorithm outperforms existing algorithms utilizing other types of features as well as the produced scene clustering results are more human-like.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuat4"><b>TuAT4</b></a></td>
               <td class="r">#104</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuat4" title="Click to go to the Program at a Glance"><b>Medical Robot and Systems 1</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#192460" title="Click to go to the Author Index">Hennersperger, Christoph</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#149133" title="Click to go to the Author Index">Wu, Liao</a></td><td class="r">Queensland Univ. of Tech</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat4_01">11:15-11:30, Paper TuAT4.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0099.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('99'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Using Contours As Boundary Conditions for Elastic Registration During Minimally Invasive Hepatic Surgery</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168203" title="Click to go to the Author Index">Haouchine, Nazim</a></td><td class="r">INRIA</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195273" title="Click to go to the Author Index">Roy, Frederick</a></td><td class="r">INRIA</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195274" title="Click to go to the Author Index">Untereiner, Lionel</a></td><td class="r">Inria</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107722" title="Click to go to the Author Index">Cotin, Stephane</a></td><td class="r">INRIA</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab99" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0099.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Soft_tissue_Modeling" title="Click to go to the Keyword Index">Soft-tissue Modeling</a></span><br>
                           <strong>Abstract:</strong> We address in this paper the ill-posed problem of initial alignment of pre-operative to intra-operative data for augmented reality during minimally invasive hepatic surgery. This problem consists on finding the rigid transformation that relates the scanning reference and the endoscopic camera pose, and the non-rigid transformation that undergone the liver w.r.t its scanned rest configuration. Where most of the related works assume a known initial registration, we propose a method that permits to recover the deformation undergone by the liver while simultaneously finding the rotational and translational parts of the transformation. Our formulation considers the boundaries of the liver with its surrounding tissues as hard constraints directly encoded in an energy minimization process. We expose experiments on real in-vivo data of human hepatic surgery and synthetic data, and compare our method with related works.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat4_02">11:30-11:45, Paper TuAT4.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0109.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('109'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Towards Hybrid Control of a Flexible Curvilinear Surgical Robot with Visual/Haptic Guidance</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#149133" title="Click to go to the Author Index">Wu, Liao</a></td><td class="r">Queensland Univ. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184527" title="Click to go to the Author Index">Wu, Keyu</a></td><td class="r">National Univ. of Singapore</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106795" title="Click to go to the Author Index">Ren, Hongliang</a></td><td class="r">Faculty of Engineering, National Univ. of Singapore</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab109" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0109.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Servoing" title="Click to go to the Keyword Index">Visual Servoing</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a></span><br>
                           <strong>Abstract:</strong> Comprised of multiple telescoptic precurved tubes that can independently rotate and translate, concentric tube robots (CTRs) are favorable in minimally invasive surgeries thanks to their small size and considerable dexterity along with curvilinear accessibility. However, there is a lack of nvestigation on improvement of the surgeons’ perception which in turn can be used to guide the telemanipulation. In this work, we proposed an eye-in-hand configuration for the concentric tube robot by adding an endoscope to the tip of the inner tube, which provides direct and intuitive visual sensing ability for the operator. Based on this visual feedback, we further developed two frameworks for the hybrid control of CTR, namely Teleoperation Before Visual Servoing (TBVS) and Teleoperation During Visual Servoing (TDVS). The structures of these two frameworks were elaborated with key algorithms derived. The effectiveness of the proposed methods were demonstrated through a series of experiments both in free space and in a confined environment (inside a skull model). The results manifested that the visual guidance had the potential of assisting the operator to control the CTR more efficiently.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat4_03">11:45-12:00, Paper TuAT4.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0419.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('419'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Automatic Force-Compliant Robotic Ultrasound Screening of Abdominal Aortic Aneurysms</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195282" title="Click to go to the Author Index">Virga, Salvatore</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190122" title="Click to go to the Author Index">Zettinig, Oliver</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192065" title="Click to go to the Author Index">Esposito, Marco</a></td><td class="r">Computer Aided Medical Procedures, Tech. Univ. Münche</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195771" title="Click to go to the Author Index">Pfister, Karin</a></td><td class="r">Univ. Medical Center Regensburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192072" title="Click to go to the Author Index">Frisch, Benjamin</a></td><td class="r">Computer Aided Medical Procedures, Tech. Univ. Münche</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195774" title="Click to go to the Author Index">Neff, Thomas</a></td><td class="r">KUKA Roboter GmbH</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107647" title="Click to go to the Author Index">Navab, Nassir</a></td><td class="r">TU Munich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192460" title="Click to go to the Author Index">Hennersperger, Christoph</a></td><td class="r">Tech. Univ. München</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab419" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0419.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Servoing" title="Click to go to the Keyword Index">Visual Servoing</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Detection_and_Tracking" title="Click to go to the Keyword Index">Human Detection and Tracking</a></span><br>
                           <strong>Abstract:</strong> Ultrasound (US) imaging is commonly employed for the diagnosis and staging of abdominal aortic aneurysms (AAA), mainly due to its non-invasiveness and high availability. High inter-operator variability and a lack of repeatability of current US image acquisition impair the implementation of extensive screening programs for affected patient populations. However, this opens the way to a possible automation of the procedure, and recent works have exploited the use of robotic platforms for US applications, both in diagnostic and interventional scenarios. In this work, we propose a system for autonomous robotic US acquisitions aimed at the quantitative assessment of patients’ vessel diameter for abdominal aortic aneurysm screening. Using a probabilistic measure of the US quality, we introduce an automatic estimation of the optimal pressure to be applied during the acquisition, and an online optimization of the out-of-plane rotation of the US probe to maximize the visibility of the aorta. We evaluate our method on healthy volunteers and compare the results to manual acquisitions performed by a clinical expert, demonstrating the feasibility of the presented system for AAA screening.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat4_04">12:00-12:15, Paper TuAT4.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0513.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('513'); return false" title="Click to show or hide the keywords and abstract">System Design and Development of a Robotic Device for Automated Venipuncture and Diagnostic Blood Cell Analysis</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#176690" title="Click to go to the Author Index">Balter, Max</a></td><td class="r">Rutgers Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#177842" title="Click to go to the Author Index">Chen, Alvin</a></td><td class="r">Rutgers Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195285" title="Click to go to the Author Index">Fromholtz, Alexander</a></td><td class="r">Rutgers Univ. Department of Biomedical Engineering</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195303" title="Click to go to the Author Index">Gorshkov, Alexander</a></td><td class="r">Rutgers</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183295" title="Click to go to the Author Index">Maguire, Tim</a></td><td class="r">VascuLogic LLC</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178506" title="Click to go to the Author Index">Yarmush, Martin</a></td><td class="r">Rutgers Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab513" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a></span><br>
                           <strong>Abstract:</strong> Diagnostic blood testing is the most prevalent medical procedure performed in the world and forms the cornerstone of modern health care delivery. Yet blood tests are still predominantly carried out in centralized labs using large-volume samples acquired by manual venipuncture, and no end-to-end solution from blood draw to sample analysis exists today. Our group is developing a platform device that merges robotic phlebotomy with automated diagnostics to rapidly deliver patient information at the site of the blood draw. The system couples an image-guided venipuncture robot, designed to address the challenges of routine venous access, with a centrifuge-based blood analyzer to obtain quantitative measurements of hematology. In this paper, we first present the system design and architecture of the integrated device. We then perform a series of in vitro experiments to evaluate the cannulation accuracy of the system on blood vessel phantoms. Next, we assess the effects of vessel diameter, needle gauge, flow rate, and viscosity on the rate of sample collection. Finally, we demonstrate proof-of-concept of a white cell assay on the blood analyzer using in vitro human samples spiked with fluorescently labeled microbeads.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuat5"><b>TuAT5</b></a></td>
               <td class="r">#105</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuat5" title="Click to go to the Program at a Glance"><b>Actuator</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#100766" title="Click to go to the Author Index">Suzumori, Koichi</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#110966" title="Click to go to the Author Index">Carloni, Raffaella</a></td><td class="r">Univ. of Twente</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat5_01">11:15-11:30, Paper TuAT5.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0671.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('671'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Controlling a Multi-Joint Arm Actuated by Pneumatic Muscles with Quasi-DDP Optimal Control</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#175590" title="Click to go to the Author Index">Kumar Hari Shankar Lal Das, Ganesh</a></td><td class="r">LAAS/CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117589" title="Click to go to the Author Index">Tondu, Bertrand</a></td><td class="r">Univ. of Toulouse</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#116328" title="Click to go to the Author Index">Manhes, Jérôme</a></td><td class="r">LAAS-CNRS, Univ. De Toulouse, CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#202210" title="Click to go to the Author Index">Forget, Florent</a></td><td class="r">LAAS-CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103148" title="Click to go to the Author Index">Stasse, Olivier</a></td><td class="r">CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107187" title="Click to go to the Author Index">Soueres, Philippe</a></td><td class="r">LAAS-CNRS</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab671" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0671.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Hydraulic_Pneumatic_Actuators" title="Click to go to the Keyword Index">Hydraulic/Pneumatic Actuators</a>, <a href="IROS16_KeywordIndexMedia.html#Optimal_Control" title="Click to go to the Keyword Index">Optimal Control</a>, <a href="IROS16_KeywordIndexMedia.html#Biologically_Inspired_Robots" title="Click to go to the Keyword Index">Biologically-Inspired Robots</a></span><br>
                           <strong>Abstract:</strong> Pneumatic actuators have inherent compliance and hence they are very interesting for applications involving interaction with environment or human. But controlling such kind of actuators is not trivial. The paper presents an implementation	of iterative Linear Quadratic regulator (iLQR) based optimal control framework to control an anthropomorphic arm with each joint actuated by an agonist-antagonistic pair of Mckibben artificial muscles. The method is applied to positioning tasks and generation of explosive movements by maximizing the link speed. It is then compared to traditional control strategies to justify that optimal control is effective in controlling the position in highly non-linear pneumatic systems. Also the importance of varying compliance is highlighted by repeating the tasks at different compliance level. The algorithm validation is reported here by several simulations and hardware experiments in which the shoulder and elbow flexion are controlled simultaneously.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat5_02">11:30-11:45, Paper TuAT5.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0684.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('684'); return false" title="Click to show or hide the keywords and abstract">Modeling and Benchmarking Energy Efficiency of Variable Stiffness Actuators on the Example of the DLR FSJ</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114235" title="Click to go to the Author Index">Wolf, Sebastian</a></td><td class="r">DLR - German Aerospace Center</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196146" title="Click to go to the Author Index">Feenders, Jan-Emmo</a></td><td class="r">DLR - German Aerospace Center</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab684" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Variable_Stiffness_Actuator_Design_and_Control" title="Click to go to the Keyword Index">Variable Stiffness Actuator Design and Control</a>, <a href="IROS16_KeywordIndexMedia.html#Joint_Mechanism_Design" title="Click to go to the Keyword Index">Joint/Mechanism Design</a>, <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a></span><br>
                           <strong>Abstract:</strong> Robots with Variable Stiffness Actuators (VSA) are intrinsically flexible in the joints. The built-in mechanical spring has the advantage of a higher peak performance, in some extend increased safety for humans interacting physically with the robot, and promises a more energy efficient robot for certain trajectories. This paper shows the modeling process of a VSA including energy losses on the example of the DLR Floating Spring Joint (FSJ). The model includes the full actuator dynamics with losses in electromechanical transformation of the motors. Furthermore, it models bearing and gear friction with stiction, Coulomb friction, viscous friction, and load dependent effects. With the obtained model the energy losses of benchmark trajectories are investigated and compared with a comparable stiff joint actuator.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat5_03">11:45-12:00, Paper TuAT5.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1158.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1158'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Elastic Energy Storage in Leaf Springs for a Lever-Arm Based Variable Stiffness Actuator</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#150933" title="Click to go to the Author Index">Barrett, Eamon</a></td><td class="r">Univ. of Twente</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103432" title="Click to go to the Author Index">Fumagalli, Matteo</a></td><td class="r">Aalborg Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110966" title="Click to go to the Author Index">Carloni, Raffaella</a></td><td class="r">Univ. of Twente</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1158" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1158.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Variable_Stiffness_Actuator_Design_and_Control" title="Click to go to the Keyword Index">Variable Stiffness Actuator Design and Control</a></span><br>
                           <strong>Abstract:</strong> The increasing use of Variable Stiffness Actuators (VSAs) in robotic joints is helping robots to meet the demands of human-robot interaction, requiring high safety and adaptability. The key feature of a VSA is the ability to exploit internal elastic elements to obtain a variable output stiffness. These allow the joints to store mechanical energy supplied through interaction with the environment and make the system more robust, efficient, and safe. This paper discusses the design of leaf springs for a sub-class of VSAs that use variable lever arm ratios as means to change their output stiffness. Given the trade-off between compactness and the maximum energy storage capacity, the internal springs’ dimensions and material choice are assessed through a theoretical analysis and practical experiments.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat5_04">12:00-12:15, Paper TuAT5.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1521.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1521'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Untethered Three-Arm Pneumatic Robot Using Hose-Free Pneumatic Actuator</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184182" title="Click to go to the Author Index">Kitamori, Takaaki</a></td><td class="r">Tokyo Inst. of Tech. Department of Mechanical and Aero</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#159360" title="Click to go to the Author Index">Wada, Akira</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190813" title="Click to go to the Author Index">Nabae, Hiroyuki</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100766" title="Click to go to the Author Index">Suzumori, Koichi</a></td><td class="r">Tokyo Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1521" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1521.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Soft_bodied_Robots" title="Click to go to the Keyword Index">Soft-bodied Robots</a>, <a href="IROS16_KeywordIndexMedia.html#New_Actuators" title="Click to go to the Keyword Index">New Actuators</a>, <a href="IROS16_KeywordIndexMedia.html#Hydraulic_Pneumatic_Actuators" title="Click to go to the Keyword Index">Hydraulic/Pneumatic Actuators</a></span><br>
                           <strong>Abstract:</strong> Compressors and air hoses limit the mobility of mobile robots and the use of pneumatic actuators such that small pneumatic actuators, which can be driven without compressors, are strongly desired. We have been developing a new hose-free pneumatic actuator based on reversible gas/liquid chemical reactions. This novel pneumatic actuator neither requires compressors nor air hoses for its source of power because its pressure source is in the actuator itself. In this paper, we present a way to estimate gas generation speed from both theoretical and experimental aspects. We also present a new untethered soft robot, which has this actuator located internally. This robot has three hands and can move them up in 16 s and down in 35 s.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuat6"><b>TuAT6</b></a></td>
               <td class="r">#106</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuat6" title="Click to go to the Program at a Glance"><b>Underactuated Robots</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#124103" title="Click to go to the Author Index">Yu, Hongnian</a></td><td class="r">Bournemouth Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#150828" title="Click to go to the Author Index">Farnioli, Edoardo</a></td><td class="r">Univ. Di Pisa</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat6_01">11:15-11:30, Paper TuAT6.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0453.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('453'); return false" title="Click to show or hide the keywords and abstract">Modelling and Dynamic Analysis of Underactuated Capsule Systems with Friction-Induced Hysteresis</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192495" title="Click to go to the Author Index">Liu, Pengcheng</a></td><td class="r">Bournemouth Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#124103" title="Click to go to the Author Index">Yu, Hongnian</a></td><td class="r">Bournemouth Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192496" title="Click to go to the Author Index">Cang, Shuang</a></td><td class="r">Bournemouth Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab453" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Micro_Nano_Robots" title="Click to go to the Keyword Index">Micro/Nano Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Dynamics" title="Click to go to the Keyword Index">Dynamics</a></span><br>
                           <strong>Abstract:</strong> This paper studies modelling and dynamic analysis of underactuated capsule systems exhibiting friction-induced hysteresis. The motion mechanism is novel in utilizing internal centripetal torques generated by a vibration micro-motor mounted on the platform. Up to now, most investigations in frictional interactions towards capsule systems were confined into static or quasi-dynamic circumstance, where it is difficult to facilitate online use and control. It is the first time the dynamic frictional characteristics (non-reversible drooping and hysteresis) are studied towards these systems. An analytical study is primarily conducted to reveal the non-reversible characteristic for the static friction, the pre-sliding regime as well as the pure sliding regime, and the frictional limit boundaries are identified. Subsequently, the studies are mainly focused on dynamic analysis, including friction-driven vibrational responses and qualitative changes induced by control parameter (mass ratio) in capsule dynamics. It is found that the models predict periodic responses for the parameters considered and the average capsule velocity can be controlled through proper tuning of the control parameter around identified control points. The results demonstrate good captions of experimentally observed frictional characteristics, quenching of friction-induced vibrations and satisfaction of energy requirements.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat6_02">11:30-11:45, Paper TuAT6.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0921.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('921'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Mechanics-Based Control of Underactuated 3D Robotic Walking: Dynamic Gait Generation under Torque Constraints</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#151224" title="Click to go to the Author Index">Powell, Matthew</a></td><td class="r">Georgia Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#134049" title="Click to go to the Author Index">Ames, Aaron</a></td><td class="r">Georgia Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab921" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0921.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Humanoid_and_Bipedal_Locomotion" title="Click to go to the Keyword Index">Humanoid and Bipedal Locomotion</a></span><br>
                           <strong>Abstract:</strong> This paper presents a novel method of stabilizing hybrid models of torque-constrained, underactuated walking robots -- without using nonlinear gait optimization -- by leveraging properties of the mechanics of the robot. At its core, the controller stabilizes the transfer of angular momentum from one leg to the next through continuous-time control coupled with hybrid system models that capture impacts that occur at foot strike. In particular, conservation of angular momentum at impact allows for computation of the exact transfer of momentum as a function of the robot's step length and vertical center of mass velocity just prior to foot impact. This motivates the construction of continuous-time reference trajectories for the robot's step length and vertical center of mass with endpoints corresponding to a desired transfer of angular momentum. Stabilization to these trajectories results in stable walking, as indicated by numeric Poincar'{e} analysis. The controller is implemented in simulation of a five-link, underactuated 3D robot via Model Predictive Control which provides a means of achieving walking under non-trivial actuation limits.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat6_03">11:45-12:00, Paper TuAT6.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1039.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1039'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Differential Flatness and Control of Protocentric Aerial Manipulators with Any Number of Arms and Mixed Rigid-/Elastic-Joints</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169311" title="Click to go to the Author Index">Yuksel, Burak</a></td><td class="r">Max Planck Inst. for Biological Cybernetics</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171719" title="Click to go to the Author Index">Buondonno, Gabriele</a></td><td class="r">Sapienza Univ. of Rome</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104988" title="Click to go to the Author Index">Franchi, Antonio</a></td><td class="r">LAAS-CNRS</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1039" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1039.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a>, <a href="IROS16_KeywordIndexMedia.html#Manipulation_Planning_and_Control" title="Click to go to the Keyword Index">Manipulation Planning and Control</a></span><br>
                           <strong>Abstract:</strong> In this paper we introduce a particularly relevant class of aerial manipulators that we name protocentric. These robots are formed by an underactuated aerial vehicle, a planar-Vertical Take-Off and Landing (PVTOL), equipped with any number of different parallel manipulator arms with the only property that all the first joints are attached at the Center of Mass (CoM) of the PVTOL, while the center of actuation of the PVTOL can be anywhere. We prove that protocentric aerial manipulators (PAMs) are differentially flat systems regardless the number of joints of each arm and their kinematic and dynamic parameters. The set of flat outputs is constituted by the CoM of the PVTOL and the absolute orientation angles of all the links. The relative degree of each output is equal to four. More amazingly, we prove that protocentric aerial manipulators are differentially flat even in the case that any number of the joints are elastic, no matter the internal distribution between elastic and rigid joints. The set of flat outputs is the same but in this case the total relative degree grows quadratically with the number of elastic joints. We validate the theory by simulating object grasping and transportation tasks with unknown mass and parameters and using a controller based on dynamic feedback linearization.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat6_04">12:00-12:15, Paper TuAT6.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1267.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1267'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Frontal Plane Stabilization and Hopping with a 2DOF Tail</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#174419" title="Click to go to the Author Index">Wenger, Garrett</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#143651" title="Click to go to the Author Index">De, Avik</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#108300" title="Click to go to the Author Index">Koditschek, Daniel</a></td><td class="r">Univ. of Pennsylvania</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1267" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1267.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Legged_Robots" title="Click to go to the Keyword Index">Legged Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Biologically_Inspired_Robots" title="Click to go to the Keyword Index">Biologically-Inspired Robots</a></span><br>
                           <strong>Abstract:</strong> The Jerboa, a tailed bipedal robot with two hip-actuated, passive-compliant legs and a doubly actuated tail, has been shown both formally and empirically to exhibit a variety of stable hopping and running gaits in the sagittal plane. In this paper we take the first steps toward operating Jerboa as a fully spatial machine by addressing the predominant mode of destabilization away from the sagittal plane: body roll. We develop a provably stable controller for underactuated aerial stabilization of the coupled body roll and tail angles, that uses just the tail torques. We show that this controller is successful at reliably reorienting the Jerboa body in roughly 150 ms of freefall from a large set of initial conditions. This controller also enables (and, indeed, appears intuitively to be crucial for) sustained empirically stable hopping in the frontal plane by virtue of its substantial robustness against destabilizing perturbations and calibration errors. The controller as well as the analysis methods developed here are applicable to any robotic platform with a similar doubly-actuated spherical tail joint.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuat7"><b>TuAT7</b></a></td>
               <td class="r">#107</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuat7" title="Click to go to the Program at a Glance"><b>Perception for Grasping and Manipulation</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#135120" title="Click to go to the Author Index">Zhang, Yizhai</a></td><td class="r">Northwestern Pol. Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#105000" title="Click to go to the Author Index">Sasaki, Yoko</a></td><td class="r">National Inst. of Advanced Industrial Science and Tech</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat7_01">11:15-11:30, Paper TuAT7.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0510.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('510'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Incremental Scene Understanding on Dense SLAM</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191771" title="Click to go to the Author Index">Li, Chi</a></td><td class="r">Johns Hopkins Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195895" title="Click to go to the Author Index">Xiao, Han</a></td><td class="r">Johns Hopkins Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#182196" title="Click to go to the Author Index">Tateno, Keisuke</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#123306" title="Click to go to the Author Index">Tombari, Federico</a></td><td class="r">Univ. of Bologna</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107647" title="Click to go to the Author Index">Navab, Nassir</a></td><td class="r">TU Munich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104128" title="Click to go to the Author Index">Hager, Gregory</a></td><td class="r">Johns Hopkins Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab510" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0510.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a></span><br>
                           <strong>Abstract:</strong> We present an architecture for online, incremental scene modeling which combines a SLAM-based scene understanding framework with semantic segmentation and object pose estimation. The core of this approach comprises a probabilistic inference scheme that predicts semantic labels for object hypotheses at each new frame. From these hypotheses, recognized scene structures are incrementally constructed and tracked. Semantic labels are inferred using a multi-domain convolutional architecture which operates on the image time series and which enables efficient propagation of features and robust model registration. To evaluate this architecture, we introduce a large-scale RGB-D dataset JHUSEQ-25 as a new benchmark for the sequence-based scene understanding in complex and densely cluttered scenes. This dataset contains 25 RGB-D video sequences with 100,000 labeled frames in total. We validate our method on this dataset and demonstrate improved performance of semantic segmentation and 6-DoF object pose estimation compared with methods based on the single view.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat7_02">11:30-11:45, Paper TuAT7.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0584.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('584'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Active Exploration Using Gaussian Random Fields and Gaussian Process Implicit Surfaces</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#181988" title="Click to go to the Author Index">Caccamo, Sergio</a></td><td class="r">KTH Royal Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#133489" title="Click to go to the Author Index">Bekiroglu, Yasemin</a></td><td class="r">Univ. of Birmingham</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140071" title="Click to go to the Author Index">Ek, Carl Henrik</a></td><td class="r">Univ. of Bristol</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101716" title="Click to go to the Author Index">Kragic, Danica</a></td><td class="r">KTH</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab584" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0584.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Force_and_Tactile_Sensing" title="Click to go to the Keyword Index">Force and Tactile Sensing</a></span><br>
                           <strong>Abstract:</strong> In this work we study the problem of exploring surfaces and building compact 3D representations of the environment surrounding a robot through active perception. We propose an online probabilistic framework that merges visual and tactile measurements using Gaussian Random Field and Gaussian Process Implicit Surfaces. The system investigates incomplete point clouds in order to find a small set of regions of interest which are then physically explored with a robotic arm equipped with tactile sensors. We show experimental results obtained using a PrimeSense camera, a Kinova Jaco2 robotic arm and Optoforce sensors on different scenarios. We then demostrate how to use the online framework for object detection and terrain classification.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat7_03">11:45-12:00, Paper TuAT7.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0799.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('799'); return false" title="Click to show or hide the keywords and abstract">Object Proposal Using 3D Point Cloud for DRC-HUBO+</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#147515" title="Click to go to the Author Index">Shin, Seunghak</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#155961" title="Click to go to the Author Index">Shim, Inwook</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#147235" title="Click to go to the Author Index">Jung, Jiyoung</a></td><td class="r">Naver Labs</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103397" title="Click to go to the Author Index">Bok, Yunsu</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102941" title="Click to go to the Author Index">Oh, Jun Ho</a></td><td class="r">Korea Advanced Inst. of Sci. and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101760" title="Click to go to the Author Index">Kweon, In So</a></td><td class="r">KAIST</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab799" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a></span><br>
                           <strong>Abstract:</strong> We present an object proposal method which utilizes the 3D data obtained from a depth sensor as well as the color information of images. Our object proposal method is designed to improve the performance of the object detection for a mobile robot equipped with a camera and a laser scanner. Compared to traditional object proposal methods using only 2D images, the proposed method provides much less number of candidate windows for object detection. We show less than 100 object proposal windows per image using the proposed method result in high recall tested on the public dataset. Our method presents object proposals in 3D space as well as in 2D image thus it can further be applied to following tasks for mobile robots such as 3D location and pose estimation of the target object after successful object detection. We validate our method using the real-world object detection dataset for outdoor mobile robots captured during the DRC Finals 2015 and the public dataset for comparison with the previous methods.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat7_04">12:00-12:15, Paper TuAT7.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1058.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1058'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>High Precision Grasp Pose Detection in Dense Clutter</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196458" title="Click to go to the Author Index">Gualtieri, Marcus</a></td><td class="r">Northeastern Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#166531" title="Click to go to the Author Index">ten Pas, Andreas</a></td><td class="r">Northeastern Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#147893" title="Click to go to the Author Index">Saenko, Kate</a></td><td class="r">ICSI & UC Berkeley EECS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106889" title="Click to go to the Author Index">Platt, Robert</a></td><td class="r">Northeastern Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1058" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1058.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Learning" title="Click to go to the Keyword Index">Robot Learning</a></span><br>
                           <strong>Abstract:</strong> This paper considers the problem of grasp pose detection in point clouds. We follow a general algorithmic structure that first generates a large set of 6-DOF grasp candidates and then classifies each of them as a good or a bad grasp. Our focus in this paper is on improving the second step by using depth sensor scans from large online datasets to train a convolutional neural network. We propose two new representations of grasp candidates, and we quantify the effect of using prior knowledge of two forms: instance or category knowledge of the object to be grasped, and pretraining the network on simulated depth data obtained from idealized CAD models. Our analysis shows that a more informative grasp candidate representation as well as pretraining and prior knowledge significantly improve grasp detection. We evaluate our approach on a Baxter Research Robot and demonstrate an average grasp success rate of 93% in dense clutter. This is a 20% improvement compared to our prior work.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuat8"><b>TuAT8</b></a></td>
               <td class="r">#108</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuat8" title="Click to go to the Program at a Glance"><b>Model Learning</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#155614" title="Click to go to the Author Index">del Pobil, Angel P.</a></td><td class="r">Jaume-I Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#129631" title="Click to go to the Author Index">Zeeshan, Arif Muhammad</a></td><td class="r">ETH Zurich</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat8_01">11:15-11:30, Paper TuAT8.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0149.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('149'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Generalizing a Learned Inverse Dynamic Model of KUKA LWR IV+ for Load Variations Using Regression in the Model Space</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168256" title="Click to go to the Author Index">Shareef, Zeeshan</a></td><td class="r">Univ. of Bielefeld</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#154437" title="Click to go to the Author Index">Reinhart, Rene Felix</a></td><td class="r">Fraunhofer-Gesellschaft</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104760" title="Click to go to the Author Index">Steil, Jochen J.</a></td><td class="r">Bielefeld Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab149" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0149.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Learning" title="Click to go to the Keyword Index">Robot Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Dynamics" title="Click to go to the Keyword Index">Dynamics</a>, <a href="IROS16_KeywordIndexMedia.html#Model_Learning" title="Click to go to the Keyword Index">Model Learning</a></span><br>
                           <strong>Abstract:</strong> In this paper, we show the generalization of an inverse dynamic model for KUKA LWR IV+ under load mass variations. We use a modular approach based on regression in the model space. First, inverse dynamic models for the known masses are learned using a recently proposed approach called Independent Joint Learning (IJL). In IJL the torque errors due to unmodeled dynamics of the real robot are estimated using only joint-local information. Second, a mapping from load mass to model parameters of torque error model is learned in order to generalize the inverse dynamics to new load masses. The modular approach improves the accuracy of an existing KUKA LWR IV+ inverse dynamic model. The results are compared with a single step IJL approach. The results show the excellent generalization for new load masses using regression in the model space.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat8_02">11:30-11:45, Paper TuAT8.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0733.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('733'); return false" title="Click to show or hide the keywords and abstract">A Reservoir Computing Approach for Learning Forward Dynamics of Industrial Manipulators</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184062" title="Click to go to the Author Index">Polydoros, Athanasios S.</a></td><td class="r">Aalborg Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140145" title="Click to go to the Author Index">Nalpantidis, Lazaros</a></td><td class="r">Aalborg Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab733" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Model_Learning" title="Click to go to the Keyword Index">Model Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Learning" title="Click to go to the Keyword Index">Robot Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Reinforcement_Learning" title="Click to go to the Keyword Index">Robot Reinforcement Learning</a></span><br>
                           <strong>Abstract:</strong> Many robot learning algorithms depend on a model of the robot's forward dynamics for simulating potential trajectories and ultimately learning a required task. In this paper, we present a data-driven reservoir computing approach and apply it for learning forward dynamics models. Our proposed machine learning algorithm exploits the concepts of dynamic reservoir, self-organized learning and Bayesian inference. We have evaluated our approach on data-sets gathered from two industrial robotic manipulators and compared it on both step-by-step and multi-step trajectory prediction scenarios with state-of-the-art algorithms. The evaluation considers the algorithms' convergence and prediction performance on joint and operational space for varying prediction horizons, as well as computational time. Results show that the proposed algorithm performs better than the state-of-the-art, converges fast and can achieve accurate predictions over longer horizons, which makes it a reliable, data-efficient approach for learning forward models.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat8_03">11:45-12:00, Paper TuAT8.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1036.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1036'); return false" title="Click to show or hide the keywords and abstract">Nonparametric Distribution Regression Applied to Sensor Modeling</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#159122" title="Click to go to the Author Index">Tallavajhula, Abhijeet</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191371" title="Click to go to the Author Index">Poczos, Barnabas</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106604" title="Click to go to the Author Index">Kelly, Alonzo</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1036" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Model_Learning" title="Click to go to the Keyword Index">Model Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Calibration_and_Identification" title="Click to go to the Keyword Index">Calibration and Identification</a></span><br>
                           <strong>Abstract:</strong> Sensor models, which specify the distribution of sensor observations, are a widely used and integral part of robotics algorithms. Observation distributions are commonly approximated by parametric models, which are limited in their expressiveness, and may require careful design to suit an application. In this paper, we propose nonparametric distribution regression as a procedure to model sensors. It is a data-driven procedure to predict distributions that makes few assumptions. We apply the procedure to model raw	distributions from real sensors, and also demonstrate its utility to a mobile robot state estimation task. We show that nonparametric distribution regression adapts to characteristics in the training data, leading to realistic predictions. The same procedure competes favorably with baseline parametric models across applications. The results also help develop intuition for different sensor modeling situations. Our procedure is useful when distributions are inherently noisy, and sufficient data is available.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat8_04">12:00-12:15, Paper TuAT8.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1534.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1534'); return false" title="Click to show or hide the keywords and abstract">Online Learning for Characterizing Unknown Environments in Ground Robotic Vehicle Models</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183974" title="Click to go to the Author Index">Koppel, Alec</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105310" title="Click to go to the Author Index">Fink, Jonathan</a></td><td class="r">ARL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183979" title="Click to go to the Author Index">Warnell, Garrett</a></td><td class="r">U.S. Army Res. Lab</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114860" title="Click to go to the Author Index">Stump, Ethan</a></td><td class="r">US Army Res. Lab</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#134263" title="Click to go to the Author Index">Ribeiro, Alejandro</a></td><td class="r">Univ. of Pennsylvania</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1534" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Learning_Control" title="Click to go to the Keyword Index">Learning Control</a>, <a href="IROS16_KeywordIndexMedia.html#Wheeled_Robots" title="Click to go to the Keyword Index">Wheeled Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Learning" title="Click to go to the Keyword Index">Visual Learning</a></span><br>
                           <strong>Abstract:</strong>  In pursuit of increasing the operational tempo of a ground robotics platform in unknown domains, we consider the problem of predicting the distribution of structural state- estimation error due to poorly-modeled platform dynamics as well as environmental effects. Such predictions are a critical component of any modern control approach that utilizes uncertainty information to provide robustness in control design. We use an online learning algorithm based on matrix factorization techniques to fit a statistical model of error that provides enough expressive power to enable prediction directly from mo- tion control signals and low-level visual features. Moreover, we empirically demonstrate that this technique compares favorably to predictors that do not incorporate this information.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuat9"><b>TuAT9</b></a></td>
               <td class="r">#204~205</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuat9" title="Click to go to the Program at a Glance"><b>(Special Session) Autonomous Farming Technologies and Agricultural Robotics</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#116755" title="Click to go to the Author Index">Sa, Inkyu</a></td><td class="r">ETH Zurich</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#197049" title="Click to go to the Author Index">An, Ho Seok</a></td><td class="r">Univ. of Auckland</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat9_01">11:15-11:30, Paper TuAT9.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0079.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('79'); return false" title="Click to show or hide the keywords and abstract">Proof-Of-Concept of a Robotic Apple Harvester</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#187696" title="Click to go to the Author Index">Davidson, Joseph</a></td><td class="r">Washington State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195210" title="Click to go to the Author Index">Silwal, Abhisesh</a></td><td class="r">Washington State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195211" title="Click to go to the Author Index">Hohimer, Cameron</a></td><td class="r">Washington State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#187728" title="Click to go to the Author Index">Karkee, Manoj</a></td><td class="r">Washington State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195212" title="Click to go to the Author Index">Mo, Changki</a></td><td class="r">Washington State Univ. Tri-Cities</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195213" title="Click to go to the Author Index">Zhang, Qin</a></td><td class="r">Washington State Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab79" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robotics_in_Agriculture_and_Forestry" title="Click to go to the Keyword Index">Robotics in Agriculture and Forestry</a></span><br>
                           <strong>Abstract:</strong> There are no mechanical harvesters for the fresh market apple industry commercially available. The absence of automated harvesting technology is a critical problem because of rising production costs and increasing uncertainty about future labor availability. This paper presents the preliminary design of a robotic apple harvester. The approach adopted was to develop a low-cost, ‘undersensed’ system for modern orchard systems with fruiting wall architectures. A machine vision system fuses Circular Hough Transform and blob analysis to detect clustered and occluded fruit. The design includes a custom, six degree of freedom manipulator with an underactuated, passively compliant end-effector. After fruit localization, the system makes a linear approach to the apple and replicates the human picking process. Integrated testing of the robotic harvesting system has been completed in a laboratory environment with a replica apple tree for proof-of-concept demonstration. Experimental results show that the system picked 95 of the 100 fruit attempted with average localization and picking times of 1.2 and 6.8 seconds, respectively, per fruit. Additional work planned in preparation for field evaluation in a commercial orchard is also described.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat9_02">11:30-11:45, Paper TuAT9.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0930.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('930'); return false" title="Click to show or hide the keywords and abstract">Row Following in Pergola Structured Orchards</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195304" title="Click to go to the Author Index">Bell, Jamie</a></td><td class="r">The Univ. of Auckland</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107803" title="Click to go to the Author Index">MacDonald, Bruce</a></td><td class="r">Univ. of Auckland</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111352" title="Click to go to the Author Index">Ahn, Ho Seok</a></td><td class="r">The Univ. of Auckland, Auckland</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab930" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robotics_in_Agriculture_and_Forestry" title="Click to go to the Keyword Index">Robotics in Agriculture and Forestry</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a></span><br>
                           <strong>Abstract:</strong> Mobile service robots have the potential to improve the efficiency of fruit production in orchards. One of the key tasks that such robots must perform is traversing the rows. Many of the past implementations of row following in orchards have been developed for rows where the trees appear like walls on both sides. Another orchard structure that is used is the pergola, where a sparse array of trunks and posts hold up a canopy, which resembles a ceiling. Navigation in pergola structured environments has received less attention. The variations in the pergola environment- including the presence of tall weeds, hanging branches, undulating terrain and varying geometry- make following the rows a challenging problem. This paper presents solutions for finding the row centreline in pergola structured environments, in the presence of real world variability. A 3D laser scanner is used to measure the positions of posts and trunks, amongst the other features in the pergola. From the extracted features, the mode gradient of nearest neighbours is used to find the row direction and hence the centreline. The practicality of the system is demonstrated by autonomously driving a mobile robot through over 5000 meters of a kiwifruit orchard with a pergola structure, using the row detection method. This method performs favourably compared to an existing method of row detection in kiwifruit orchards.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat9_03">11:45-12:00, Paper TuAT9.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0974.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('974'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Can You Pick a Broccoli? 3D-Vision Based Detection and Localisation of Broccoli Heads in the Field</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178846" title="Click to go to the Author Index">Kusumam, Keerthy</a></td><td class="r">Univ. of Lincoln</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#167573" title="Click to go to the Author Index">Krajník, Tomáš</a></td><td class="r">Univ. of Lincoln</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196058" title="Click to go to the Author Index">Pearson, Simon</a></td><td class="r">Univ. of Lincoln</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106258" title="Click to go to the Author Index">Cielniak, Grzegorz</a></td><td class="r">Univ. of Lincoln</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104029" title="Click to go to the Author Index">Duckett, Tom</a></td><td class="r">Univ. of Lincoln</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab974" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0974.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Robotics_in_Agriculture_and_Forestry" title="Click to go to the Keyword Index">Robotics in Agriculture and Forestry</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a></span><br>
                           <strong>Abstract:</strong> This paper presents a 3D vision system for robotic harvesting of broccoli using low-cost RGB-D sensors. The presented method addresses the tasks of detecting mature broccoli heads in the field and providing their 3D locations relative to the vehicle. The paper evaluates different 3D features, machine learning and temporal filtering methods for detection of broccoli heads. Our experiments show that a combination of Viewpoint Feature Histograms, Support Vector Machine classifier and a temporal filter to track the detected heads results in a system that detects broccoli heads with 95.2% precision. We also show that the temporal filtering can be used to generate a 3D map of the broccoli head positions in the field.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat9_04">12:00-12:15, Paper TuAT9.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1649.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1649'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Development of an Autonomous Tomato Harvesting Robot with Rotational Plucking Gripper</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117269" title="Click to go to the Author Index">Yaguchi, Hiroaki</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142598" title="Click to go to the Author Index">Nagahama, Kotaro</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196926" title="Click to go to the Author Index">Hasegawa, Takaomi</a></td><td class="r">DENSO Corp</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106348" title="Click to go to the Author Index">Inaba, Masayuki</a></td><td class="r">The Univ. of Tokyo</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1649" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1649.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Manufacturing_and_Automation" title="Click to go to the Keyword Index">Manufacturing and Automation</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Gripper_and_Hand_Design" title="Click to go to the Keyword Index">Gripper and Hand Design</a></span><br>
                           <strong>Abstract:</strong> In this paper,	we present a design and development an autonomous tomato harvesting robot. We developed a harvesting robot with stereo camera which can measure depth in short range and direct sunlight and plucking gripper using the infinity rotational joint. We also evaluate the developed robot through harvesting in the tomato robot competition and the real farm. In the tomato robot competition, the robot harvested tomatoes from tomato clusters and tomato trees, harvesting speed was about 80[s/fruit] and successful rate was about 60%. In the real farm we evaluated the robot with tomato trees in semi-outdoor environment to show the effectiveness and robustness under direct sunlight. According to the result of harvesting with real tomatoes, we improved the robot motion and finally harvesting speed was up to 23[s/fruit], however the gripper may grasp multiple fruits in case of very cluttered cluster and the calyx also may be broken when the stem angle is deep from the rotation axis. To avoid this situation, a grasp state estimation of the gripper and simultaneous recognition fruit and stem positions are next problems to improve the harvesting successful rate.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuat10"><b>TuAT10</b></a></td>
               <td class="r">#206~208</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuat10" title="Click to go to the Program at a Glance"><b>Humanoid Robots 1</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#105872" title="Click to go to the Author Index">Henaff, Patrick</a></td><td class="r">CNRS, INRIA, Univ. of Lorraine,</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#196567" title="Click to go to the Author Index">Nava, Gabriele</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat10_01">11:15-11:30, Paper TuAT10.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0292.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('292'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Walking Control in Water Considering Reaction Forces from Water for Humanoid Robots with a Waterproof Suit</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#167316" title="Click to go to the Author Index">Kojio, Yuta</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#188419" title="Click to go to the Author Index">Karasawa, Tatsushi</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#165253" title="Click to go to the Author Index">Kojima, Kunio</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#188368" title="Click to go to the Author Index">Koyama, Ryo</a></td><td class="r">Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#149938" title="Click to go to the Author Index">Sugai, Fumihito</a></td><td class="r">Tokyo Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117479" title="Click to go to the Author Index">Nozawa, Shunichi</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#135552" title="Click to go to the Author Index">Kakiuchi, Yohei</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106350" title="Click to go to the Author Index">Okada, Kei</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106348" title="Click to go to the Author Index">Inaba, Masayuki</a></td><td class="r">The Univ. of Tokyo</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab292" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0292.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a>, <a href="IROS16_KeywordIndexMedia.html#Robotics_in_Hazardous_Fields" title="Click to go to the Keyword Index">Robotics in Hazardous Fields</a></span><br>
                           <strong>Abstract:</strong> In this paper, we develop a waterproof suit for humanoid robots and propose an underwater walking control method. Although very few life-sized humanoid robots are completely waterproof, we can easily make these humanoid robots watertight by putting a waterproof suit on them. In water, humanoid robots are influenced by the two forces due to the water: buoyancy and drag force. We take buoyancy into account when generating a walking pattern because the force is large and easy to estimate before walking. However, drag force is small and difficult to precisely predict and therefore, we treat the force as an unknown disturbance. In our method, we modify footsteps based on the Capture Point in order to deal with large disturbances. We verify the effectiveness of the proposed methods through an experiment in which a life-sized humanoid robot walks on a floor, stairs and debris in water.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat10_02">11:30-11:45, Paper TuAT10.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0574.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('574'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Achievement of Localization System for Humanoid Robots with Virtual Horizontal Scan Relative to Improved Odometry Fusing Internal Sensors and Visual Information</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#155978" title="Click to go to the Author Index">Kumagai, Iori</a></td><td class="r">Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118028" title="Click to go to the Author Index">Ueda, Ryohei</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#149938" title="Click to go to the Author Index">Sugai, Fumihito</a></td><td class="r">Tokyo Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117479" title="Click to go to the Author Index">Nozawa, Shunichi</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#135552" title="Click to go to the Author Index">Kakiuchi, Yohei</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106350" title="Click to go to the Author Index">Okada, Kei</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106348" title="Click to go to the Author Index">Inaba, Masayuki</a></td><td class="r">The Univ. of Tokyo</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab574" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0574.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a></span><br>
                           <strong>Abstract:</strong> To achieve tasks in unknown environments with high reliability, highly accurate localization during task execution is necessary for humanoid robots. In this paper, we discuss a localization system which can be applied to a humanoid robot when executing tasks in the real world. During tasks where several disturbances occur such as change of their posture in a manipulation or oscillations in locomotion, humanoid robots typically do not possess a referential to a constant plane which can in turn be used as part of fast and cost efficient localization methods. It is also not recommended for humanoid robots to suspend its tasks to build the environmental map or run recognition process to compensate localization errors in consideration for the operation efficiency. We solve these problems by first computing an improved odometry estimate through the fusion between visual odometry, feedforward commands from gait generator and orientation from inertia sensors. This fusion reduces the errors from the difference of the robot model, effects of slippage or oscillations in walking and drift in the estimation of the visual odometry. It is generally difficult to design an exact error model of the robot motion due to the non-linear error elements such as stabilization control, slippage of the feet and rigidity of the links. In our case, we succeed in improving the accuracy of the odometry by using a velocity error model assuming a normal distribution and measuring its parameters on the real robot. This measurment proved that this assumption was approximately reasonable. Next, This improved estimate is used to generate a 3D point cloud from the accumulation of successive laser scans. Such point cloud is then properly sliced to create a constant height horizontal virtual scan and finally, this slice is used as an observation base and fed to a 2D SLAM method. We evaluate our localization system in a real world task execution experiment using the JAXON robot such as terrain walking, door opening and valve turning task and show how our system can be used as a practical solution for humanoid robots localization during complex tasks execution processes. Our contribution is enabling a humanoid robot to localize itself within approximately 5.0[cm] and 1.0[deg] accuracy and 40[Hz] frequency in unknown environments. We concluded that the accuracy of our localization system was enough to increase the autonomy and the success rate of humanoid robots performing various tasks.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat10_03">11:45-12:00, Paper TuAT10.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0805.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('805'); return false" title="Click to show or hide the keywords and abstract">Measurement and Analysis of Physical Parameters of the Handshake between Two Persons According to Simple Social Contexts</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#193125" title="Click to go to the Author Index">Tagne, Gilles</a></td><td class="r">Univ. De Loraine, LORIA</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105872" title="Click to go to the Author Index">Henaff, Patrick</a></td><td class="r">CNRS, INRIA, Univ. of Lorraine,</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196274" title="Click to go to the Author Index">Gregori, Nicolas</a></td><td class="r">PERSEUS Lab. Univ. De Lorraine</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab805" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Biologically_Inspired_Robots" title="Click to go to the Keyword Index">Biologically-Inspired Robots</a></span><br>
                           <strong>Abstract:</strong> In order to facilitate and improve robots social acceptance, they must be equipped with behaviors similar to those of humans. It is therefore necessary to study and model the phenomenon to be reproduce. This paper studies and analyzes the physical parameters of the handshake in order to have its characteristic features (frequency, duration, strength, synchronization, etc.) used to model this interaction. Features that would later help to develop bio-inspired adaptive controllers, which will allow humanoid robots to better interact with humans according to simple social contexts.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuat10_04">12:00-12:15, Paper TuAT10.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1171.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1171'); return false" title="Click to show or hide the keywords and abstract">Stability Analysis and Design of Momentum-Based Controllers for Humanoid Robots</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196567" title="Click to go to the Author Index">Nava, Gabriele</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#165208" title="Click to go to the Author Index">Romano, Francesco</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111052" title="Click to go to the Author Index">Nori, Francesco</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#161571" title="Click to go to the Author Index">Pucci, Daniele</a></td><td class="r">Italian Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1171" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a>, <a href="IROS16_KeywordIndexMedia.html#Dynamics" title="Click to go to the Keyword Index">Dynamics</a></span><br>
                           <strong>Abstract:</strong> Envisioned applications for humanoid robots call for the design of balancing and walking controllers. While promising results have been recently achieved, robust and reliable controllers are still a challenge for the control community dealing with humanoid robotics. Momentum-based strategies have proven their effectiveness for controlling humanoids balancing, but the stability analysis of these controllers is still missing. The contribution of this paper is twofold. First, we numerically show that the application of state-of-the-art momentum-based control strategies may lead to unstable zero dynamics. Secondly, we propose simple modifications to the control architecture that avoid instabilities at the zero-dynamics level. Asymptotic stability of the closed loop system is shown by means of a Lyapunov analysis on the linearized system's joint space. The theoretical results are validated with both simulations and experiments on the iCub humanoid robot.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tulunch1"><b>TuLunch1</b></a></td>
               <td class="r">Grand Ballroom</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tulunch1" title="Click to go to the Program at a Glance"><b>RSJ Lunch for Industry and Academia Collaboration</b></a></td>
               <td class="r">Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#102292" title="Click to go to the Author Index">Okada, Masafumi</a></td><td class="r">Tokyo Inst. of Tech</td></tr>

</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuk1"><b>TuK1</b></a></td>
               <td class="r">#111</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuk1" title="Click to go to the Program at a Glance"><b>Keynote Talk 1. Tetsunari Inamura: A Cloud Based VR Platform for Sharing
<br>Embodied Experience in HRI</b></a></td>
               <td class="r">Keynote session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#101826" title="Click to go to the Author Index">Nakauchi, Yasushi</a></td><td class="r">Univ. of Tsukuba</td></tr>

</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuk2"><b>TuK2</b></a></td>
               <td class="r">#112</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuk2" title="Click to go to the Program at a Glance"><b>Keynote Talk 2. Jae-Bok Song: Smart Use of Springs for Robot Mechanisms</b></a></td>
               <td class="r">Keynote session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#121488" title="Click to go to the Author Index">Zimmermann, Uwe E.</a></td><td class="r">Kuka Lab. GmbH</td></tr>

</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tut21"><b>TuT21</b></a></td>
               <td class="r">#111</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tut21" title="Click to go to the Program at a Glance"><b>Human-Robot Interaction</b></a></td>
               <td class="r">Teaser Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#102949" title="Click to go to the Author Index">De Luca, Alessandro</a></td><td class="r">Sapienza Univ. of Rome</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#105621" title="Click to go to the Author Index">Guo, Yi</a></td><td class="r">Stevens Inst. of Tech</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_01">14:20-14:21, Paper TuT21.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0086.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('86'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Learning In-Contact Control Strategies from Demonstration</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190593" title="Click to go to the Author Index">Racca, Mattia</a></td><td class="r">Aalto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168113" title="Click to go to the Author Index">Pajarinen, Joni</a></td><td class="r">TU Darmstadt</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#119372" title="Click to go to the Author Index">Montebelli, Alberto</a></td><td class="r">Univ. of Skövde</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105044" title="Click to go to the Author Index">Kyrki, Ville</a></td><td class="r">Aalto Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab86" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0086.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Learning_from_Demonstration" title="Click to go to the Keyword Index">Learning from Demonstration</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Compliance_and_Impedance_Control" title="Click to go to the Keyword Index">Compliance and Impedance Control</a></span><br>
                           <strong>Abstract:</strong> Learning to perform tasks like pulling a door handle or pushing a button, inherently easy for a human, can be surprisingly difficult for a robot. A crucial problem in these kinds of in-contact tasks is the context specificity of pose and force requirements. In this paper, a robot learns in-contact tasks from human kinesthetic demonstrations. To address the need to balance between the position and force constraints, we propose a model based on the hidden semi-Markov model (HSMM) and Cartesian impedance control. The model captures uncertainty over time and space and allows the robot to smoothly satisfy a task’s position and force constraints by online modulation of impedance controller stiffness according to the HSMM state belief. In experiments, a KUKA LWR 4+ robotic arm equipped with a force/torque sensor at the wrist successfully learns from human demonstrations how to pull a door handle and push a button.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_02">14:21-14:22, Paper TuT21.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0184.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('184'); return false" title="Click to show or hide the keywords and abstract">Augmentation of Human Arm Motor Control by Isotropic Force Manipulability</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140131" title="Click to go to the Author Index">Petric, Tadej</a></td><td class="r">Jozef Stefan Inst</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195454" title="Click to go to the Author Index">Goljat, Rok</a></td><td class="r">Jozef Stefan Inst</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107479" title="Click to go to the Author Index">Babic, Jan</a></td><td class="r">Jozef Stefan Inst</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab184" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Performance_Augmentation" title="Click to go to the Keyword Index">Human Performance Augmentation</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Wearable_Robots" title="Click to go to the Keyword Index">Wearable Robots</a></span><br>
                           <strong>Abstract:</strong> In this paper we propose a novel control approach for robots that are physically coupled with humans, such as exoskeletons and assisting devices. In contrast with the conventional controllers, where assistance is usually provided indifferently of the configuration of the arm and the direction of the motion, we propose a control method that compensates the anisotropic property of the manipulability related to the human arm. Consequently, the assistive behavior of the proposed method allows the user to perform tasks in arm configurations that are otherwise unsuitable due to the lack of manipulability. In effect, the proposed method transforms the elliptic shape of the force manipulability ellipsoid to a circular shape in the whole space of human arm configurations. The proposed approach was evaluated by the arm-reaching task that involved pushing of a heavy object on a plane. We tested the accuracy and efficiency of the proposed method under several conditions that involved motions with added weight and friction. The results of our study show that the proposed approach significantly improves the human motor control ability and maintains the desired accuracy of the movement.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_03">14:22-14:23, Paper TuT21.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0247.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('247'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Human Force Augmentation : Optimal Control Parameters Tuning Using Structured Hoo Synthesis</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#162530" title="Click to go to the Author Index">Abroug, Neil</a></td><td class="r">CEA-LIST / Interactive Robotics Lab</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122477" title="Click to go to the Author Index">Lamy, Xavier</a></td><td class="r">French Atomic Energy Commission (CEA)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#109333" title="Click to go to the Author Index">Laroche, Edouard</a></td><td class="r">Univ. of Strasbourg</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab247" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0247.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a>, <a href="IROS16_KeywordIndexMedia.html#Compliance_and_Impedance_Control" title="Click to go to the Keyword Index">Compliance and Impedance Control</a></span><br>
                           <strong>Abstract:</strong> Human-Robot Co-Working is an active research domain in the field of robotics and control theory. Most of pioneering contributions addressing Co-Working interaction control problems has focused on PID based control. On the other hand, there are, in modern control theory, several tools allowing to design and tune arbitrarily complex servo-controllers. Non-Smooth optimization is one of recently developed tools that addresses the problem of tuning structured controllers according to Hoo, H2, and pole placement specifications. In this paper, the problem of force amplification is transcribed into high level Hoo specifications and is solved using Non-Smooth optimization. The resulting controller is tested on an industrial robot and shows similar performances as the state of the art while respecting some extra robustness certificates. Therefore, thanks to the abstraction level of the proposed technique, it becomes possible to tune interaction controllers with high number of tunable parameters according to complex performance/robustness specifications.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_04">14:23-14:24, Paper TuT21.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0248.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('248'); return false" title="Click to show or hide the keywords and abstract">Development of a Robotic Teaching Interface for Human to Human Skill Transfer</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107047" title="Click to go to the Author Index">Yang, Chenguang</a></td><td class="r">Plymouth Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#189348" title="Click to go to the Author Index">Liang, Peidong</a></td><td class="r">Harbin Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#150752" title="Click to go to the Author Index">Ajoudani, Arash</a></td><td class="r">Advanced Robotics Department</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#150704" title="Click to go to the Author Index">Li, Zhijun</a></td><td class="r">Shanghai Jiao Tong Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103420" title="Click to go to the Author Index">Bicchi, Antonio</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab248" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a></span><br>
                           <strong>Abstract:</strong> The tutor-tutee hand-in-hand teaching may be the most effective approach for a tutee to acquire new motor skills. Repetitive nature of such procedures in a group setting usually results in a high labour cost and time inefficiency. Potential solution can be utilizing robotic platforms playing the role of tutors for demonstrating and transferring the required skills. This requires an appropriate guidance scheme to integrate the tutor's motor functionalities into the robot's control architecture. For instance, for hand-in-hand supervision of the writing task, the tutor's corrections can be applied when necessary, while a very compliant motion can be achieved if no errors are detected. Inspired by this behavior, we develop a teaching interface using a dual-arm robotic platform. In our setup, one arm is connected to the tutee’s arm providing guidance through a variable stiffness control approach, and the other to the tutor to capture the motion and to feedback the tutee’s performance in a haptic manner. The reference stiffness for the tutor’s arm stiffness is estimated in real-time and replicated by the tutee’s robotic arm. Comparative experiments have been carried out on a dual-arm Baxter robot. The results imply that the human tutor is able to intuitively transfer writing skills to the tutee and also show superior learning performance over over some conventional teaching by demonstration techniques.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_05">14:24-14:25, Paper TuT21.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0331.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('331'); return false" title="Click to show or hide the keywords and abstract">Analysis of Velocity's Influence on Forces and Muscular Activity in the Context of Sit-To-Stand Motion Assisted by an Elderly Care Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111081" title="Click to go to the Author Index">DallaLibera, Fabio</a></td><td class="r">Panasonic Corp</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#146201" title="Click to go to the Author Index">Tsusaka, Yuko</a></td><td class="r">Panasonic Corp</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#181810" title="Click to go to the Author Index">Okazaki, Yasunao</a></td><td class="r">Panasonic Corp</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#182328" title="Click to go to the Author Index">Futakuchi, Ryutaro</a></td><td class="r">Panasonic Corp</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#157551" title="Click to go to the Author Index">Yamamoto, Masaki</a></td><td class="r">Panasonic</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#181889" title="Click to go to the Author Index">Shikata, Noriyuki</a></td><td class="r">Panasonic</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#181887" title="Click to go to the Author Index">Terashima, Masayuki</a></td><td class="r">Panasonic Corp</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab331" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Systems__Healthcare__and_Assisted_Living" title="Click to go to the Keyword Index">Medical Systems, Healthcare, and Assisted Living</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> The sit-to-stand movement is an apparently simple yet fundamental activity of daily life. Failure of performing this movement strongly impacts the quality of life of an increasing number of elderly people. Much research thus focuses on assisting this motion. The time necessary for rising up from a sit position is very short for healthy subjects, in the order of few seconds. Using similar speeds for the assisted motion would create safety concerns, hence slower speeds are usually employed. In this paper we experimentally investigate the effects of this speed reduction. We detail the relationship among robot's speed, forces acting on the robot's user and muscular activation. From the results of this analysis we derive indications on the speeds appropriate for assisting the sit-to-stand movement.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_06">14:25-14:26, Paper TuT21.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0467.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('467'); return false" title="Click to show or hide the keywords and abstract">Human Centric Spatial Affordances for Improving Human Activity Recognition</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104604" title="Click to go to the Author Index">Kim, David Inkyu</a></td><td class="r">Univ. of Southern California</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103260" title="Click to go to the Author Index">Martinson, Eric</a></td><td class="r">Toyota InfoTechnology Center, USA</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab467" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Detection_and_Tracking" title="Click to go to the Keyword Index">Human Detection and Tracking</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Learning" title="Click to go to the Keyword Index">Robot Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Centered_Robotics" title="Click to go to the Keyword Index">Human Centered Robotics</a></span><br>
                           <strong>Abstract:</strong> Spatial affordance can be defined as the functionality a space, or place, lends to human activity. Different places afford different activity possibilities - sleeping is mostly done in the bedroom, and cooking is mostly done in the kitchen. Semantic place labels like kitchen and bedroom, therefore, provide context with which a robot can better infer human activity. Real rooms, however, often defy simple place labels, as they can be multi-purpose, supporting many different types of human activity. The solution is to identify the spatial affordances associated with the current nexus of human activity – a micro-level place labeling. In this paper, we will demonstrate how to estimate these local spatial affordances by integrating a deep learning based place estimator with human pose estimation. The resulting affordances are then used to improve activity recognition using Bayesian belief network.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_07">14:26-14:27, Paper TuT21.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0471.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('471'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Exploiting Deep Semantics and Compositionality of Natural Language for Human-Robot-Interaction</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195816" title="Click to go to the Author Index">Eppe, Manfred</a></td><td class="r">International Computer Science Inst. Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195823" title="Click to go to the Author Index">Trott, Sean</a></td><td class="r">International Computer Science Inst. Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195822" title="Click to go to the Author Index">Feldman, Jerome</a></td><td class="r">International Computer Science Inst. Berkeley</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab471" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0471.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Cognitive_Human_Robot_Interaction" title="Click to go to the Keyword Index">Cognitive Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Companions_and_Social_Human_Robot_Interaction" title="Click to go to the Keyword Index">Robot Companions and Social Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> We develop a natural language interface for human robot interaction, which implements reasoning about deep semantics in natural language. To realize the required deep analysis, we employ methods from cognitive linguistics, namely the modular and compositional framework of Embodied Construction Grammar (ECG). Thanks to ECG, robots are able to solve fine-grained reference resolution problems and other issues related to deep semantics in natural language. This includes also verbal interaction with humans to clarify commands and queries that are too ambiguous to be executed safely. We implement our NLU framework as ROS package and present proof-of-concept scenarios with different smulated robots, as well as a survey on the state of the art.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_08">14:27-14:28, Paper TuT21.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0494.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('494'); return false" title="Click to show or hide the keywords and abstract">Studying of Rectilinear Locomotion for a Two-Segment System with Anisotropic Dry Friction Model</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195864" title="Click to go to the Author Index">Tang, Wenbin</a></td><td class="r">Shanghai Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#188810" title="Click to go to the Author Index">Li, Hengyu</a></td><td class="r">Shanghai Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110527" title="Click to go to the Author Index">Xie, Shaorong</a></td><td class="r">Shanghai Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110532" title="Click to go to the Author Index">Luo, Jun</a></td><td class="r">Shanghai Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab494" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Biologically_Inspired_Robots" title="Click to go to the Keyword Index">Biologically-Inspired Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Micro_Nano_Robots" title="Click to go to the Keyword Index">Micro/Nano Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a></span><br>
                           <strong>Abstract:</strong> This paper contributes to the understanding of the fundamental properties of rectilinearly locomotion of an one-dimensional system travelling on the horizontal plane, where dry Coulomb friction acting between it and surface. We discuss an approximate steady-state motion on a simplified two-segment system, which is propelled by a periodic internal excitation. First, the analysis is presented of the sufficient and necessary conditions for the system to move from the state of rest. Then, the explicit equations to calculate the constant average velocity of the steady-state are found. In addition, the influences of different parameters on the average velocity of the steady-state motion are discussed. Finally, the obtained theoretical results are verified by the numerical simulations.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_09">14:28-14:29, Paper TuT21.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0628.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('628'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Torque Control Based Sensorless Hand Guiding for Direct Robot Teaching</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#150700" title="Click to go to the Author Index">Lee, Sang-Duck</a></td><td class="r">Korea Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192840" title="Click to go to the Author Index">Ahn, Kuk Hyun</a></td><td class="r">Korea Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104776" title="Click to go to the Author Index">Song, Jae-Bok</a></td><td class="r">Korea Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab628" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0628.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Compliance_and_Impedance_Control" title="Click to go to the Keyword Index">Compliance and Impedance Control</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> In recent years, much research has been done to develop various direct teaching schemes, in which an operator directly teaches a robot by hand guiding instead of using a teach pendant. However, conventional direct teaching methods are usually sensor based, thus leading to an expensive solution. To deal with this problem, this study proposes a sensorless hand guiding method based on torque control. The dynamic model of a robot along with the motor current and friction model is used to determine the user’s intention to move the end-effector of a robot instead of directly sensing the external force by the user. A friction torque observer is employed for the friction model identification. The proposed method was experimentally verified using a 6 DOF robot. The experimental results show that the proposed sensorless hand guiding scheme is quite useful for practical direct teaching although the sensitivity is slightly lower than the sensor-based approach.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_10">14:29-14:30, Paper TuT21.10</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0687.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('687'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Vision-Guided Dual Arm Sewing System for Stent Graft Manufacturing</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158712" title="Click to go to the Author Index">Huang, Bidan</a></td><td class="r">Imperial Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169568" title="Click to go to the Author Index">Vandini, Alessandro</a></td><td class="r">Imperial Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169281" title="Click to go to the Author Index">Hu, Yang</a></td><td class="r">Imperial Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169685" title="Click to go to the Author Index">Lee, Su-Lin</a></td><td class="r">Imperial Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117925" title="Click to go to the Author Index">Yang, Guang-Zhong</a></td><td class="r">Imperial Coll. London</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab687" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0687.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Manufacturing_and_Automation" title="Click to go to the Keyword Index">Manufacturing and Automation</a>, <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Learning_from_Demonstration" title="Click to go to the Keyword Index">Learning from Demonstration</a></span><br>
                           <strong>Abstract:</strong> This paper presents an intelligent sewing system for personalized stent graft manufacturing, a challenging sewing task that is currently performed manually. Inspired by medical suturing robots, we have adopted a single-sided sewing technique using a curved needle to perform the task of sewing stents onto fabric. A motorized surgical needle driver was attached to a 7 d.o.f robot arm to manipulate the needle with a second robot controlling the position of the mandrel. A learning-from-demonstration approach was used to program the robot to sew stents onto fabric. The demonstrated sewing skill was segmented to several phases, each of which was encoded with a Gaussian Mixture Model. Generalized sewing movements were then generated from these models and were used for task execution. During execution, a stereo vision system was adopted to guide the robots to adjust the learnt movements according to the needle pose. Two experiments are presented here with this system and the results show that our system can robustly perform the sewing task as well as adapt to various needle poses. The accuracy of the sewing system was within 2mm.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_11">14:30-14:31, Paper TuT21.11</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0749.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('749'); return false" title="Click to show or hide the keywords and abstract">Multi-Modal Integration of Dynamic Audiovisual Patterns for an Interactive Reinforcement Learning Scenario</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#176129" title="Click to go to the Author Index">Cruz, Francisco</a></td><td class="r">Univ. of Hamburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#173028" title="Click to go to the Author Index">Parisi, German Ignacio</a></td><td class="r">Univ. of Hamburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184496" title="Click to go to the Author Index">Twiefel, Johannes</a></td><td class="r">Univ. of Hamburg, Department of Informatics, Knowledge Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110222" title="Click to go to the Author Index">Wermter, Stefan</a></td><td class="r">Univ. of Hamburg</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab749" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Cognitive_Human_Robot_Interaction" title="Click to go to the Keyword Index">Cognitive Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Reinforcement_Learning" title="Click to go to the Keyword Index">Robot Reinforcement Learning</a></span><br>
                           <strong>Abstract:</strong> Robots in domestic environments are receiving more attention, especially in scenarios where they should interact with parent-like trainers for dynamically acquiring and refining knowledge. A prominent paradigm for dynamically learning new tasks has been reinforcement learning. However, due to excessive time needed for the learning process, a promising extension has been made by incorporating an external parent-like trainer into the learning cycle in order to scaffold and speed up the apprenticeship using advice about what actions should be performed for achieving a goal. In interactive reinforcement learning, different uni-modal control interfaces have been proposed that are often quite limited and do not take into account multiple sensor modalities. In this paper, we propose the integration of audiovisual patterns to provide advice to the agent using multi-modal information. In our approach, advice can be given using either speech, gestures, or a combination of both. We introduce a neural network-based approach to integrate multi-modal information from uni-modal modules based on their confidence. Results show that multi-modal integration leads to a better performance of interactive reinforcement learning with the robot being able to learn faster with greater rewards compared to uni-modal scenarios.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_12">14:31-14:32, Paper TuT21.12</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0901.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('901'); return false" title="Click to show or hide the keywords and abstract">Compliant Control for Soft Robots: Emergent Behavior of a Tendon Driven Anthropomorphic Arm</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118177" title="Click to go to the Author Index">Martius, Georg</a></td><td class="r">IST Austria</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#188955" title="Click to go to the Author Index">Hostettler, Rafael</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105608" title="Click to go to the Author Index">Knoll, Alois</a></td><td class="r">Tech. Univ. Muenchen TUM</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196368" title="Click to go to the Author Index">Der, Ralf</a></td><td class="r">Univ. of Leipzig</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab901" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Learning_Control" title="Click to go to the Keyword Index">Learning Control</a>, <a href="IROS16_KeywordIndexMedia.html#Soft_bodied_Robots" title="Click to go to the Keyword Index">Soft-bodied Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> With the accelerated development of robot technologies, optimal control becomes one of the central themes of research. In traditional approaches, the controller, by its internal functionality, finds appropriate actions on the basis of the history of sensor values, guided by the goals, intentions, objectives, learning schemes, and so forth. While very successful with classical robots, these methods run into severe difficulties when applied to soft robots, a new field of robotics with large interest for human-robot interaction. We claim that a novel controller paradigm opens new perspective for this field. This paper applies a recently developed neuro controller with differential extrinsic synaptic plasticity to a muscle-tendon driven arm-shoulder system from the Myorobotics toolkit. In the experiments, we observe a vast variety of self-organized behavior patterns: when left alone, the arm realizes pseudo-random sequences of different poses. By applying physical forces, the system can be entrained into definite motion patterns like wiping a table. Most interestingly, after attaching an object, the controller gets in a functional resonance with the object's internal dynamics, starting to shake spontaneously bottles half-filled with water or sensitively driving an attached pendulum into a circular mode. When attached to the crank of a wheel the neural system independently develops to rotate it. In this way, the robot discovers affordances of objects its body is interacting with.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_13">14:32-14:33, Paper TuT21.13</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0941.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('941'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Mass Control of Pneumatic Soft Continuum Actuators with Commodity Components</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158603" title="Click to go to the Author Index">Deimel, Raphael</a></td><td class="r">TU Berlin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195713" title="Click to go to the Author Index">Radke, Marcel</a></td><td class="r">TU Berlin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101767" title="Click to go to the Author Index">Brock, Oliver</a></td><td class="r">Tech. Univ. Berlin</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab941" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0941.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Soft_bodied_Robots" title="Click to go to the Keyword Index">Soft-bodied Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Calibration_and_Identification" title="Click to go to the Keyword Index">Calibration and Identification</a></span><br>
                           <strong>Abstract:</strong> Soft pneumatic hands offer the advantage of intrinsic mechanical compliance. We argue that to fully leverage the compliance available in soft pneumatic actuators, they should be controlled using air mass rather than position or force, as is customary in most research in soft robotics. We propose an air-mass controller that can servo to a preset position and also allows for the exploitation of fast, mechanical compliance without additional control burden. The proposed mass control scheme is based on discrete commodity valves and pressure sensors, filling a gap in available mass control systems for small-scale soft continuum actuators. The proposed mass controller exhibits low drift for mass trajectories lasting tens of seconds, without requiring a precise model of the actuator. Continuous mass control enables applications for soft robotics, in which leveraging compliance during actuation is of central importance.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_14">14:33-14:34, Paper TuT21.14</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0966.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('966'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>It-Knee: An Exoskeleton with Ideal Torque Transmission Interface for Ergonomic Power Augmentation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195165" title="Click to go to the Author Index">Saccares, Lorenzo</a></td><td class="r">Fondazione Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#132997" title="Click to go to the Author Index">Sarakoglou, Ioannis</a></td><td class="r">Fondazione Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105401" title="Click to go to the Author Index">Tsagarakis, Nikos</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab966" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0966.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Performance_Augmentation" title="Click to go to the Keyword Index">Human Performance Augmentation</a>, <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Rehabilitation_Robotics" title="Click to go to the Keyword Index">Rehabilitation Robotics</a></span><br>
                           <strong>Abstract:</strong> This work presents the development of iT-Knee, a novel modular knee exoskeleton prototype that makes use of a torque transmission structure/interface to deliver pure assistive torque to the knee articulation, with the objective to act as rehabilitation tool, or a single joint add-on exoskeleton device for power augmentation. The specific kinematics features employed by iT-Knee can accommodate not only the translational of its instantaneous center of rotation in the sagittal plane and on its orthogonal direction, but also the knee varus/valgus angle as well as the internal/external rotation tibia movements delivering pure torque only around the flexion/extension movement. The above functionality results in superior transparency and comfortability which will potentially allow prolonged period of use without generating significant discomfort. The iT-Knee kinematics, its mechanism implementation and actuation unit are introduced. The Self-aligning feature combined with a fast lock/release mounting system on the human leg shortens the setup time required to wear it, increasing its user-friendly perception. iT-Knee uses a torque controlled actuation module to generate torque around the knee flexion extension enabling the implementation of several interaction control schemes that combine assistive functionality on demand with high back drivability/transparency during unloaded free motions of the human knee. Experimental results are presented which effectively demonstrate the validity and functionality of the iT-Knee device.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_15">14:34-14:35, Paper TuT21.15</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1022.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1022'); return false" title="Click to show or hide the keywords and abstract">Human Intent Forecasting Using Intrinsic Kinematic Constraints</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158796" title="Click to go to the Author Index">Hu, Ninghang</a></td><td class="r">Univ. of Amsterdam</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171127" title="Click to go to the Author Index">Bestick, Aaron</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#125428" title="Click to go to the Author Index">Englebienne, Gwenn</a></td><td class="r">Univ. of Twente</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#112939" title="Click to go to the Author Index">Bajcsy, Ruzena</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105546" title="Click to go to the Author Index">Krose, Ben</a></td><td class="r">Univ. of Amsterdam</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1022" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Detection_and_Tracking" title="Click to go to the Keyword Index">Human Detection and Tracking</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> The performance of human-robot collaboration tasks can be improved by incorporating predictions of the human collaborator's movement intentions. These predictions allow a collaborative robot to both provide appropriate assistance and plan its own motion so it does not interfere with the human. In the specific case of human reach intent prediction, prior work has divided the task into two pieces: recognition of human activities and prediction of reach intent. In this work, we propose a joint model for simultaneous recognition of human activities and prediction of reach intent based on skeletal pose. Since future reach intent is tightly linked to the action a person is performing at present, we hypothesize that this joint model will produce better performance on the recognition and prediction tasks than past approaches. In addition, our approach incorporates a simple human kinematic model which allows us to generate features that compactly capture the reachability of objects in the environment and the motion cost to reach those objects, which we anticipate will improve performance. Experiments using the CAD-120 benchmark dataset show that both the joint modeling approach and the human kinematic features give improved F1 scores versus the previous state of the art.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_16">14:35-14:36, Paper TuT21.16</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1123.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1123'); return false" title="Click to show or hide the keywords and abstract">Combining Real and Virtual Sensors for Measuring Interaction Forces and Moments Acting on a Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171719" title="Click to go to the Author Index">Buondonno, Gabriele</a></td><td class="r">Sapienza Univ. of Rome</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102949" title="Click to go to the Author Index">De Luca, Alessandro</a></td><td class="r">Sapienza Univ. of Rome</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1123" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Force_and_Tactile_Sensing" title="Click to go to the Keyword Index">Force and Tactile Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a></span><br>
                           <strong>Abstract:</strong> We address the problem of estimating an external wrench acting along the structure of a robot manipulator, together with the contact position where the external force is being applied. For this, we consider the combined use of a force/torque sensor mounted at the robot base and of a model-based virtual sensor. The virtual sensor is provided by the residual vector commonly used for collision detection and isolation in human-robot interaction. Integrating the two types of measurement tools provides an efficient way to estimate all unknown quantities, using also the recursive Newton-Euler algorithm for dynamic computations. Different operative conditions are considered, including the special cases of point-wise interaction (pure contact force), known contact location, and of a base sensor measuring only forces. We highlight also the conditions for a correct estimation to be fully virtual, i.e., without resorting to a force/torque sensor. Realistic simulations assess the estimation performance for a 7R robot in motion, subject to an unknown external force applied to an unknown location.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_17">14:36-14:37, Paper TuT21.17</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1127.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1127'); return false" title="Click to show or hide the keywords and abstract">A Passivity-Based Admittance Control Design Using Feedback Interconnections</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#149446" title="Click to go to the Author Index">Kim, Min Jun</a></td><td class="r">POSTECH</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#172750" title="Click to go to the Author Index">Lee, Woongyong</a></td><td class="r">POSTECH</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103861" title="Click to go to the Author Index">Ott, Christian</a></td><td class="r">German Aerospace Center (DLR)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100095" title="Click to go to the Author Index">Chung, Wan Kyun</a></td><td class="r">POSTECH</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1127" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Compliance_and_Impedance_Control" title="Click to go to the Keyword Index">Compliance and Impedance Control</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a></span><br>
                           <strong>Abstract:</strong> Admittance control is a well-established and popular control strategy in modern robotics. However, the standard admittance controller has several limitations. First, the passivity can be guaranteed by finding a set of control parameters that makes the admittance function positive real. However, this approach cannot be applied to multi degrees-of-freedom robot because it requires transfer function analysis. Second, standard admittance controller is not suitable when the system is exposed to unexpected environmental interaction (of which interaction force is not measured) due to the wall sticking effect. To overcome these limitations, this paper proposes an admittance controller of which structure can be constructed by feedback interconnection of passive subsystems. The proposed approach was verified using experiments and simulations.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_18">14:37-14:38, Paper TuT21.18</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1142.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1142'); return false" title="Click to show or hide the keywords and abstract">A Persuasive Learning from Demonstration System Architecture for Social Group Recreational Activities</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#154259" title="Click to go to the Author Index">Louie, Wing-Yue Geoffrey</a></td><td class="r">Univ. of Toronto</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104747" title="Click to go to the Author Index">Nejat, Goldie</a></td><td class="r">Univ. of Toronto</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1142" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Companions_and_Social_Human_Robot_Interaction" title="Click to go to the Keyword Index">Robot Companions and Social Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Learning_from_Demonstration" title="Click to go to the Keyword Index">Learning from Demonstration</a></span><br>
                           <strong>Abstract:</strong> Group-based recreational activities have shown to have a number of health benefits for people of all ages. The handful of social robots designed to facilitate such activities are currently only able to implement a priori known recreational activities that have been pre-programmed by human experts. Once deployed in their intended facility, these robots are not able to learn new activities from non-expert humans. In this paper, we present the development of a novel learning from demonstration (LfD) system architecture for a social robot in order for it to learn from non-expert teachers the structure of an activity and monitor the execution of the new activity. In order to obtain user compliance, personalized persuasive strategies are also learned by the robot to use while implementing the activity during human-robot interactions (HRI) with the intended users. The architecture has been integrated into our socially assistive robot Tangy to learn the group-based activity Bingo. System performance experiments were conducted with Tangy to first learn to facilitate Bingo from non-expert teachers and then use the learned activity to physically facilitate Bingo with multiple users. The results showed Tangy was able to effectively and efficiently learn the new Bingo activity structure as well as personalize its persuasive strategies to individual users in order to obtain activity compliance.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_19">14:38-14:39, Paper TuT21.19</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1219.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1219'); return false" title="Click to show or hide the keywords and abstract">Robot-Assisted Pedestrian Regulation in an Exit Corridor</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169853" title="Click to go to the Author Index">Jiang, Chao</a></td><td class="r">Stevens Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192501" title="Click to go to the Author Index">Ni, Zhen</a></td><td class="r">South Dakota State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105621" title="Click to go to the Author Index">Guo, Yi</a></td><td class="r">Stevens Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184793" title="Click to go to the Author Index">He, Haibo</a></td><td class="r">Univ. of Rhode Island</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1219" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Learning_Control" title="Click to go to the Keyword Index">Learning Control</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Reinforcement_Learning" title="Click to go to the Keyword Index">Robot Reinforcement Learning</a></span><br>
                           <strong>Abstract:</strong> Due to the fast-is-slower phenomenon in emergency escape, it is desirable to regulate pedestrian flow at the exit or a bottleneck. Modification of pedestrian facilities was previously studied to increase the efficiency and safety by the transportation community. We propose a robot-assisted pedestrian regulation scheme and study passive human-robot interaction (HRI), where the robot acts as a dynamic obstacle that interacts with pedestrians. Such a robot-assisted solution replaces expensive infrastructure modification with real-time reconfigurability. In the paper, we first formulate a robot-assisted flow optimization problem based on the social force models of pedestrian dynamics with embedded HRI forces. We then present an online learning algorithm based on adaptive dynamic programming (ADP) to generate motion control so that the robot can re-plan and adapt its motion to real-time pedestrian flows. The ADP control process uses observed flow information only but not the models of pedestrians, and provides feedback control with online learning and control capability. Simulation results demonstrate efficiency of the proposed method.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_20">14:39-14:40, Paper TuT21.20</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1422.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1422'); return false" title="Click to show or hide the keywords and abstract">Autonomous Question Answering with Mobile Robots in Human-Populated Environments</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#144379" title="Click to go to the Author Index">Chung, Michael Jae-Yoon</a></td><td class="r">Univ. of Washington</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105556" title="Click to go to the Author Index">Pronobis, Andrzej</a></td><td class="r">Univ. of Washington</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106008" title="Click to go to the Author Index">Cakmak, Maya</a></td><td class="r">Univ. of Washington</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104592" title="Click to go to the Author Index">Fox, Dieter</a></td><td class="r">Univ. of Washington</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106592" title="Click to go to the Author Index">Rao, Rajesh P. N.</a></td><td class="r">Univ. of Washington</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1422" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Service_Robots" title="Click to go to the Keyword Index">Service Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Centered_Robotics" title="Click to go to the Keyword Index">Human Centered Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Personal_Robots" title="Click to go to the Keyword Index">Personal Robots</a></span><br>
                           <strong>Abstract:</strong> Autonomous mobile robots will soon become ubiquitous in human-populated environments. Besides their typical applications in fetching, delivery, or escorting, such robots present the opportunity to assist human users in their daily tasks by gathering and reporting up-to-date knowledge about the environment. In this paper, we explore this use case and present an end-to-end framework that enables a mobile robot to answer natural language questions about the state of a large-scale, dynamic environment asked by the inhabitants of that environment. The system parses the question and estimates an initial viewpoint that is likely to contain information for answering the question based on prior environment knowledge. Then, it autonomously navigates towards the viewpoint while dynamically adapting to changes and new information. The output of the system is an image of the most relevant part of the environment that allows the user to obtain an answer to their question. We additionally demonstrate the benefits of a continuously operating information gathering robot by showing how the system can answer retrospective questions about the past state of the world using incidentally recorded sensory data. We evaluate our approach with a custom mobile robot deployed in a university building, with questions collected from occupants of the building. We demonstrate our system's ability to respond to these questions in different environmental conditions.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_21">14:40-14:41, Paper TuT21.21</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1428.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1428'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Human-Robot Shared Workspace Collaboration Via Hindsight Optimization</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171638" title="Click to go to the Author Index">Pellegrinelli, Stefania</a></td><td class="r">National Res. Council of Italy, Inst. of Industrial Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#148680" title="Click to go to the Author Index">Admoni, Henny</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142586" title="Click to go to the Author Index">Javdani, Shervin</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105832" title="Click to go to the Author Index">Srinivasa, Siddhartha</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1428" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1428.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#AI_Reasoning_Methods" title="Click to go to the Keyword Index">AI Reasoning Methods</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a></span><br>
                           <strong>Abstract:</strong> Our human-robot collaboration research aims to improve the fluency and efficiency of interactions between humans and robots when executing a set of tasks in a shared workspace. During human-robot collaboration, a robot and a user must often complete a disjoint set of tasks that use an overlapping set of objects, without using the same object simultaneously. A key challenge is deciding what task the robot should perform next in order to facilitate fluent and efficient collaboration. Most prior work does so by first predicting the human's intended goal, and then selecting actions given that goal. However, it is often difficult, and sometimes impossible, to infer the human's exact goal in real time, and this serial predict-then-act method is not adaptive to changes in human goals. In this paper, we present a system for inferring a probability distribution over human goals, and producing assistance actions given that distribution in real time. The aim is to minimize the disruption caused by the nature of human-robot shared workspace. We extend recent work utilizing Partially Observable Markov Decision Processes (POMDPs) for shared autonomy in order to provide assistance without knowing the exact goal. We evaluate our system in a study with 28 participants, and show that our POMDP model outperforms state of the art predict-then-act models by producing fewer human-robot collisions and less human idling time.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_22">14:41-14:42, Paper TuT21.22</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1499.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1499'); return false" title="Click to show or hide the keywords and abstract">Iterative Learning of Variable Impedance Control for Human-Robot Cooperation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103750" title="Click to go to the Author Index">Yamawaki, Tasuku</a></td><td class="r">National Defense Acad. of Japan</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195617" title="Click to go to the Author Index">Ishikawa, Hiroki</a></td><td class="r">National Defense Acad. of Japan</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103758" title="Click to go to the Author Index">Yashima, Masahito</a></td><td class="r">National Defense Acad. of Japan</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1499" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Dexterous_Manipulation" title="Click to go to the Keyword Index">Dexterous Manipulation</a></span><br>
                           <strong>Abstract:</strong> In this study, we propose a novel iterative learning scheme, which generates time-series data comprising the impedance value for human-robot cooperative work, where the human operator moves the end-effector from an initial position to a goal position. The proposed learning scheme iteratively updates time-series data comprising the variable impedance value to minimize a task-oriented cost function. Two types of noise reduction technique are used in the proposed learning scheme, which reduce the noise in the time direction and in the trial direction. The validity of the proposed method was verified based on experiments.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_23">14:42-14:43, Paper TuT21.23</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1644.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1644'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Development of a Grasping Force-Feedback User Interface for Surgical Robot System</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#150528" title="Click to go to the Author Index">Kim, Uikyum</a></td><td class="r">SungKyunKwan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192032" title="Click to go to the Author Index">Seok, Dong-Yeop</a></td><td class="r">Sungkyunkwan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169516" title="Click to go to the Author Index">Kim, Yong Bum</a></td><td class="r">Sungskyunkwan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142760" title="Click to go to the Author Index">Lee, Dong-Hyuk</a></td><td class="r">Korea Inst. of Industrial Tech. (KITECH)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102438" title="Click to go to the Author Index">Choi, Hyouk Ryeol</a></td><td class="r">Sungkyunkwan Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1644" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1644.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Surgical_Robotics" title="Click to go to the Keyword Index">Surgical Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a>, <a href="IROS16_KeywordIndexMedia.html#Tactile_Sensing" title="Click to go to the Keyword Index">Tactile Sensing</a></span><br>
                           <strong>Abstract:</strong> This paper presents a grasping force-feedback user interface (GFUI) which transfers the accurate kinesthetic force detected during robot-assisted minimally invasive surgery (RMIS) to a user, and allows the users to perform grasping motions with articulation and intuition. Using the GFUI, a developed robot hardware (S-surge) is controlled, and the force data measured from the robot are reflected to the GFUI. To evaluate the effect of grasping force-feedback control using the GFUI, a group of subjects (n=10) participated in a standard peg transfer tasks with and without the feedback control. To conduct the tasks, a developed robot hardware and a commercial haptic device were used which make the entire surgical environment including the GFUI as a master-slave controlled system. As a result, the performance of the GFUI was verified through the conducted tasks using the entire surgical system.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_24">14:43-14:44, Paper TuT21.24</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1648.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1648'); return false" title="Click to show or hide the keywords and abstract">Using IMU Data to Demonstrate Hand-Clapping Games to a Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160571" title="Click to go to the Author Index">Fitter, Naomi</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107509" title="Click to go to the Author Index">Kuchenbecker, Katherine J.</a></td><td class="r">Univ. of Pennsylvania</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1648" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Companions_and_Social_Human_Robot_Interaction" title="Click to go to the Keyword Index">Robot Companions and Social Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Entertainment_Robotics" title="Click to go to the Keyword Index">Entertainment Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Learning" title="Click to go to the Keyword Index">Robot Learning</a></span><br>
                           <strong>Abstract:</strong> All over the world, people find joy and amusement in playing hand-clapping games such as “Pat-a-cake” and “Slide.” Thus, as robots enter everyday human spaces and work together with people, we see potential for them to entertain, engage, and assist humans through cooperative clapping games. This paper explores how data recorded from a pair of commonly available inertial measurement units (IMUs) worn on a human’s hands can contribute to the teaching of a hand-clapping robot. We identified representative hand-clapping activities, considered approaches to identify games, and conducted a study to record hand-clapping motion data. Analysis of data from fifteen participants indicates that support vector machines and Markov chain analysis can correctly classify 95.5% of the demonstrated hand-clapping motions (from ten discrete actions) and 92.3% of the hand-clapping game demonstrations recorded in the study. These results were calculated by withholding a participant’s entire dataset for testing, so these results should represent general system behavior for new users. Overall, this research lays the groundwork for a simple and efficient method that people could use to demonstrate hand-clapping games to robots.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut21_25">14:44-14:45, Paper TuT21.25</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1659.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1659'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Hybrid Force/Velocity Control for Physical Human-Robot Collaboration Tasks</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168257" title="Click to go to the Author Index">Magrini, Emanuele</a></td><td class="r">Sapienza Univ. of Rome</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102949" title="Click to go to the Author Index">De Luca, Alessandro</a></td><td class="r">Sapienza Univ. of Rome</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1659" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1659.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a></span><br>
                           <strong>Abstract:</strong> During human-robot collaboration tasks, we may physically touch the robot at a generic location and engage an intentional exchange of forces while realizing coordinated motion of the common contact point. In order to control the relative motion and the exchanged contact forces, the latter need to be estimated without using any local force sensing device. Building upon our recent works, we generalize the classical hybrid force/velocity control design to this situation, handling complementary quantities along the directions of a suitable contact task frame in a dynamically decoupled way. The contact force is estimated online using our residual method together with an external sensor to localize the contact point, and the time-varying contact task frame is obtained analytically from this estimate. Experimental results are presented for a KUKA LWR4 robot using a Kinect sensor.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tut22"><b>TuT22</b></a></td>
               <td class="r">#112</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tut22" title="Click to go to the Program at a Glance"><b>Manipulation and Grasping</b></a></td>
               <td class="r">Teaser Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#105239" title="Click to go to the Author Index">Ozawa, Ryuta</a></td><td class="r">Ritsumeikan Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#103589" title="Click to go to the Author Index">Perdereau, Véronique</a></td><td class="r">Univ. Pierre Et Marie Curie - Paris 6</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_01">14:20-14:21, Paper TuT22.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0066.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('66'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Design of Low-Cost and Easy-Assemblable Robotic Hands with Stiff and Elastic Gear Trains</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#187346" title="Click to go to the Author Index">Hirano, Yasuyuki</a></td><td class="r">Ritumeikan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190351" title="Click to go to the Author Index">Akiyama, Kensaku</a></td><td class="r">Sankyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105239" title="Click to go to the Author Index">Ozawa, Ryuta</a></td><td class="r">Ritsumeikan Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab66" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0066.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Gripper_and_Hand_Design" title="Click to go to the Keyword Index">Gripper and Hand Design</a>, <a href="IROS16_KeywordIndexMedia.html#Multifingered_Hands" title="Click to go to the Keyword Index">Multifingered Hands</a></span><br>
                           <strong>Abstract:</strong> This paper proposes a method to design low-cost robotic hands that require almost toolless assembly, and have special transmissions and joint mechanisms. The transmissions consist of active and passive gear trains manufactured by 3D printers to realize coordinated motions among joints and to transmit the driving force to underactuated joints. The joints are designed with pop-in structures to simplify the assembly of the designed robotic hands and its maintenance. The robotic hands even enabled us to exchange some components without necessitating a shutdown. By using these techniques, we manufactured two robotic hands with different passive gear trains. One of the hands had three fingers with stiff constrained gear trains and soft fingertips, and the other had five fingers with elastic gear trains. Experiments were conducted to validate the performance of the proposed robotic hands and to clarify the roles of the different gear trains.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_02">14:21-14:22, Paper TuT22.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0134.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('134'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Learning Compliant Assembly Motions from Demonstration</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195350" title="Click to go to the Author Index">Suomalainen, Markku Heikki</a></td><td class="r">Aalto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105044" title="Click to go to the Author Index">Kyrki, Ville</a></td><td class="r">Aalto Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab134" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0134.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Manipulation_and_Compliant_Assembly" title="Click to go to the Keyword Index">Manipulation and Compliant Assembly</a>, <a href="IROS16_KeywordIndexMedia.html#Learning_from_Demonstration" title="Click to go to the Keyword Index">Learning from Demonstration</a>, <a href="IROS16_KeywordIndexMedia.html#Task_Planning" title="Click to go to the Keyword Index">Task Planning</a></span><br>
                           <strong>Abstract:</strong> Automating assembly processes outside controlled factory environments is still rare, mostly because of the inherent position uncertainties. The use of compliant motions allows robustness against the uncertainty, but automatic planning of compliant motion sequences is not computationally feasible. In this paper, we show how compliant assembly motions can be learned from human demonstrations. A human teacher will kinesthetically demonstrate compliant motions where the physical shapes of assembled parts guide the motion. From these demonstrations, the proposed method identifies desired direction of movement, the number of compliant axes and their directions. We use this information to construct an impedance controller which can reproduce the assembly motion despite uncertainty in the starting position. The method is studied with a KUKA LWR4+ arm in two test setups with different number of physically constrained degrees of freedom. The experimental study shows that the method is able to correctly identify the motion parameters and allows the robot to successfully perform the demonstrated assembly motion from various unseen starting positions.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_03">14:22-14:23, Paper TuT22.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0145.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('145'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Grasping Bulky Objects with Two Anthropomorphic Hands</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191938" title="Click to go to the Author Index">Rojas de Silva Gonzalez, Francisco Abiud</a></td><td class="r">Univ. Pol. De Catalunya (UPC)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104168" title="Click to go to the Author Index">Suarez, Raul</a></td><td class="r">Univ. Pol. De Catalunya (UPC)</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab145" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0145.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Multifingered_Hands" title="Click to go to the Keyword Index">Multifingered Hands</a>, <a href="IROS16_KeywordIndexMedia.html#Cooperative_Manipulators" title="Click to go to the Keyword Index">Cooperative Manipulators</a></span><br>
                           <strong>Abstract:</strong> This paper presents an algorithm to compute precision grasps for bulky objects using two anthropomorphic hands. We use objects modeled as point clouds obtained from a sensor camera or from a CAD model. We process the point clouds dividing them into two set of slices, one for each hand, where we look for sets of triplets of points. Each triplet must accomplish some physical conditions based on the structure of the hands. Then, triplets of points from each set of slices are evaluated to find a combination that satisfies the force closure condition (FC). Once one valid couple of triplets have been found the inverse kinematics of the system is computed in order to know if the points are kinematically reachable by the hands, if so, motion planning and a collision check are performed to asses if the grasp configuration of the system is suitable. The paper includes application examples of the proposed approach.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_04">14:23-14:24, Paper TuT22.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0156.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('156'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Grasp Envelopes: Extracting Constraints on Gripper Postures from Online Reconstructed 3D Models</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117783" title="Click to go to the Author Index">Stoyanov, Todor</a></td><td class="r">Örebro Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#135477" title="Click to go to the Author Index">Krug, Robert</a></td><td class="r">Oerebro Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180049" title="Click to go to the Author Index">Muthusamy, Rajkumar</a></td><td class="r">Aalto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105044" title="Click to go to the Author Index">Kyrki, Ville</a></td><td class="r">Aalto Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab156" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0156.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a></span><br>
                           <strong>Abstract:</strong> Grasping systems that build upon meticulously planned hand postures rely on precise knowledge of object geometry, mass and frictional properties - assumptions which are often violated in practice. In this work, we propose an alternative solution to the problem of grasp acquisition in simple autonomous pick and place scenarios, by utilizing the concept of grasp envelopes: sets of constraints on gripper postures. We propose a fast method for extracting grasp envelopes for objects that fit within a known shape category, placed in an unknown environment. Our approach is based on grasp envelope primitives, which encode knowledge of human grasping strategies. We use environment models, reconstructed from noisy sensor observations, to refine the grasp envelope primitives and extract bounded envelopes of collision-free gripper postures. Also, we evaluate the envelope extraction procedure both in a stand alone fashion, as well as an integrated component of an autonomous picking system.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_05">14:24-14:25, Paper TuT22.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0160.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('160'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Preparatory Object Reorientation for Task-Oriented Grasping</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171078" title="Click to go to the Author Index">Nguyen, Anh</a></td><td class="r">Inst. Italiano Di Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#147896" title="Click to go to the Author Index">Kanoulas, Dimitrios</a></td><td class="r">Inst. Italiano Di Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100179" title="Click to go to the Author Index">Caldwell, Darwin G.</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105401" title="Click to go to the Author Index">Tsagarakis, Nikos</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab160" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0160.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Mobile_Manipulation" title="Click to go to the Keyword Index">Mobile Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a></span><br>
                           <strong>Abstract:</strong> This paper describes a new task-oriented grasping method to reorient a rigid object to its nominal pose, which is defined as the configuration that it needs to be grasped from,in order to successfully execute a particular manipulation task. Our method combines two key insights: (1) a visual 6 Degreeof- Freedom (DoF) pose estimation technique based on 2D-3D point correspondences is used to estimate the object pose in real-time and (2) the rigid transformation from the current to the nominal pose is computed online and the object is reoriented over a sequence of steps. The outcome of this work is a novel method that can be effectively used in the preparatory phase of a manipulation task, to permit a robot to start from arbitrary object placements and configure the manipulated objects to the nominal pose, as required for the execution of a subsequent task. We experimentally demonstrate the effectiveness of our approach on a full-size humanoid robot (WALK-MAN) using different objects with various pose settings under real-time constraints.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_06">14:25-14:26, Paper TuT22.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0175.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('175'); return false" title="Click to show or hide the keywords and abstract">ALPHA: A Hybrid Self-Adaptable Hand for a Social Humanoid Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168485" title="Click to go to the Author Index">Cerruti, Giulio</a></td><td class="r">IRCCyN, Aldebaran</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103238" title="Click to go to the Author Index">Chablat, Damien</a></td><td class="r">Inst. De Recherche En Communications Et Cybernétique De Nante</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#120707" title="Click to go to the Author Index">Gouaillier, David</a></td><td class="r">Aldebaran Robotics</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#108888" title="Click to go to the Author Index">Sakka, Sophie</a></td><td class="r">IRCCyN / Univ. of Poitiers</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab175" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Multifingered_Hands" title="Click to go to the Keyword Index">Multifingered Hands</a>, <a href="IROS16_KeywordIndexMedia.html#Gripper_and_Hand_Design" title="Click to go to the Keyword Index">Gripper and Hand Design</a></span><br>
                           <strong>Abstract:</strong> This paper presents a novel design of a compact and light-weight robotic hand for a social humanoid robot. The proposed system is able to perform common hand gestures and self-adaptable grasps by mixing under-actuated and self-adaptable hand kinematics in a unique design. The hand answers the need for precise finger postures and sensor-less force feedback during gestures and for finger adaptation and autonomous force distribution during grasps. These are provided by a dual actuation system embodied within the palm and the fingers. Coexistence is ensured by compliant transmissions based on elastomer bars rather than classical tension springs, thanks to their high elastic coefficient at reduced sizes and strains. The proposed solution significantly reduces the weight and the size of the hand by using a reduced number of small actuators for gesturing and a single motor for grasping. The hand prototype (ALPHA) is realized to confirm the design feasibility and functional capabilities. It is controlled to provide safe human-robot interaction and preserve mechanical integrity in order to be embodied on a humanoid robot.<p>Keywords: social humanoid robot, robotic hand, gesture, grasping.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_07">14:26-14:27, Paper TuT22.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0189.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('189'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Task-Relevant Grasp Selection: A Joint Solution to Planning Grasps and Manipulative Motion Trajectories</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#159480" title="Click to go to the Author Index">Ghalamzan Esfahani, Amir Masoud</a></td><td class="r">Univ. of Birmingham</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195802" title="Click to go to the Author Index">Mavrakis, Nikos</a></td><td class="r">Univ. of Birmingham</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142216" title="Click to go to the Author Index">Kopicki, Marek</a></td><td class="r">Univ. of Birmingham</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104289" title="Click to go to the Author Index">Stolkin, Rustam</a></td><td class="r">Univ. of Birmingham</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107290" title="Click to go to the Author Index">Leonardis, Ales</a></td><td class="r">Univ. of Birmingham</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab189" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0189.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Manipulation_Planning_and_Control" title="Click to go to the Keyword Index">Manipulation Planning and Control</a></span><br>
                           <strong>Abstract:</strong> This paper addresses the problem of jointly planning both grasps and subsequent manipulative actions. Previously, these two problems have typically been studied in isolation, however joint reasoning is essential to enable robots to complete real manipulative tasks. In this paper, the two problems are addressed jointly and a solution that takes both into consideration is proposed. To do so, a manipulation capability index is defined, which is a function of both the task execution waypoints and the object grasping contact points. We build on recent state-of-the-art grasp-learning methods, to show how this index can be combined with a likelihood function computed by a probabilistic model of grasp selection, enabling the planning of grasps, which have a high likelihood of being stable, but which also maximise the robot's capability to deliver a desired post-grasp task trajectory. We also show how this paradigm can be extended, from a single arm and hand, to enable efficient grasping and manipulation with a bi-manual robot. We demonstrate the effectiveness of the approach using experiments on a simulated as well as a real robot.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_08">14:27-14:28, Paper TuT22.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0230.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('230'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Physics-Based Damage-Aware Manipulation Strategy Planning Using Scene Dynamics Anticipation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#151578" title="Click to go to the Author Index">Fromm, Tobias</a></td><td class="r">Jacobs Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105458" title="Click to go to the Author Index">Birk, Andreas</a></td><td class="r">Jacobs Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab230" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0230.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Animation_and_Simulation" title="Click to go to the Keyword Index">Animation and Simulation</a>, <a href="IROS16_KeywordIndexMedia.html#Industrial_Robots" title="Click to go to the Keyword Index">Industrial Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Service_Robots" title="Click to go to the Keyword Index">Service Robots</a></span><br>
                           <strong>Abstract:</strong> We present a damage-aware planning approach which determines the best sequence to manipulate a number of objects in a scene. This works on task-planning level, abstracts from motion planning and anticipates the dynamics of the scene using a physics simulation. Instead of avoiding interaction with the environment, we take unintended motion of other objects into account and plan manipulation sequences which minimize the potential damage. Our method can also be used as a validation measure to judge planned motions for their feasibility in terms of damage avoidance. We evaluate our approach on one industrial scenario (autonomous container unloading) and one retail scenario (shelf replenishment).
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_09">14:28-14:29, Paper TuT22.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0322.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('322'); return false" title="Click to show or hide the keywords and abstract">Control and Modeling for Direct Teaching of Industrial Articulated Robotic Arms</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195627" title="Click to go to the Author Index">Wang, Shuai</a></td><td class="r">Shanghai JiaoTong Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#116805" title="Click to go to the Author Index">Yuan, Jianjun</a></td><td class="r">Shanghai Jiao Tong Univ. China</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195622" title="Click to go to the Author Index">Fu, Xiajun</a></td><td class="r">Shanghai Jiao Tong Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195624" title="Click to go to the Author Index">Wang, Ning</a></td><td class="r">Shanghai Jiao Tong Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#113024" title="Click to go to the Author Index">Zhang, Weijun</a></td><td class="r">Shanghai Jiao Tong Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195631" title="Click to go to the Author Index">Xu, Peiqi</a></td><td class="r">Siasun Robot& Automation CO., LTD</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab322" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Manipulation_and_Compliant_Assembly" title="Click to go to the Keyword Index">Manipulation and Compliant Assembly</a>, <a href="IROS16_KeywordIndexMedia.html#Compliance_and_Impedance_Control" title="Click to go to the Keyword Index">Compliance and Impedance Control</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a></span><br>
                           <strong>Abstract:</strong> This paper presents an improved force-free control method based on current, which can be applied to industrial articulated robotic arms with large mass and large friction torque for direct teaching. Three kinds of torques that influence direct teaching are analyzed, and thus a calibration method and a compensation model are proposed. The model has been demonstrated effective through experiments. Considering the issue of large inertia, some security strategies are proposed at last and also well testified.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_10">14:29-14:30, Paper TuT22.10</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0352.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('352'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Dual-Arm Coordinated-Motion Task Specification and Performance Evaluation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#135711" title="Click to go to the Author Index">Park, H. Andy</a></td><td class="r">Purdue Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101031" title="Click to go to the Author Index">Lee, C. S. George</a></td><td class="r">Purdue Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab352" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0352.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Cooperative_Manipulators" title="Click to go to the Keyword Index">Cooperative Manipulators</a>, <a href="IROS16_KeywordIndexMedia.html#Manipulation_Planning_and_Control" title="Click to go to the Keyword Index">Manipulation Planning and Control</a>, <a href="IROS16_KeywordIndexMedia.html#Dexterous_Manipulation" title="Click to go to the Keyword Index">Dexterous Manipulation</a></span><br>
                           <strong>Abstract:</strong> Performing manipulation tasks in human environments often requires coordinated motions. The Extended-Cooperative-Task Space (ECTS) presents a unified representation describing any type of coordinated motions of two end-effectors. This paper focuses on the specification of dual-arm motion tasks based on the ECTS representation and the evaluation of the performance of the specified tasks. We first examine how the ECTS motion variables can be used effectively to specify coordinated motions to manipulate different types of objects. Then we derive new performance indices that can be used to evaluate and optimize the configurations of dual-arm robot systems performing any type of coordinated-motion tasks. The experimental results of coordinated-motion tasks performed by a Baxter robot demonstrate intuitiveness and efficiency of ECTS-based task specifications. We also show that the proposed ECTS performance indices can enable redundancies in a dual-arm system to be effectively utilized for a larger range of workspace as well as desired control objectives.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_11">14:30-14:31, Paper TuT22.11</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0365.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('365'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Fast Computation of Contact Points for Robotic Simulations Based on CAD Models without Tessellation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195379" title="Click to go to the Author Index">Crozet, Sébastien</a></td><td class="r">CEA</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#157592" title="Click to go to the Author Index">Leon, Jean-Claude</a></td><td class="r">Grenoble Univ. - INRIA</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#185515" title="Click to go to the Author Index">Merlhiot, Xavier</a></td><td class="r">Interactive Simulation Lab. CEA LIST</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab365" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0365.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Animation_and_Simulation" title="Click to go to the Keyword Index">Animation and Simulation</a>, <a href="IROS16_KeywordIndexMedia.html#Contact_Modelling" title="Click to go to the Keyword Index">Contact Modelling</a>, <a href="IROS16_KeywordIndexMedia.html#Collision_Detection_and_Avoidance" title="Click to go to the Keyword Index">Collision Detection and Avoidance</a></span><br>
                           <strong>Abstract:</strong> Computing multiple contact points between geometric models evolved in a virtual 3D environment is central to many robotic simulation applications. While this task can be performed efficiently and robustly between complex polyhedra, using the exact analytic geometric models issued by CAD modelers still suffers from efficiency limitations. Yet models composed of smooth surfaces are required to ensure smooth contact constraints, thus avoiding possible numerical artifacts which may dramatically affect the behavior of the system in the case of functional contacts. This paper builds on the observation that industrial CAD models are mostly composed of simple surfaces to perform an off-line identification of similar features and build a bounding volume hierarchy in order to locate potential contacts. Those are then computed by dedicated analytic methods, or an iterative root-finder, depending on the actual geometric representations of the features. In the context of dynamic simulation of robotic tasks, our method exhibits interactive computation times while naturally providing better result accuracy than existing polyhedron-specific algorithms.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_12">14:31-14:32, Paper TuT22.12</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0378.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('378'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Implementation of Twisting Skill to Robot Hands for Manipulating Linear Deformable Objects</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#182214" title="Click to go to the Author Index">Takizawa, Masaru</a></td><td class="r">The Univ. of Electro-Communications</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101327" title="Click to go to the Author Index">Kudoh, Shunsuke</a></td><td class="r">The Univ. of Electro-Communications</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100603" title="Click to go to the Author Index">Suehiro, Takashi</a></td><td class="r">The Univ. of Electro-Communications</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab378" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0378.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Dexterous_Manipulation" title="Click to go to the Keyword Index">Dexterous Manipulation</a></span><br>
                           <strong>Abstract:</strong> This paper describes the implementation of a twisting skill to a multi-DOF end effector for manipulating linear deformable objects (LDOs). When manipulating of LDOs such as rope knotting, an important factor is to control torsion. However, controlling torsion is difficult using only the robot arm movement because of the range limit of a manipulator and the danger of collision with the environment such as a table and objects to be tied. On the other hand, implementing the movement into a robot hand as in-hand manipulation means that the robot-arm movement can be compact. In this study, we extract what is needed in a ``twisting skill'', implement them into multi-DOF end effectors mounted on a dual-arm robot, and evaluate the skill. Then, we demonstrate rope manipulation including a knotting task and verify the effect by using a twisting skill. Our results show that the proposed method can be used to control the twisted angle of the rope with the required precision. Furthermore, using the proposed twisting skill, when placing a rope on a table top, the accuracy and reproducibility are improved compared with the case when the skill is not used. Finally, using the proposed method, clove hitches are successfully tied.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_13">14:32-14:33, Paper TuT22.13</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0415.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('415'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Physics-Based Model of a Rectangular Garment for Robotic Folding</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171924" title="Click to go to the Author Index">Petrík, Vladimír</a></td><td class="r">Czech Tech. Univ. in Prague</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#124835" title="Click to go to the Author Index">Smutny, Vladimir</a></td><td class="r">Faculty of Electrical Engineering, Czech Tech. Pra</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#134274" title="Click to go to the Author Index">Krsek, Pavel</a></td><td class="r">Czech Tech. Univ. in Prague</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168627" title="Click to go to the Author Index">Hlavac, Vaclav</a></td><td class="r">Czech Tech. Univ. in Prague</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab415" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0415.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_Planning_for_Manipulators" title="Click to go to the Keyword Index">Motion Planning for Manipulators</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a></span><br>
                           <strong>Abstract:</strong> The ability to perform an accurate robotic fold is essential to obtain the properly folded garment. Available solutions rely on a rough folding surface or on a comprehensive simulation, both preventing the garment from slipping on the table during folding. This paper proposes a new algorithm for a folding path design respecting the garment material properties and preventing the garment slipping. The folding path is derived based on the equilibrium of forces under the simplifying assumptions of a rectangular and homogeneous garment. This approach allows folding the rectangular garment on a low friction table surface as we demonstrated in the experiments performed by a dual-arm robotic testbed.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_14">14:33-14:34, Paper TuT22.14</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0546.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('546'); return false" title="Click to show or hide the keywords and abstract">Modeling and Control of an Ornithopter for Diving</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169782" title="Click to go to the Author Index">Rose, Cameron</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180540" title="Click to go to the Author Index">Mahmoudieh, Parsa</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101253" title="Click to go to the Author Index">Fearing, Ronald</a></td><td class="r">Univ. of California at Berkeley</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab546" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Model_Learning" title="Click to go to the Keyword Index">Model Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Biologically_Inspired_Robots" title="Click to go to the Keyword Index">Biologically-Inspired Robots</a></span><br>
                           <strong>Abstract:</strong> This paper details a method for identifying a set of piece-wise affine linear models that can be used for control design for flapping-winged flight. The paper focuses on a diving maneuver as the application for the models. The flight conditions during the dive are segmented into separate dynamically similar regions, and least-squares is used to estimate affine linear models for each modeling region. These models are used to compute the reachability sets for recovery conditions for safe diving. The point within the dive to begin recovery was determined using the backward reachable set by checking the current pose for inclusion in the reachable set. Using this control method, 2.2 meter dives were achieved at a success rate of 60 percent. The data-driven automatic modeling techniques and controller design processes can be extended to additional flight maneuvers, provided sufficient previous data have been collected for model generation.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_15">14:34-14:35, Paper TuT22.15</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0674.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('674'); return false" title="Click to show or hide the keywords and abstract">In-Hand Object Shape Identification Using Invariant Proprioceptive Signatures</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179807" title="Click to go to the Author Index">Vasquez, Alex</a></td><td class="r">Sorbone Univ. UPMC Univ. Paris 06</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#170977" title="Click to go to the Author Index">Kappassov, Zhanat</a></td><td class="r">Pierre and Marie Curie Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103589" title="Click to go to the Author Index">Perdereau, Véronique</a></td><td class="r">Univ. Pierre Et Marie Curie - Paris 6</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab674" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Multifingered_Hands" title="Click to go to the Keyword Index">Multifingered Hands</a>, <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a></span><br>
                           <strong>Abstract:</strong> Most modern approaches for tactile object recognition with robotic hands do not use proprioceptive data. In those that do, a limited number of objects with similar shapes is recognized. Furthermore, Self-Organizing Maps (SOM) based on raw values of joint angles/torques are frequently implemented which requires large sets of training data. In this paper, we present an approach based only on joint angles of a robotic hands to identify the shape of an object regardless its size and position within the hand. A representation of the joint angles is created to endow the robotic hand with proprioception. Support Vector Machine (SVM) is implemented for shape identification using patterns or signatures generated on this representation when the objects are grasped. To illustrate the scope of this method, tests are performed on five shapes present in common objects. Both SVM and SOM trained with signatures were at least 10% more accuracy than the ones trained with raw values of joint angles. Training sets are reduced at least 85% with respect to other works. An accuracy of 94% was obtained on large ranges of dimensions of the shapes and positions.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_16">14:35-14:36, Paper TuT22.16</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0778.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('778'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Classifying and Sorting Cluttered Piles of Unknown Objects with Robots: A Learning Approach</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192438" title="Click to go to the Author Index">Kujala, Janne V.</a></td><td class="r">ZenRobotics Ltd</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192982" title="Click to go to the Author Index">Lukka, Tuomas J.</a></td><td class="r">ZenRobotics Ltd</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192986" title="Click to go to the Author Index">Holopainen, Harri</a></td><td class="r">ZenRobotics Ltd</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab778" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0778.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Learning" title="Click to go to the Keyword Index">Robot Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Industrial_Robots" title="Click to go to the Keyword Index">Industrial Robots</a></span><br>
                           <strong>Abstract:</strong> We consider the problem of sorting a densely cluttered pile of unknown objects using a robot. This yet unsolved problem is relevant in the robotic waste sorting business.<p>By extending previous active learning approaches to grasping, we show a system that learns the task autonomously. Instead of predicting just whether a grasp succeeds, we predict the classes of the objects that end up being picked and thrown onto the target conveyor. Segmenting and identifying objects from the uncluttered target conveyor, as opposed to the working area, is easier due to the added structure since the thrown objects will be the only ones present.<p>Instead of trying to segment or otherwise understand the cluttered working area in any way, we simply allow the controller to learn a mapping from an RGBD image in the neighborhood of the grasp to a predicted result -- all segmentation etc. in the working area is implicit in the learned function. The grasp selection operates in two stages: The first stage is hardcoded and outputs a distribution of possible grasps that sometimes succeed. The second stage uses a purely learned criterion to choose the grasp to make from the proposal distribution created by the first stage.<p>In an experiment, the system quickly learned to make good pickups and predict correctly, in advance, which class of object it was going to pick up and was able to sort the objects from a densely cluttered pile by color.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_17">14:36-14:37, Paper TuT22.17</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0933.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('933'); return false" title="Click to show or hide the keywords and abstract">Online Planning of Optimal Trajectories on Assigned Paths with Dynamic Constraints for Robot Manipulators</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196353" title="Click to go to the Author Index">Casalino, Andrea</a></td><td class="r">Pol. Di Milano</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#129139" title="Click to go to the Author Index">Zanchettin, Andrea Maria</a></td><td class="r">Pol. Di Milano</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#10019" title="Click to go to the Author Index">Rocco, Paolo</a></td><td class="r">Pol. Di Milano</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab933" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a>, <a href="IROS16_KeywordIndexMedia.html#Dynamics" title="Click to go to the Keyword Index">Dynamics</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Planning_for_Manipulators" title="Click to go to the Keyword Index">Motion Planning for Manipulators</a></span><br>
                           <strong>Abstract:</strong> This paper addresses time-optimal path-constrained trajectory planning. Given a geometric path for a manipulator, this paper focuses on the selection of the time law along the path. This law minimizes the time required to complete the path and at the same time is consistent with constraints, both at kinematic and dynamic levels. To obtain the optimal law a decision algorithm for the acceleration along the path has been developed. Remarkably, the algorithm is amenable to online implementation, thus allowing for path replanning. An experimental validation on an ABB IRB140 robot is shown.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_18">14:37-14:38, Paper TuT22.18</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1238.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1238'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Wolverine: A Wearable Haptic Interface for Grasping in Virtual Reality</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196585" title="Click to go to the Author Index">Choi, Inrak</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114825" title="Click to go to the Author Index">Hawkes, Elliot Wright</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#162083" title="Click to go to the Author Index">Christensen, David</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196601" title="Click to go to the Author Index">Ploch, Christopher</a></td><td class="r">Stanford Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196594" title="Click to go to the Author Index">Follmer, Sean</a></td><td class="r">Stanford Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1238" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1238.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Wearable_Robots" title="Click to go to the Keyword Index">Wearable Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> The Wolverine is a mobile, wearable haptic device designed for simulating the grasping of rigid objects in a virtual reality interface. In contrast to prior work on wearable force feedback gloves, we focus on creating a low cost and lightweight device that renders a force directly between the thumb and three fingers to simulate objects held in pad opposition (precision) type grasps. Leveraging low-power brake-based locking sliders, the system can withstand over 100N of force between each finger and the thumb, and only consumes 0.24 mWh (0.87 joules) for each braking interaction. Integrated sensors are used both for feedback control and user input: time-of-flight sensors provide the position of each finger and an IMU provides overall orientation tracking. This paper describes the mechanical design, control strategy, and performance analysis of the Wolverine system and provides a comparison with several existing wearable haptic devices.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_19">14:38-14:39, Paper TuT22.19</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1261.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1261'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Direct and Realistic Handover of a Virtual Object</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111012" title="Click to go to the Author Index">Kim, Jun-Sik</a></td><td class="r">Korea Inst. of Science & Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#147390" title="Click to go to the Author Index">Park, Jung-Min</a></td><td class="r">Korea Inst. of Science and Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1261" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1261.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Virtual_Reality_and_Interfaces" title="Click to go to the Keyword Index">Virtual Reality and Interfaces</a></span><br>
                           <strong>Abstract:</strong> We propose a method to induce a realistic two-hand manipulation of virtual objects by representing their reactions to multiple contacts of hands and fingers in a unified way. One of the key problems in achieving physics-based virtual interaction is that there is no accurate haptic feedback to the user's hands and fingers, which must introduce a mismatch between the real hands and their virtual counterparts. This problem becomes more serious when multiple hands or fingers are involved to a single virtual object, for example, a hand-over situation from one hand to the other. The key contribution of this paper is to represent and describe the physical status of a virtual object in the local object space and to change the status in the object point of view in order to avoid its drastic changes by the physical-virtual mismatch. The experiments compare the performance of the proposed method with objects in different shapes when using 2D and 3D visualization devices.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_20">14:39-14:40, Paper TuT22.20</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1297.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1297'); return false" title="Click to show or hide the keywords and abstract">Toward Physics-Based Virtual Reality Testbeds for Intelligent Robot Manipulators - an Erobotics Approach</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160342" title="Click to go to the Author Index">Guiffo Kaigom, Eric</a></td><td class="r">Inst. for Man-Machine Interaction, RWTH AachenUniversity</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105269" title="Click to go to the Author Index">Rossmann, Juergen</a></td><td class="r">RWTH Aachen Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1297" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Virtual_Reality_and_Interfaces" title="Click to go to the Keyword Index">Virtual Reality and Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Animation_and_Simulation" title="Click to go to the Keyword Index">Animation and Simulation</a></span><br>
                           <strong>Abstract:</strong> The increasingly pervasive usage of robotic automation not only calls for a high performance of physical robot manipulators, but also implies challenging requirements on 3D robot simulators. A skillful reaction of simulated robots to unpredicted contact forces has become an especially stringent necessity to support the development of emerging applications for which a compliant robotic manipulation is indispensable. This paper focuses on the simulation of physics-based virtual testbeds in which simulated robots are made capable of accommodating external forces exerted by the environment on their manipulators. For this, we integrate methods from multi-body dynamics modeling, advanced robotics, and virtual reality to develop a robotics simulation basis that facilitates the employment of compliant robotic manipulations in simulated applications. The validation of this basis in joint space indicates that a physics-based simulation of robots endowed with active physical interaction control can capture and reflect significant effects which characterize the interactive behavior of physical compliant robots. We demonstrate the practical utility of our approach to unlock competitive advantages in fully simulated applications prone to position uncertainty. Furthermore, we show the potential of using compliant digital twins of physical robots to introduce compliant robotic manipulations into real applications.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_21">14:40-14:41, Paper TuT22.21</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1419.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1419'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Self-Aligning Gripper Using an Electrostatic/Gecko-Like Adhesive</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#125826" title="Click to go to the Author Index">Dadkhah, Mohammad</a></td><td class="r">Perception Robotics</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196739" title="Click to go to the Author Index">Zhao, Zhanyue</a></td><td class="r">Illinois Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#119026" title="Click to go to the Author Index">Wettels, Nicholas</a></td><td class="r">Perception Robotics</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106228" title="Click to go to the Author Index">Spenko, Matthew</a></td><td class="r">Illinois Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1419" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1419.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Gripper_and_Hand_Design" title="Click to go to the Keyword Index">Gripper and Hand Design</a>, <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Industrial_Robots" title="Click to go to the Keyword Index">Industrial Robots</a></span><br>
                           <strong>Abstract:</strong> This paper introduces a new robotic gripper for flat surfaces based on a novel electrostatic/gecko-like adhesive. This unique gripping solution overcomes the shortcomings of vacuum grippers for part-handling by eliminating the need for a compressed air system and offering more rapid actuation, thus achieving significant potential cost savings and throughput improvements in manufacturing processes. Results demonstrate the gripper's performance on a variety of both smooth and rough surfaces, including fabrics, as well as show that the gripper is able to successfully pick up and release glass and carbon fiber sheets for over 100 cycles.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_22">14:41-14:42, Paper TuT22.22</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1465.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1465'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Vision-Based Precision Manipulation with Underactuated Hands: Simple and Effective Solutions for Dexterity</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122725" title="Click to go to the Author Index">Calli, Berk</a></td><td class="r">Yale Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103607" title="Click to go to the Author Index">Dollar, Aaron</a></td><td class="r">Yale Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1465" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1465.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Dexterous_Manipulation" title="Click to go to the Keyword Index">Dexterous Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Servoing" title="Click to go to the Keyword Index">Visual Servoing</a>, <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a></span><br>
                           <strong>Abstract:</strong> In this paper, a method is proposed for vision-based within-hand precision manipulation with underactuated grippers. The method combines the advantages of adaptive underactuation with the robustness of visual servoing algorithms by employing simple action sets in actuator space, called precision manipulation primitives (PMPs). It is shown that, with this approach, reliable precision manipulation is possible even without joint and force sensors by using only minimal gripper kinematics information. An adaptation method is also utilized in the vision loop to enhance the system’s transient performance. The proposed methods are analyzed with experiments using various target objects and reference signals. The results indicate that underactuated hands, even with minimalistic sensing and control via visual servoing, can provide a simple and inexpensive solution to allow low-fidelity precision manipulation.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_23">14:42-14:43, Paper TuT22.23</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1480.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1480'); return false" title="Click to show or hide the keywords and abstract">Development of a Dual-Cable Hand Exoskeleton System for Virtual Reality</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#163195" title="Click to go to the Author Index">Park, Yeon gyu</a></td><td class="r">UNIST(Ulsan National Inst. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#163203" title="Click to go to the Author Index">Jo, Inseong</a></td><td class="r">UNIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#109405" title="Click to go to the Author Index">Bae, Joonbum</a></td><td class="r">UNIST</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1480" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Wearable_Robots" title="Click to go to the Keyword Index">Wearable Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a></span><br>
                           <strong>Abstract:</strong> In this paper, a hand exoskeleton system for virtual reality is proposed. As a virtual reality interface for the hand, a wearable system should be able to measure finger joint angles and apply force feedback to the fingers with a simple and light design. In the proposed system, the finger joint angles are measured by a tendon-inspired cable mechanism. Also, another cable is used for force feedback to the finger. Using the measured finger joint angles and motor currents, the cable-driven actuation system applies desired force to the fingers. That is, when the desired force is zero, the motor position is controlled to track the changed cable length; when the desired force needs to be applied, the motor current is controlled to generate the desired force. For a smooth transition between two control strategies, linearly changing proportions of each control strategy is applied in the transition range. A prototype of the proposed system was manufactured, and the proposed control algorithms were verified by experiments.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_24">14:43-14:44, Paper TuT22.24</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1581.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1581'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Real-Time Grasp Planning Based on Motion Field Graph for Human-Robot Cooperation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#150551" title="Click to go to the Author Index">Hwang, Jae Pyung</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196833" title="Click to go to the Author Index">Yang, Myungsik</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102631" title="Click to go to the Author Index">Suh, Il Hong</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140162" title="Click to go to the Author Index">Kwon, Taesoo</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1581" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1581.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Telerobotics" title="Click to go to the Keyword Index">Telerobotics</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a></span><br>
                           <strong>Abstract:</strong> We present a real-time framework for planning natural and smooth grasping motions in an online manner based on the interaction with human. The proposed framework is able to change its grasp strategies agilely according to the interaction with human. Given human demonstrations, we develop a motion field graph consisting of nodes and edges where the nodes contains reference finger poses and their time derivatives, and the edges indicates the similarity between a pair of nodes. Based on the graph, a new grasping motion can be planned that adapts to the changes of the environment and the interaction. The motion field guarantees smooth motions by integrating the velocities obtained from the demonstrations. To validate the framework, we build a demo system where a human can hand a cup over to a tele-operated robot or a virtual humanoid avatar which are controlled by an another person at a remote location. Also, the virtual avatar can grasp and manipulate a cola can.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut22_25">14:44-14:45, Paper TuT22.25</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1655.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1655'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>HEXOTRAC: A Highly Under-Actuated Hand Exoskeleton for Finger Tracking and Force Feedback</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#132997" title="Click to go to the Author Index">Sarakoglou, Ioannis</a></td><td class="r">Fondazione Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#167261" title="Click to go to the Author Index">Brygo, Anais</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196890" title="Click to go to the Author Index">Mazzanti, Dario</a></td><td class="r">Fondazione Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111420" title="Click to go to the Author Index">Garcia Hernandez, Nadia Vanessa</a></td><td class="r">Center for Res. and Advanced Studies of the IPN</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100179" title="Click to go to the Author Index">Caldwell, Darwin G.</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105401" title="Click to go to the Author Index">Tsagarakis, Nikos</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1655" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1655.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Rehabilitation_Robotics" title="Click to go to the Keyword Index">Rehabilitation Robotics</a></span><br>
                           <strong>Abstract:</strong> Exoskeletons offer an intuitive method for actuating multiple DOF of the body; this makes them attractive for applications where generation and coupling of artificial forces to the limbs is needed. Force feedback hand exoskeletons have been continuously considered for whole hand haptic interaction in virtual reality simulators, in teleoperation setups and for rehabilitation. In hand exoskeletons finger tracking, actuation and transmission systems must be embedded in confined spaces, matching at the same time the profound dexterity of the hand transparently and without causing a burden. Most of the design approaches for such systems have remained largely experimental due to hardware limitations, impacting heavily on important functional and ergonomic factors. This paper presents the design of a novel 3-digit hand exoskeleton, which addresses the issues of finger tracking and force feedback. It proposes a new approach for the application of the feedback force with a single attachment at the fingertip through a 6DoF kinematic chain. This kinematic linkage allows for unconstrained reach of the fingers within their full workspace and facilitates a sensor system for high resolution 6DOF tracking of the fingertips. At the same time the highly under-actuated mechanism permits application of a bidirectional feedback force at the fingertips. The hand exoskeleton fits an large range of hand sizes and requires no mechanical alignment between the linkage and the fingers, whatsoever. Preliminary results show the efficacy of this system as a tracking and force feedback device for the hand.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubi1"><b>TuBI1</b></a></td>
               <td class="r">#111</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubi1" title="Click to go to the Program at a Glance"><b>Interactive Session: Human-Robot Interaction</b></a></td>
               <td class="r">Interactive Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#102949" title="Click to go to the Author Index">De Luca, Alessandro</a></td><td class="r">Sapienza Univ. of Rome</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#105621" title="Click to go to the Author Index">Guo, Yi</a></td><td class="r">Stevens Inst. of Tech</td></tr>
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubi2"><b>TuBI2</b></a></td>
               <td class="r">#112</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubi2" title="Click to go to the Program at a Glance"><b>Interactive Session: Manipulation and Grasping</b></a></td>
               <td class="r">Interactive Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#105239" title="Click to go to the Author Index">Ozawa, Ryuta</a></td><td class="r">Ritsumeikan Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#103589" title="Click to go to the Author Index">Perdereau, Véronique</a></td><td class="r">Univ. Pierre Et Marie Curie - Paris 6</td></tr>
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubt1"><b>TuBT1</b></a></td>
               <td class="r">#101</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubt1" title="Click to go to the Program at a Glance"><b>Wheeled System Control</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#100766" title="Click to go to the Author Index">Suzumori, Koichi</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#190813" title="Click to go to the Author Index">Nabae, Hiroyuki</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt1_01">14:50-15:05, Paper TuBT1.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0535.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('535'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Robust Trajectory Tracking Controllers for Pose-Regulation of Wheeled Mobile Robots</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117062" title="Click to go to the Author Index">Becerra, Hector M.</a></td><td class="r">Centro De Investigación En Matemáticas (CIMAT)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195789" title="Click to go to the Author Index">Colunga Ramírez, José Armando</a></td><td class="r">Centro De InvestigaciÓn En MatemÁticas, A.c</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#159452" title="Click to go to the Author Index">Romero Velazquez, Jose Guadalupe</a></td><td class="r">Lab. Des Signaux Et Systèmes, CNRS–SUPELEC</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab535" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0535.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Wheeled_Robots" title="Click to go to the Keyword Index">Wheeled Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a></span><br>
                           <strong>Abstract:</strong> Two robust kinematic controllers for position trajectory tracking of a perturbed wheeled mobile robot are presented. We address a final objective of fixed-time pose-regulation, which means that the robot position and orientation must reach desired final values simultaneously in a user-defined time. To achieve that, we propose the robust tracking of adequate trajectories for position, which drives the robot to get a desired final orientation due to the nonholonomic motion constraint. Hence, the main contribution of the paper is a complete strategy to define adequate reference trajectories as well as robust controllers to track them in order to enforce the pose-regulation of a wheeled mobile robot in a desired time. Realistic simulations show the good performance of the proposed scheme even in the presence of strong disturbances.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt1_02">15:05-15:20, Paper TuBT1.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0645.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('645'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Eccentric Crank Rover : A Novel Crank Wheel Mechanism with Eccentric Wheels</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169503" title="Click to go to the Author Index">Komura, Hirotaka</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107755" title="Click to go to the Author Index">Endo, Gen</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100766" title="Click to go to the Author Index">Suzumori, Koichi</a></td><td class="r">Tokyo Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab645" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0645.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Wheeled_Robots" title="Click to go to the Keyword Index">Wheeled Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Climbing_Robots" title="Click to go to the Keyword Index">Climbing Robots</a></span><br>
                           <strong>Abstract:</strong> The crank wheel mechanism, consisting of a wheel mechanism and parallel links connected to each wheel, achieved high mobility and efficiency because it has both wheels and legs in a simple structure. However, each prior model of crank wheel mechanism has had shortcomings such as mass oscillation or fragile structure. In this paper, we propose a novel crank wheel mechanism, the &quot;Eccentric Crank Rover&quot;(ECR), which is an enhanced crank wheel mechanism with eccentric wheels. The eccentric wheels increase the under-body clearance, and change the body trajectory from straight to trochoid curve, which has the same shape as the crank legs but opposite phase trajectory. Thus, the body itself acts as a &quot;second&quot; crank leg. We experimentally confirmed higher step climbability, larger clearance, and lower cost of transport than other models such as normal wheel model, eccentric wheel model, and crank wheel model without eccentric wheel.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt1_03">15:20-15:35, Paper TuBT1.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0822.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('822'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Passive Robotic Walker Path Following with Bang-Bang Hybrid Control Paradigm</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196249" title="Click to go to the Author Index">Divan, Stefano</a></td><td class="r">Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192613" title="Click to go to the Author Index">Andreetto, Marco</a></td><td class="r">Univ. of Trento</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#112613" title="Click to go to the Author Index">Fontanelli, Daniele</a></td><td class="r">Univ. of Trento</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#115820" title="Click to go to the Author Index">Palopoli, Luigi</a></td><td class="r">Univ. of Trento</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab822" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0822.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Wheeled_Robots" title="Click to go to the Keyword Index">Wheeled Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Service_Robots" title="Click to go to the Keyword Index">Service Robots</a></span><br>
                           <strong>Abstract:</strong> This paper presents a control algorithm that steers a robotic walking assistant along a planned path using electromechanical brakes. The device is modeled as a Dubins’ car, i.e., a wheeled vehicle that moves only forward in the plane and has a limited turning radius. In order to reduce the cost of the hardware, no force sensor is employed. This feature hampers the application of control algorithms based on a modulated braking action. A viable solution is based on the application of on/off braking action, thus forcing the vehicle to turn with a fixed turning radius. In order to avoid the annoying chattering behaviour, which is the inevitable companion of all bang-bang solutions, we propose a hybrid controller based on three discrete states that rule the application of the braking action. The resulting feedback controller secures a gentle convergence of the user toward the planned path and her steady progress towards the destination. This is obtained by using two independent hystereses thresholds, the first one associated with the approaching phase and the second with the following phase. The system convergence toward the path is formally proved. Simulations and experiments show the effectiveness of the proposed approach and the good level of comfort for the user.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt1_04">15:35-15:50, Paper TuBT1.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0850.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('850'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Permanent Magnet-Assisted Omnidirectional Ball Drive</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#172954" title="Click to go to the Author Index">Özgür, Ayberk</a></td><td class="r">École Pol. Fédérale De Lausanne</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#174011" title="Click to go to the Author Index">Johal, Wafa</a></td><td class="r">École Pol. Fédérale De Lausanne</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#116697" title="Click to go to the Author Index">Dillenbourg, Pierre</a></td><td class="r">EPFL</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab850" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0850.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a>, <a href="IROS16_KeywordIndexMedia.html#Wheeled_Robots" title="Click to go to the Keyword Index">Wheeled Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> We present an omnidirectional ball wheel drive design that utilizes a permanent magnet as the drive roller to generate the contact force. Particularly interesting for novel human-mobile robot interaction scenarios where the users are expected to physically interact with many palm-sized robots, our design combines simplicity, low cost and compactness. We first detail our design and explain its key parameters. Then, we present our implementation and compare it with an omniwheel drive built with identical conditions and similar cost. Finally, we elaborate on the main advantages and drawbacks of our design.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt1_05">15:50-16:05, Paper TuBT1.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1360.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1360'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>R-Crank: Amphibious All Terrain Mobile Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196659" title="Click to go to the Author Index">Yamada, Shintaro</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#108155" title="Click to go to the Author Index">Hirose, Shigeo</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107755" title="Click to go to the Author Index">Endo, Gen</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100766" title="Click to go to the Author Index">Suzumori, Koichi</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190813" title="Click to go to the Author Index">Nabae, Hiroyuki</a></td><td class="r">Tokyo Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1360" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1360.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Wheeled_Robots" title="Click to go to the Keyword Index">Wheeled Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Robotics_in_Hazardous_Fields" title="Click to go to the Keyword Index">Robotics in Hazardous Fields</a></span><br>
                           <strong>Abstract:</strong> An amphibious all terrain mobile robot is required for disaster response tasks. In this paper, we propose &quot;R-Crank&quot; , which has a simple four wheels driving configuration with crank links, connecting to ipsilateral front and rear wheels so as to form parallel four-bar linkage systems.The crank links act as legs because their trajectory with respect to the ground become trochoid curve, which is commonly used as leg trajectories for a legged robot. These links achieve high terrain adaptability while maintaining simple structure of the wheeled robot. We developed a prototype model and carried out basic experiments. The robot achieved to climb 0.3 m	of a step(76.9% of the wheel diameter), stairs and a snowy slope. In addition, we propose amphibious structure of crank links by installing multiple rigid paddles. The propulsive force generated by the crank links was measured and empirically derived the optimum parameters by using a small-sized experimental apparatus. Finally the robot equipped with the amphibious crank links achieved propulsion on water at 0.13 m/s. We also verified that the robot achieved going up and into water through stairs.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubt2"><b>TuBT2</b></a></td>
               <td class="r">#102</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubt2" title="Click to go to the Program at a Glance"><b>Detection and Segmentation</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#101785" title="Click to go to the Author Index">Burgard, Wolfram</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#100760" title="Click to go to the Author Index">Ang Jr, Marcelo H</a></td><td class="r">National Univ. of Singapore</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt2_01">14:50-15:05, Paper TuBT2.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0017.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('17'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Augmenting Deep Convolutional Neural Networks with Depth-Based Layered Detection for Human Detection</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103260" title="Click to go to the Author Index">Martinson, Eric</a></td><td class="r">Toyota InfoTechnology Center, USA</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171441" title="Click to go to the Author Index">Yalla, Ganesh</a></td><td class="r">Toyota InfoTechnology Center</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab17" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0017.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Detection_and_Tracking" title="Click to go to the Keyword Index">Human Detection and Tracking</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a></span><br>
                           <strong>Abstract:</strong> Deep convolutional neural networks are being increasingly deployed for image classification tasks as they can learn sensor and environmental independence from large quantities of training data. Most, however, have focused on classifying uploaded photographs rather than the often occluded, arbitrary height and camera angles images found commonly in robotic applications. In this work, we look at the performance of the popular AlexNet architecture to detect people in different robotic scenarios using different sensors and/or environments. Furthermore, we demonstrate how fusing this architecture with the depth-based layered detection system, a more traditional geometric feature-based classifier, leads to significant improvements in classification precision/recall, whether working with depth data alone or a combination of depth and RGB images.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt2_02">15:05-15:20, Paper TuBT2.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0179.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('179'); return false" title="Click to show or hide the keywords and abstract">3D Region Segmentation Using Topological Persistence</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#157520" title="Click to go to the Author Index">Beksi, William</a></td><td class="r">Univ. of Minnesota</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101709" title="Click to go to the Author Index">Papanikolopoulos, Nikos</a></td><td class="r">Univ. of Minnesota</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab179" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Environment_Monitoring_and_Management" title="Click to go to the Keyword Index">Environment Monitoring and Management</a>, <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a></span><br>
                           <strong>Abstract:</strong> A 'region' is an important concept in interpreting 3D point cloud data since regions may correspond to objects in a scene. To correctly interpret 3D point cloud data, we need to partition the dataset into regions that correspond to objects or parts of an object. In this paper, we present a region growing approach that combines global (topological) and local (color, surface normal) information to segment 3D point cloud data. Using ideas from persistent homology theory, our algorithm grows a simplicial complex representation of the point cloud dataset. At each step in the growth process we compute the zeroth homology group of the complex, which corresponds to the number of connected components, and use color and surface normal statistics to build regions. Lastly, we extract out the segmented regions of the dataset. We show that this method provides a stable segmentation of point cloud data in the presence of noise and poorly sampled data, thus providing advantages over contemporary region-based segmentation techniques.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt2_03">15:20-15:35, Paper TuBT2.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0308.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('308'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Simultaneous Segmentation, Estimation and Analysis of Articulated Motion from Dense Point Cloud Sequence</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195632" title="Click to go to the Author Index">Kim, Youngji</a></td><td class="r">Korea Advanced Inst. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#161785" title="Click to go to the Author Index">Lim, Hwasup</a></td><td class="r">Korea Inst. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104680" title="Click to go to the Author Index">Ahn, Sang Chul</a></td><td class="r">KIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#126962" title="Click to go to the Author Index">Kim, Ayoung</a></td><td class="r">Korea Advanced Inst. of Science Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab308" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0308.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Surveillance_Systems" title="Click to go to the Keyword Index">Surveillance Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Robotics_in_Construction" title="Click to go to the Keyword Index">Robotics in Construction</a></span><br>
                           <strong>Abstract:</strong> In this paper, we present a unified approach for Expectation and Maximization (EM) based motion segmentation, estimation and analysis from dense point cloud data. When identifying an underlying motion, literature mainly focuses on three related topics: motion segmentation, estimation and analysis. These topics are, however, mostly considered separately while integrated approaches are rare. Our approach specifically focuses on analyzing articulated motion from dense point cloud data by simultaneously solving for three topics using an integrated approach. No prior knowledge, such as background regions, number of segments and correspondence, is required since two iterations in this algorithm allow us to seamlessly accomplish integration of the three tasks. The first iteration of the algorithm is performed between segmentation and estimation, followed by the second iteration between motion estimation and analysis. For the first iteration, we propose EM based subspace clustering algorithm. For the second iteration, we simply fuse the motion analysis method from [1] into an iterative motion estimation algorithm. As a result, we can extract label, correspondence and motion of moving objects simultaneously from dense point cloud sequence. In experiment, we validate the performance of the proposed method on both synthetic and real world data.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt2_04">15:35-15:50, Paper TuBT2.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0397.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('397'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Robust Moving Objects Detection in Lidar Data Exploiting Visual Cues</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192499" title="Click to go to the Author Index">Postica, Gheorghii</a></td><td class="r">Pol. Di Milano</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179933" title="Click to go to the Author Index">Romanoni, Andrea</a></td><td class="r">Pol. Di Milano</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103628" title="Click to go to the Author Index">Matteucci, Matteo</a></td><td class="r">Pol. Di Milano</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab397" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0397.VI.mpg" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a></span><br>
                           <strong>Abstract:</strong> Detecting moving objects in dynamic scenes from sequences of lidar scans is an important task in object tracking, mapping, localization, and navigation. Many works focus on changes detection in previously observed scenes, while a very limited amount of literature addresses moving objects detection. The state-of-the-art method exploits Dempster-Shafer Theory to evaluate the occupancy of a lidar scan and to discriminate points belonging to the static scene from moving ones. In this paper we improve both speed and accuracy of this method by discretizing the occupancy representation, and by removing false positives through visual cues. Many false positives lying on the ground plane are also removed thanks to a novel ground plane removal algorithm. Efficiency is improved through an octree indexing strategy. Experimental evaluation against the KITTI public dataset shows the effectiveness of our approach, both qualitatively and quantitatively with respect to the state-of-the-art.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt2_05">15:50-16:05, Paper TuBT2.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1012.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1012'); return false" title="Click to show or hide the keywords and abstract">Lost and Found: Detecting Small Road Hazards for Self-Driving Vehicles</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183245" title="Click to go to the Author Index">Pinggera, Peter</a></td><td class="r">Daimler</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196344" title="Click to go to the Author Index">Ramos, Sebastian</a></td><td class="r">Daimler AG R&D</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#153551" title="Click to go to the Author Index">Gehrig, Stefan</a></td><td class="r">Daimler AG</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180896" title="Click to go to the Author Index">Franke, Uwe</a></td><td class="r">Daimler</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196350" title="Click to go to the Author Index">Rother, Carsten</a></td><td class="r">TU Dresden</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183246" title="Click to go to the Author Index">Mester, Rudolf</a></td><td class="r">Goethe Univ. Frankfurt</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1012" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Intelligent_Transportation_Systems" title="Click to go to the Keyword Index">Intelligent Transportation Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Collision_Detection_and_Avoidance" title="Click to go to the Keyword Index">Collision Detection and Avoidance</a></span><br>
                           <strong>Abstract:</strong> Detecting small obstacles on the road ahead is a critical part of the driving task which has to be mastered by fully autonomous cars. In this paper, we present a method based on stereo vision to reliably detect such obstacles from a moving vehicle. The proposed algorithm performs statistical hypothesis tests in disparity space directly on stereo image data, assessing freespace and obstacle hypotheses on independent local patches. This detection approach does not depend on a global road model and handles both static and moving obstacles. For evaluation, we employ a novel lost-cargo image sequence dataset comprising more than two thousand frames with pixelwise annotations of obstacle and free-space and provide a thorough comparison to several stereo-based baseline methods. The dataset will be made available to the community to foster further research on this important topic. The proposed approach outperforms all considered baselines in our evaluations on both pixel and object level and runs at frame rates of up to 20 Hz on 2 mega-pixel stereo imagery. Small obstacles down to the height of 5 cm can successfully be detected at 20 m distance at low false positive rates.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubt3"><b>TuBT3</b></a></td>
               <td class="r">#103</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubt3" title="Click to go to the Program at a Glance"><b>Haptics</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#111807" title="Click to go to the Author Index">Fischer, Gregory Scott</a></td><td class="r">Worcester Pol. Inst. WPI</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#110950" title="Click to go to the Author Index">Patoglu, Volkan</a></td><td class="r">Sabanci Univ</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt3_01">14:50-15:05, Paper TuBT3.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1369.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1369'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Six Degrees of Freedom Haptic Interface for Laparoscopic Training</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196684" title="Click to go to the Author Index">Agboh, Wisdom C.</a></td><td class="r">Sabanci Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#153447" title="Click to go to the Author Index">Yalcin, Mustafa</a></td><td class="r">Sabanci Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110950" title="Click to go to the Author Index">Patoglu, Volkan</a></td><td class="r">Sabanci Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1369" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1369.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> We present the novel kinematics, workspace characterization, functional prototype and impedance control of a six degrees of freedom haptic interface designed to train surgeons for laparoscopic procedures, through virtual reality simulations. The parallel kinematics of the device is constructed by connecting a 3RRP planar parallel mechanism to a linearly actuated modified delta mechanism with a connecting link. The configuration level forward and inverse kinematics of the device assume analytic solutions, while its workspace can be shaped to enable large end-effector translations and rotations, making it well-suited for laparoscopy operations. Furthermore, the haptic interface features a low apparent inertia with high structural stiffness, thanks to its parallel kinematics with grounded actuators. A model-based open-loop impedance controller with feed-forward gravity compensation has been implemented for the device and various virtual tissue/organ stiffness levels have been rendered.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt3_02">15:05-15:20, Paper TuBT3.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1458.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1458'); return false" title="Click to show or hide the keywords and abstract">Towards a Haptic Feedback Framework for Multi-DOF Robotic Laparoscopic Surgery Platforms</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178514" title="Click to go to the Author Index">Munawar, Adnan</a></td><td class="r">Worcester Pol. Inst</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111807" title="Click to go to the Author Index">Fischer, Gregory Scott</a></td><td class="r">Worcester Pol. Inst. WPI</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1458" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Surgical_Robotics" title="Click to go to the Keyword Index">Surgical Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Collision_Detection_and_Avoidance" title="Click to go to the Keyword Index">Collision Detection and Avoidance</a></span><br>
                           <strong>Abstract:</strong> The use of robotics for laparoscopic surgery has been an established field for over a decade. However, with the influx of advanced tools and algorithms for general purpose robotics, there is a need to incorporate these advancements into medical robotics technology. The daVinci Research Kit and its software framework provides a step towards these advancements. This paper presents the development of new tools and utilization of previously developed tools used for general purpose robotics, and their tailored use in medical robotics. Additionally, a method for computing haptic forces for tele-operated surgical robots is presented. The technique utilizes elastic, Spherical Proxy Regions (SPR) to readily compute directional interaction forces and manipulate them to create a dynamic behavior at the surgeon/user’s manipulator.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt3_03">15:20-15:35, Paper TuBT3.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1609.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1609'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>ANYpulator: Design and Control of a Safe Robotic Arm</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196195" title="Click to go to the Author Index">Bodie, Karen</a></td><td class="r">Verity Studios AG</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#193489" title="Click to go to the Author Index">Bellicoso, C. Dario</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114045" title="Click to go to the Author Index">Hutter, Marco</a></td><td class="r">ETH Zurich</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1609" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1609.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Manipulation_and_Compliant_Assembly" title="Click to go to the Keyword Index">Manipulation and Compliant Assembly</a>, <a href="IROS16_KeywordIndexMedia.html#Force_Control" title="Click to go to the Keyword Index">Force Control</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a></span><br>
                           <strong>Abstract:</strong> The present paper introduces a manipulator that is developed to combine safe and dynamic interaction tasks. The system is built from lightweight carbon fiber links and novel high-performance series elastic actuator units that provide dynamic movement capability, low-impedance joint torque control, and inherent interaction safety. This enabled the implementation of a model-based direct force control method purely based on joint torque regulation. Using unified force and motion control, the end-effector position can be accurately and dynamically tracked in task space while acting safely upon (unexpected) contacts with the environment. The force control component is implemented in a novel way that shows reduced forces in comparison to existing methods when navigating across a surface of unpredictable orientation and friction. ANYpulator is tested using a haptic feedback method that renders the system dynamics and contact forces back to the user.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt3_04">15:35-15:50, Paper TuBT3.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1628.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1628'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Visuo-Haptic Transmission of Contact Information Improve Operation of Active Scope Camera</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196752" title="Click to go to the Author Index">Funamizu, Takahito</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#143775" title="Click to go to the Author Index">Nagano, Hikaru</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110833" title="Click to go to the Author Index">Konyo, Masashi</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100118" title="Click to go to the Author Index">Tadokoro, Satoshi</a></td><td class="r">Tohoku Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1628" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1628.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Performance_Augmentation" title="Click to go to the Keyword Index">Human Performance Augmentation</a>, <a href="IROS16_KeywordIndexMedia.html#Search_and_Rescue_Robots" title="Click to go to the Keyword Index">Search and Rescue Robots</a></span><br>
                           <strong>Abstract:</strong> Disaster response robots for searching in a narrow area have limited space to mount tactile sensors, but the operators require sensory feedback to recognize contact situations with the surrounding environment. This study proposes a new approach to transmitting contact information of a remote-operated snake-like robot called Active Scope Camera (ASC) to the operator using simple configurations for the sensing and display methods. For the sensing side, we develop a contact estimation method with a limited number of tactile sensors. We establish the method to localize the contact position and the magnitude by sensing multiple propagated vibrations based on experiments and formulations. Preliminary experiments show that the developed method estimates a collision angle with high probability (93.8% at the worst condition) at several collisional situations. For the display side, we combine visual and vibrotactile feedback to provide the operator both directional and temporal cues to perceive contact events. The proposed visualization method uses colored bars, peripherally superposed on the video image, to show the estimated contact location and magnitude. A single DoF vibrotactile feedback is used for a joystick interface to control the head movement of the ASC. The effect of vibrotactile feedback on the response time to contact events is evaluated. Finally, we investigate the performance of the operation by identifying the contact behavior at simulated scenarios. Experimental results show that collision times per operation time is decreased by the developed feedback system compared with a simple video-based operation.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt3_05">15:50-16:05, Paper TuBT3.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1661.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1661'); return false" title="Click to show or hide the keywords and abstract">Two-Channel Electrotactile Stimulation for Sensory Feedback of Fingers of Prosthesis</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196257" title="Click to go to the Author Index">Choi, Kyunghwan</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183083" title="Click to go to the Author Index">Kim, Pyungkang</a></td><td class="r">Korea Advanced Inst. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#126125" title="Click to go to the Author Index">Kim, Kyung-Soo</a></td><td class="r">KAIST(Korea Advanced Inst. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#126129" title="Click to go to the Author Index">Kim, Soohyun</a></td><td class="r">KAIST(Korea Advanced Inst. of Science and Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1661" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Rehabilitation_Robotics" title="Click to go to the Keyword Index">Rehabilitation Robotics</a></span><br>
                           <strong>Abstract:</strong> — Electrotactile stimulation has been used to provide sensory information of forearm prosthesis to users. Although conventional sensory feedback method, where one electrode expresses sensory information of only one finger, could provide force information of three fingers by using three electrodes, it showed less cognitive accuracy when more than two electrodes were stimulated simultaneously compared to individual stimulation. To improve the cognitive accuracy, we presented a sensory feedback method called two-channel electrotactile stimulation for the thumb, index and middle fingers using only two electrodes. The force information of the index and middle fingers was delivered into two electrodes, respectively, by intermittent stimulation. The information of the thumb was delivered to the user by inserting additional offset pulses onto the channel of the index finger based on an assumption that the force of thumb is proportional to the sum of the forces of the index and middle fingers. The presence of offset pulses in the channel indicates the binary state of thumb if it contacts with an object or not. We conducted two psychophysical experiments where healthy subjects classified the binary states of each finger and identified intensity from two-channel stimuli. The cognitive accuracies for intensity identification were 78.8% and 62.2% for the intermittent stimulation and conventional method, respectively, and accuracy for classifying the fingers was 93.1% for every combination of the three fingers. The results demonstrated that the proposed two-channel electrotactile stimulation could be an attractive method to express the information of fingers of prosthesis with high accuracy.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubt4"><b>TuBT4</b></a></td>
               <td class="r">#104</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubt4" title="Click to go to the Program at a Glance"><b>Medical Robot and Systems 2</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#106263" title="Click to go to the Author Index">Yi, Byung-Ju</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#122197" title="Click to go to the Author Index">Rabenorosoa, Kanty</a></td><td class="r">FEMTO-ST Inst</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt4_01">14:50-15:05, Paper TuBT4.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1214.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1214'); return false" title="Click to show or hide the keywords and abstract">Nonholonomic Closed-Loop Velocity Control of a Soft-Tethered Magnetic Capsule Endoscope</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168741" title="Click to go to the Author Index">Taddese, Addisu</a></td><td class="r">Vanderbilt Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190412" title="Click to go to the Author Index">Slawinski, Piotr</a></td><td class="r">Vanderbilt Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196609" title="Click to go to the Author Index">Obstein, Keith</a></td><td class="r">Vanderbilt Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101892" title="Click to go to the Author Index">Valdastri, Pietro</a></td><td class="r">Vanderbilt Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1214" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Surgical_Robotics" title="Click to go to the Keyword Index">Surgical Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a></span><br>
                           <strong>Abstract:</strong> In this paper, we demonstrate velocity-level closed loop control of a tethered magnetic capsule endoscope that is actuated via serial manipulator with a permanent magnet at its end-effector. Closed-loop control (2 degrees-of-freedom in position, and 2 in orientation) is made possible with the use of a real-time magnetic localization algorithm that utilizes the actuating magnetic field and thus does not require additional hardware. Velocity control is implemented to create smooth motion that is clinically necessary for colorectal cancer diagnostics. Our control algorithm generates a spline that passes through a set of input points that roughly defines the shape of the desired trajectory. The velocity controller acts in the tangential direction to the path, while a secondary position controller enforces a nonholonomic constraint on capsule motion. A soft nonholonomic constraint is naturally imposed by the lumen while we enforce a strict constraint for both more accurate estimation of tether disturbance and hypothesized intuitiveness for a clinician’s teleoperation. An integrating disturbance force estimation control term is introduced to predict the disturbance of the tether. This paper presents the theoretical formulations and experimental validation of our methodology. Results show the system’s ability to achieve a repeatable velocity step response with low steady-state error as well as ability of the tethered capsule to maneuver around a bend.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt4_02">15:05-15:20, Paper TuBT4.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1385.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1385'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Design and Closed-Loop Control of a Tri-Layer Polypyrrole Based Telescopic Soft Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196280" title="Click to go to the Author Index">Chikhaoui, Mohamed Taha</a></td><td class="r">FEMTO-ST Inst</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#164407" title="Click to go to the Author Index">Cot, Amélie</a></td><td class="r">Femto-St Inst</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122197" title="Click to go to the Author Index">Rabenorosoa, Kanty</a></td><td class="r">FEMTO-ST Inst</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#116978" title="Click to go to the Author Index">Rougeot, Patrick</a></td><td class="r">Univ. of Franche-Comté, FEMTO-ST Inst</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103294" title="Click to go to the Author Index">Andreff, Nicolas</a></td><td class="r">Univ. De Franche Comté</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1385" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1385.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Soft_bodied_Robots" title="Click to go to the Keyword Index">Soft-bodied Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Servoing" title="Click to go to the Keyword Index">Visual Servoing</a></span><br>
                           <strong>Abstract:</strong> A novel structure of a 2 DoF telescopic soft robot using a tri-layer Polypyrrole (PPy) soft micro-actuator with deployment is presented in this paper. The kinematic model is introduced and the Position Based Visual Servo (PBVS) control with path-planning and obstacle avoidance algorithms is developed. A prototype is presented and the control schemes are validated experimentally. A satisfactory accuracy with a sub-millimetric positioning error is obtained namely 287.6 microns for a circular path and 210 microns for obstacle avoidance.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt4_03">15:20-15:35, Paper TuBT4.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1475.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1475'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Robotic System for Percutaneous Coronary Intervention Equipped with a Steerable Catheter and Force Feedback Function</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#123078" title="Click to go to the Author Index">Cha, Hyo-Jeong</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117114" title="Click to go to the Author Index">Yoon, Hyun-Soo</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196494" title="Click to go to the Author Index">Jung, Kwan young</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106263" title="Click to go to the Author Index">Yi, Byung-Ju</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103310" title="Click to go to the Author Index">Lee, Sungon</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196490" title="Click to go to the Author Index">Won, Jong Yun</a></td><td class="r">Yonsei Univ. Coll. of Medicine</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1475" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1475.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a></span><br>
                           <strong>Abstract:</strong> Purpose of this study is to propose a new robotic system equipped with a steerable catheter for percutaneous coronary intervention. Key design issues of the system are less X-ray exposure, easy sterilization, and convenient user interface. At first, vascular intervention procedure using the robotic system equipped with a steerable catheter are analyzed and compared with the conventional procedure. A two directional steerable catheter with minimum modification of a conventional catheter is developed. Robotic parts which contact the devices were designed to be easily separable from the slave robot for sterilization. Master robots are compactly and intuitively designed to conduct steering, insertion, and rotational motion. One master robot conducts steering of a catheter and the other master robot handles insertion and rotation of a catheter and a guidewire. The master-slave robotic system is evaluated using the phantom. During the phantom experiment, the contact force of the guidewire tip is measured and reflected to the user through the master robot. The system usability was evaluated with respect to task completion time, usefulness of the steerable catheter and haptic interface. It is confirmed through the experiments that the master-slave robotic system with steerable catheter is well designed to help user perform vascular intervention successfully and efficiently.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt4_04">15:35-15:50, Paper TuBT4.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1479.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1479'); return false" title="Click to show or hide the keywords and abstract">Implicit Active Constraints for Safe and Effective Guidance of Unstable Concentric Tube Robots</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#157759" title="Click to go to the Author Index">Leibrandt, Konrad</a></td><td class="r">Imperial Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114412" title="Click to go to the Author Index">Bergeles, Christos</a></td><td class="r">Univ. Coll. London</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117925" title="Click to go to the Author Index">Yang, Guang-Zhong</a></td><td class="r">Imperial Coll. London</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1479" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Telerobotics" title="Click to go to the Keyword Index">Telerobotics</a></span><br>
                           <strong>Abstract:</strong> Safe and effective telemanipulation of concentric tube robots is hindered by their complex, non-intuitive kinematics. Guidance schemes in the form of attractive and repulsive constraints can simplify task execution and facilitate natural operation of the robot by clinicians. The real-time seamless calculation and application of guidance, however, requires computationally efficient algorithms that solve the non-linear inverse kinematics of the robot and guarantee that the commanded robot configuration is stable and sufficiently away from the anatomy. This paper presents a multi-processor framework that allows on-the-fly calculation of optimal safe paths based on rapid workspace and roadmap pre-computation. The real-time nature of the developed software enables complex guidance constraints to be implemented with minimal computational overhead. A user study on a simulated challenging clinical problem demonstrated that the incorporated guiding constraints are highly beneficial for fast and accurate navigation with concentric tube robots.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt4_05">15:50-16:05, Paper TuBT4.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1680.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1680'); return false" title="Click to show or hide the keywords and abstract">Towards Dynamic Object Manipulation with Tactile Sensing for Prosthetic Hands</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180401" title="Click to go to the Author Index">Shaw-Cortez, Wenceslao</a></td><td class="r">The Univ. of Melbourne</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107362" title="Click to go to the Author Index">Oetomo, Denny</a></td><td class="r">The Univ. of Melbourne</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#161715" title="Click to go to the Author Index">Manzie, Chris</a></td><td class="r">Univ. of Melbourne</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196884" title="Click to go to the Author Index">Choong, Peter</a></td><td class="r">The Univ. of Melbourne</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1680" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Systems__Healthcare__and_Assisted_Living" title="Click to go to the Keyword Index">Medical Systems, Healthcare, and Assisted Living</a>, <a href="IROS16_KeywordIndexMedia.html#Multifingered_Hands" title="Click to go to the Keyword Index">Multifingered Hands</a>, <a href="IROS16_KeywordIndexMedia.html#Manipulation_Planning_and_Control" title="Click to go to the Keyword Index">Manipulation Planning and Control</a></span><br>
                           <strong>Abstract:</strong> The manipulation of objects is an inherent capability of dexterous human hands. However, state-of-the-art prostheses are limited to only forming grasps and gestures due to the low information transfer rate in the human-machine interface. Additionally, in the prosthetic setting the hand is not privy to such global knowledge including the object's shape, weight, friction properties, etc. Existing techniques from robotics that can manipulate objects in this prosthetic setting require compensation terms which are counterproductive to the manipulation task. The incorporation of tactile sensing leads to a simpler control formulation and enables relaxation of several restrictive assumptions inherent in the robotics applications. In this work a novel control system is proposed that incorporates tactile sensing for object manipulation and optimal distribution of contact forces for prosthetic hands. Simulation results demonstrate the ability of the proposed control system to manipulate an object, while minimizing the use of frictional forces during manipulation.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubt5"><b>TuBT5</b></a></td>
               <td class="r">#105</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubt5" title="Click to go to the Program at a Glance"><b>Navigation</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#103148" title="Click to go to the Author Index">Stasse, Olivier</a></td><td class="r">CNRS</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#122966" title="Click to go to the Author Index">Liu, Yong</a></td><td class="r">Zhejiang Univ</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt5_01">14:50-15:05, Paper TuBT5.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0355.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('355'); return false" title="Click to show or hide the keywords and abstract">Experience-Based Path Planning for Mobile Robots Exploiting User Preferences</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184587" title="Click to go to the Author Index">Nardi, Lorenzo</a></td><td class="r">Univ. of Bonn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101642" title="Click to go to the Author Index">Stachniss, Cyrill</a></td><td class="r">Univ. of Bonn</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab355" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Path_Planning_for_Multiple_Mobile_Robots_or_Agents" title="Click to go to the Keyword Index">Path Planning for Multiple Mobile Robots or Agents</a>, <a href="IROS16_KeywordIndexMedia.html#Collision_Detection_and_Avoidance" title="Click to go to the Keyword Index">Collision Detection and Avoidance</a></span><br>
                           <strong>Abstract:</strong> The demand for flexible industrial robotic solutions that are able to accomplish tasks at different locations in a factory is growing more and more. When deploying mobile robots in a factory environment, the predictability and reproducibility of their behaviors become important and are often requested. In this paper, we propose an easy-to-use motion planning scheme that can take into account user preferences for robot navigation. The preferences are extracted implicitly from the previous experiences or from demonstrations and are automatically considered in the subsequent planning steps. This leads to reproducible and thus better to predict navigation behaviors of the robot, without requiring experts to hard-coding control strategies or cost functions within a planner. Our system has been implemented and evaluated on a simulated KUKA mobile robot in different environments.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt5_02">15:05-15:20, Paper TuBT5.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0910.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('910'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Motion Control of Tracked Vehicle Based on Contact Force Model</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196290" title="Click to go to the Author Index">Kojima, Shotaro</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107167" title="Click to go to the Author Index">Ohno, Kazunori</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101226" title="Click to go to the Author Index">Suzuki, Takahiro</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196014" title="Click to go to the Author Index">Westfechtel, Thomas</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110231" title="Click to go to the Author Index">Okada, Yoshito</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100118" title="Click to go to the Author Index">Tadokoro, Satoshi</a></td><td class="r">Tohoku Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab910" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0910.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Collision_Detection_and_Avoidance" title="Click to go to the Keyword Index">Collision Detection and Avoidance</a>, <a href="IROS16_KeywordIndexMedia.html#Environment_Monitoring_and_Management" title="Click to go to the Keyword Index">Environment Monitoring and Management</a></span><br>
                           <strong>Abstract:</strong> In large industrial plants, the inspection of production lines is a heavy and costly task that puts human inspectors at high risk. In order to overcome these challenges, we have developed an autonomous plant inspection system using a mobile tracked vehicle. In this paper, we propose an autonomous navigation method for tracked vehicles based on a contact force model that enables the robot to compensate for collisions with obstacles. The model considers the influence of the contact force on the linear and angular motion of the robot. Using the model,the controllable velocity range is derived during collisions. The experimental results show that the robot is safely controlled by complying with velocity constraints. In addition, our method can generate motions such as leaving wall, L-shaped curve and crosswise locomotion in straight passage while navigation alongside the walls. The method allows the robot to smoothly follow a target path, despite colliding with obstacles.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt5_03">15:20-15:35, Paper TuBT5.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1010.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1010'); return false" title="Click to show or hide the keywords and abstract">Navigation Planning for Legged Robots in Challenging Terrain</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192731" title="Click to go to the Author Index">Wermelinger, Martin</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#146641" title="Click to go to the Author Index">Fankhauser, Péter</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#185810" title="Click to go to the Author Index">Diethelm, Remo</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#165855" title="Click to go to the Author Index">Krüsi, Philipp Andreas</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100088" title="Click to go to the Author Index">Siegwart, Roland</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114045" title="Click to go to the Author Index">Hutter, Marco</a></td><td class="r">ETH Zurich</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1010" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Legged_Robots" title="Click to go to the Keyword Index">Legged Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a></span><br>
                           <strong>Abstract:</strong> This paper presents a framework for planning safe and efficient paths for a legged robot in rough and unstructured terrain. The proposed approach allows to exploit the distinctive obstacle negotiation capabilities of legged robots, while keeping the complexity low enough to enable planning over considerable distances in short time. We compute typical terrain characteristics such as slope, roughness, and steps to build a traversability map. This map is used to assess the costs of individual robot footprints as a function of the robot-specific obstacle negotiating capabilities for steps, gaps and stairs. Our sampling-based planner employs the <i>RRT*</i> algorithm to optimize path length and safety. The planning framework has a hierarchical architecture to frequently replan the path during execution as new terrain is perceived with onboard sensors. Furthermore, a cascaded planning structure makes use of different levels of simplification to allow for fast search in simple environments, while retaining the ability to find complex solutions, such as paths through narrow passages. The proposed navigation planning framework is integrated on the quadrupedal robot StarlETH and extensively tested in simulation as well as on the real platform.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt5_04">15:35-15:50, Paper TuBT5.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1159.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1159'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Autonomous Navigation in Dynamic Social Environments Using Multi-Policy Decision Making</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#193193" title="Click to go to the Author Index">Mehta, Dhanvin</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#141803" title="Click to go to the Author Index">Ferrer, Gonzalo</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107201" title="Click to go to the Author Index">Olson, Edwin</a></td><td class="r">Univ. of Michigan</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1159" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1159.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Service_Robots" title="Click to go to the Keyword Index">Service Robots</a></span><br>
                           <strong>Abstract:</strong> In dynamic environments crowded with people, robot motion planning becomes difficult due to the complex and tightly-coupled interactions between agents. Trajectory planning methods, supported by models of typical human behavior and personal space, often produce reasonable behavior. However, they do not account for the future closed-loop interactions of other agents with the trajectory being constructed. As a consequence, the trajectories are unable to anticipate cooperative interactions (such as a human yielding), or adverse interactions (such as the robot blocking the way).<p>In this paper, we propose a new method for navigation amongst pedestrians in which the trajectory of the robot is not explicitly planned, but instead, a planning process selects one of a set of closed-loop behaviors whose utility can be predicted through forward simulation. In particular, we extend Multi-Policy Decision Making (MPDM) to this domain using the closed-loop behaviors Go-Solo, Follow-other, and Stop. By dynamically switching between these policies, we show that we can improve the performance of the robot as measured by utility functions that reward task completion and penalize inconvenience to other agents. Our evaluation includes extensive results in simulation and real-world experiments.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt5_05">15:50-16:05, Paper TuBT5.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1431.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1431'); return false" title="Click to show or hide the keywords and abstract">Towards Online Characterization of Autonomously Navigating Robots in Unstructured Environments</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#151088" title="Click to go to the Author Index">Twigg, Jeffrey</a></td><td class="r">Army Res. Lab</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#172624" title="Click to go to the Author Index">Gregory, Jason M.</a></td><td class="r">US Army Res. Lab</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105310" title="Click to go to the Author Index">Fink, Jonathan</a></td><td class="r">ARL</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1431" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a></span><br>
                           <strong>Abstract:</strong> Autonomous platforms are confronted by a diversity of challenges in unstructured environments, which make monitoring performance a non-trivial task. Some of these environments are so complex that they preclude persistent, nearby operator oversight. This absence of oversight motivates the need for an online monitoring system, specifically for robots operating in difficult environments. We develop a test methodology and set of online monitoring metrics by extending methods for characterizing robotic-systems using offline metrics. We implement this test methodology in an unstructured, outdoor environment and show the resulting performance information gained from our online monitoring solution. This online monitoring approach is generalizable such that it characterizes any robotic system that meets our set of hardware and software criteria.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubt6"><b>TuBT6</b></a></td>
               <td class="r">#106</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubt6" title="Click to go to the Program at a Glance"><b>Localization and Mapping</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#104535" title="Click to go to the Author Index">Eustice, Ryan</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#118282" title="Click to go to the Author Index">Chung, Soon-Jo</a></td><td class="r">Caltech</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt6_01">14:50-15:05, Paper TuBT6.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0139.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('139'); return false" title="Click to show or hide the keywords and abstract">Efficient Loop Closure Based on FALKO LIDAR Features for Online Robot Localization and Mapping</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#174363" title="Click to go to the Author Index">Kallasi, Fabjan</a></td><td class="r">Univ. Degli Studi Di Parma</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103311" title="Click to go to the Author Index">Lodi Rizzini, Dario</a></td><td class="r">Univ. of Parma</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab139" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a></span><br>
                           <strong>Abstract:</strong> Keypoint features detection from measurements enables efficient localization and map estimation through the compact representation and recognition of locations. The keypoint detector FALKO has been proposed to detect stable points in laser scans for localization and mapping tasks. In this paper, we present novel loop closure methods based on FALKO keypoints and compare their performance in online localization and mapping problems. The pose graph formulation is adopted, where each pose is associated to a local map of keypoints extracted from the corresponding laser scan. Loops in the graph are detected by matching local maps in two steps. First, the candidate matching scans are selected by comparing the scan signatures obtained from the keypoints of each scan. Second, the transformation between two scans is obtained by pairing and aligning the respective keypoint sets. Experiments with standard benchmark datasets assess the performance of FALKO and of the proposed loop closure algorithms in both offline and online localization and map estimation.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt6_02">15:05-15:20, Paper TuBT6.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0208.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('208'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>MO-SLAM: Multi Object SLAM with Run-Time Object Discovery through Duplicates</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195511" title="Click to go to the Author Index">Dharmasiri, Thanuja</a></td><td class="r">Monash Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#154483" title="Click to go to the Author Index">Lui, Vincent</a></td><td class="r">Monash Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#156512" title="Click to go to the Author Index">Drummond, Tom</a></td><td class="r">Monash Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab208" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0208.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a>, <a href="IROS16_KeywordIndexMedia.html#SLAM" title="Click to go to the Keyword Index">SLAM</a></span><br>
                           <strong>Abstract:</strong> In this paper, we present MO-SLAM, a novel visual SLAM system that is capable of detecting duplicate objects in the scene during run-time without requiring an offline training stage to pre-populate a database of objects. Instead, we propose a novel method to detect landmarks that belong to duplicate objects. Further, we show how landmarks belonging to duplicate objects can be converted to first-order entities which generate additional constraints for optimizing the map. We evaluate the performance of MO-SLAM with extensive experiments on both synthetic and real data, where the experimental results verify the capabilities of MO-SLAM in detecting duplicate objects and using these constraints to improve the accuracy of the map.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt6_03">15:20-15:35, Paper TuBT6.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0530.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('530'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Pop-Up SLAM: Semantic Monocular Plane SLAM for Low-Texture Environments</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191709" title="Click to go to the Author Index">Yang, Shichao</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#150597" title="Click to go to the Author Index">Song, Yu</a></td><td class="r">Beijing Jiaotong Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104298" title="Click to go to the Author Index">Kaess, Michael</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104304" title="Click to go to the Author Index">Scherer, Sebastian</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab530" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0530.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#SLAM" title="Click to go to the Keyword Index">SLAM</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Learning" title="Click to go to the Keyword Index">Visual Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a></span><br>
                           <strong>Abstract:</strong> Existing simultaneous localization and mapping (SLAM) algorithms are not robust in challenging low-texture environments because there are only few salient features. The resulting sparse or semi-dense map also conveys little information for motion planning. Though some work utilize plane or scene layout for dense map regularization, they require decent state estimation from other sources. In this paper, we propose real-time monocular plane SLAM to demonstrate that scene understanding could improve both state estimation and dense mapping especially in low-texture environments. The plane measurements come from a pop-up 3D plane model applied to each single image. We also combine planes with point based SLAM to improve robustness. On a public TUM dataset, our algorithm generates a dense semantic 3D model with pixel depth error of 6.2 cm while existing SLAM algorithms fail. On a 60 m long dataset with loops, our method creates a much better 3D model with state estimation error of 0.67%.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt6_04">15:35-15:50, Paper TuBT6.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1024.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1024'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Utilizing High-Dimensional Features for Real-Time Robotic Applications: Reducing the Curse of Dimensionality for Recursive Bayesian Estimation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#170333" title="Click to go to the Author Index">Li, Jie</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160042" title="Click to go to the Author Index">Ozog, Paul</a></td><td class="r">Ford Motor Company</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#185044" title="Click to go to the Author Index">Abernethy, Jacob</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104535" title="Click to go to the Author Index">Eustice, Ryan</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118789" title="Click to go to the Author Index">Johnson-Roberson, Matthew</a></td><td class="r">Univ. of Michigan</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1024" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1024.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Visual_Navigation" title="Click to go to the Keyword Index">Visual Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a></span><br>
                           <strong>Abstract:</strong> Feature learning has become popular in robotics due to recent advances in machine learning. In this paper, we propose a novel method to utilize the high-dimensional features from these techniques as observations in Bayesian estimation problems in a real-time manner. We develop an approach that: 1) pre-processes the observations and maps them into a new space with both reduced dimensions and a linear relationship to the estimation states; and 2) estimates the uncertainty of resulting outputs using data perturbation. The result is that deep learning approaches can be combined with more traditional filtering approaches like the Kalman filter to achieve state-of-the-art real-time performance. We validate the method by presenting the first real-time application of underwater robot localization using an imaging sonar. The proposed technique shows similar localization accuracy to benchmark approaches while simultaneously achieving real-time performance.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt6_05">15:50-16:05, Paper TuBT6.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1517.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1517'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Visual-Inertial Curve Slam</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#166248" title="Click to go to the Author Index">Meier, Kevin</a></td><td class="r">Univ. of Illinois at Urbana-Champaign</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118282" title="Click to go to the Author Index">Chung, Soon-Jo</a></td><td class="r">Univ. of Illinois at Urbana-Champaign</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101611" title="Click to go to the Author Index">Hutchinson, Seth</a></td><td class="r">Univ. of Illinois</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1517" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1517.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#SLAM" title="Click to go to the Keyword Index">SLAM</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Navigation" title="Click to go to the Keyword Index">Visual Navigation</a></span><br>
                           <strong>Abstract:</strong> We present a simultaneous localization and mapping (SLAM) algorithm that uses B'{e}zier curves as static landmark primitives rather than sparse feature points. Our approach allows us to estimate the full 6-DOF pose of a robot while providing a structured map which can be used to assist a robot in motion planning and control. We demonstrate how to reconstruct the 3-D location of curve landmarks from a stereo pair without searching for point-based stereo correspondences and how to compare the 3-D shape of curve landmarks between chronologically sequential stereo frames to solve the data association problem. We present a method to combine curve landmarks for mapping purposes, resulting in a map with a continuous set of curves that contain fewer landmark states than conventional sparse point-based SLAM algorithms. Note, to combine curves, we assume the curved landmarks are fixed to a larger curved object naturally occurring in the scene. While our algorithm is less accurate than point-based SLAM algorithms, we are able to create maps with considerably less landmark states and our algorithm can operate in settings lacking texture.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubt7"><b>TuBT7</b></a></td>
               <td class="r">#107</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubt7" title="Click to go to the Program at a Glance"><b>Manufacturing and Automation</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#178414" title="Click to go to the Author Index">Boesl, Dominik B. O.</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#155162" title="Click to go to the Author Index">Felton, Samuel</a></td><td class="r">Harvard Univ</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt7_01">14:50-15:05, Paper TuBT7.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0028.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('28'); return false" title="Click to show or hide the keywords and abstract">Programming Robotic Tool-Path and Tool-Orientations for Conformance Grinding Based on Human Demonstration</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169267" title="Click to go to the Author Index">Ng, Wu Xin Charles</a></td><td class="r">Nanyang Tech. Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#173098" title="Click to go to the Author Index">Chan, Kelvin Hau-Kong</a></td><td class="r">Rolls-Royce Singapore</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#173099" title="Click to go to the Author Index">Teo, Wee Kin</a></td><td class="r">Rolls-Royce Singapore</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100042" title="Click to go to the Author Index">Chen, I-Ming</a></td><td class="r">Nanyang Tech. Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab28" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Manufacturing_and_Automation" title="Click to go to the Keyword Index">Manufacturing and Automation</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a>, <a href="IROS16_KeywordIndexMedia.html#Industrial_Robots" title="Click to go to the Keyword Index">Industrial Robots</a></span><br>
                           <strong>Abstract:</strong> This paper describes a novel methodology for programming grinding tool-paths, tool-path orientations and grinding strategies based on the captured trajectories of a surface finishing tool operated by the Skilled-Operator. In order to extract the grinding parameters and strategy from the trajectory of the skilled operator, the manual tool-path is first segmented into tool-path primitives. The order of the manual tool-path primitives will form the grinding strategy. Next, the robotic tool-path primitives are then generated based on the boundaries of the manual tool-path primitive in a computer-aided-manufacturing software. The orientations of the manual tool-path are used to anchor the orientations of the robotic tool-path. Spherical linear interpolation and spherical spline quaternion interpolation are used to interpolate the orientations for the robotic tool-path points along the cross curves, followed by the flow curves respectively. The programmed robotic tool-paths generated were subsequently applied and proven to be able to grind the work-piece to the desired profile within the desired tolerance.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt7_02">15:05-15:20, Paper TuBT7.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0997.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('997'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Feedback-Controlled Self-Folding of Autonomous Robot Collectives</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192478" title="Click to go to the Author Index">Nisser, Martin</a></td><td class="r">Harvard Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#155162" title="Click to go to the Author Index">Felton, Samuel</a></td><td class="r">Harvard Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#130599" title="Click to go to the Author Index">Tolley, Michael Thomas</a></td><td class="r">Univ. of California, San Diego</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#109387" title="Click to go to the Author Index">Rubenstein, Michael</a></td><td class="r">Northwestern Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102429" title="Click to go to the Author Index">Wood, Robert</a></td><td class="r">Harvard Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab997" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0997.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Manufacturing_and_Automation" title="Click to go to the Keyword Index">Manufacturing and Automation</a>, <a href="IROS16_KeywordIndexMedia.html#Soft_bodied_Robots" title="Click to go to the Keyword Index">Soft-bodied Robots</a></span><br>
                           <strong>Abstract:</strong> Self-folding provides an efficient way of creating complex 3D geometries from 2D composites. However, the precision of self-folding structures is often limited by the use of open-loop folding mechanisms. In this paper we demonstrate feedback-controlled self-folding using a shape memory polymer and optical sensors to accurately control folding angles. We present a method of quickly and inexpensively fabricating large collectives of self-folding autonomous robots that can be transported in flat configurations prior to autonomous deployment at target destinations. To demonstrate this, we build a collective of robots that is manufactured in one continuous laminar composite. Individual robots in the collective detach from one other, self-fold into pre-programmed configurations and navigate by phototaxis. This method could be applied to a broad range of applications where logistics necessitate compact transport and where external manipulation is difficult or expensive, such as in space applications or delivering search-and-rescue robots in cluttered environments.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt7_03">15:20-15:35, Paper TuBT7.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1285.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1285'); return false" title="Click to show or hide the keywords and abstract">4 Robotic Revolutions - Proposing a Holistic Phase Model Describing Future Disruptions in the Evolution of Robotics and Automation and the Rise of a New Generation ‘R’ of Robotic Natives</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178414" title="Click to go to the Author Index">Boesl, Dominik B. O.</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#149630" title="Click to go to the Author Index">Liepert, Bernd</a></td><td class="r">KUKA AG / KUKA Lab. GmbH</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1285" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Manufacturing_and_Automation" title="Click to go to the Keyword Index">Manufacturing and Automation</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a></span><br>
                           <strong>Abstract:</strong> This paper describes the holistic phase model of the ‘4 Robotic Revolutions’ - disruptive waves of evolution in robotics and automation - and their impact on society, economy and other areas of human life over the next 50 years. Furthermore, an explanation is provided, why a new Generation ‘R’ of Robotic Natives will grow up in daily contact with automation technology perceiving technology as valuable enrichments for their lives. The paper closes with an outlook into reasons why humankind has to ignite a holistic discourse about the responsible application of these disruptive technologies in order to shape a sustainable future enhanced (and not threatened) by robotics and automation. In consequence, the need for self-regulation in the sense of Technology and Robotic Governance will arise.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt7_04">15:35-15:50, Paper TuBT7.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1568.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1568'); return false" title="Click to show or hide the keywords and abstract">An Assembly Sequence Generation of a Product Family for Robot Programming</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#163962" title="Click to go to the Author Index">Lee, Kimoon</a></td><td class="r">Georgia Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#167555" title="Click to go to the Author Index">Joo, Sungmoon</a></td><td class="r">Georgia Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102379" title="Click to go to the Author Index">Christensen, Henrik Iskov</a></td><td class="r">Georgia Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1568" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Manufacturing_and_Automation" title="Click to go to the Keyword Index">Manufacturing and Automation</a>, <a href="IROS16_KeywordIndexMedia.html#Manipulation_and_Compliant_Assembly" title="Click to go to the Keyword Index">Manipulation and Compliant Assembly</a>, <a href="IROS16_KeywordIndexMedia.html#AI_Reasoning_Methods" title="Click to go to the Keyword Index">AI Reasoning Methods</a></span><br>
                           <strong>Abstract:</strong> In this paper, we present an efficient assembly sequence generation method for robot programming, which can reduce the complexity of assembly sequence generation for products within a product family. Today, principle markets for robot assembly automation (e.g., automobile and consumer electronics) have a relatively short product run rate, ranging from six to twelve months. To accommodate this rapidly changing production demand, in most cases, manufacturers design a series of new products within a product family (i.e., a group of products derived from a common product platform) so that they have similar assembly sequences. Seemingly robot programming for a new product within a product family does not require extensive programming adjustments, but it is still complex and manufacturers spend large amount time and money in programming robots for each new product. To solve this problem, we present a new assembly sequence generation method. With a case-based planner, we generate feasible and similar-to-real assembly sequence candidates for a new product from assembly sequences of old products within a product family. Then, we use geometric information extracted from its CAD model to evaluate those candidate sequences. Unlike those used by previous studies, our method can generate an assembly sequence that is more suitable for the concept of a reconfigurable assembly system for general-purpose robots (assembly sequence generation for robot programming); more similar to assembly sequences used in real assembly systems (case-based planning); and more properly evaluated for robot assembly (a CAD model). We apply the proposed method to a toy airplane assembly scenario and demonstrate its feasibility
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubt8"><b>TuBT8</b></a></td>
               <td class="r">#108</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubt8" title="Click to go to the Program at a Glance"><b>(Special Session) New Horizon for Robot Audition Applications</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#107173" title="Click to go to the Author Index">Nakadai, Kazuhiro</a></td><td class="r">Honda Res. Inst. Japan Co., Ltd</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#103129" title="Click to go to the Author Index">Okuno, Hiroshi G.</a></td><td class="r">Waseda Univ</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt8_01">14:50-15:05, Paper TuBT8.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0621.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('621'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Hearing Support System Using Environment Sensor Network</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#109330" title="Click to go to the Author Index">Ishi, Carlos Toshinori</a></td><td class="r">ATR</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#131744" title="Click to go to the Author Index">Liu, Chaoran</a></td><td class="r">Osaka Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#116316" title="Click to go to the Author Index">Even, Jani</a></td><td class="r">ATR</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101694" title="Click to go to the Author Index">Hagita, Norihiro</a></td><td class="r">ATR</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab621" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0621.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a>, <a href="IROS16_KeywordIndexMedia.html#Voice__Speech_Synthesis_and_Recognition" title="Click to go to the Keyword Index">Voice, Speech Synthesis and Recognition</a>, <a href="IROS16_KeywordIndexMedia.html#Medical_Systems__Healthcare__and_Assisted_Living" title="Click to go to the Keyword Index">Medical Systems, Healthcare, and Assisted Living</a></span><br>
                           <strong>Abstract:</strong> In order to solve the problems of current hearing aid devices, we make use of environment sensor network, and propose a hearing support system, where individual target and anti-target sound sources in the environment can be selected, and spatial information of the target sound sources is reconstructed. The performance of the selective sound separation module was evaluated for different noise conditions. Results showed that signal-to-noise ratios of around 15dB could be achieved by the proposed system for a 65dB babble noise plus directional music noise condition. In the same noise condition, subjective intelligibility tests were conducted, and an improvement of 65 to 90% word intelligibility rates could be achieved by using the proposed hearing support system.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt8_02">15:05-15:20, Paper TuBT8.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0656.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('656'); return false" title="Click to show or hide the keywords and abstract">Ego-Noise Reduction Using a Motor Data-Guided Multichannel Dictionary</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196078" title="Click to go to the Author Index">Schmidt, Alexander</a></td><td class="r">FAU Erlangen</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#156429" title="Click to go to the Author Index">Deleforge, Antoine</a></td><td class="r">Inria Rennes - Bretagne Atlantique</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101508" title="Click to go to the Author Index">Kellermann, Walter</a></td><td class="r">Univ. Erlangen</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab656" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a></span><br>
                           <strong>Abstract:</strong> We address the problem of ego-noise reduction, i.e., suppressing the noise a robot causes by its own motions. Such noise degrades the recorded microphone signal massively such that the robot's auditory capabilities suffer. To suppress it, it is intuitive to use also motor data, since it provides additional information about the robot's joints and thereby the noise sources. We propose to fuse motor data to a recently proposed multichannel dictionary algorithm for ego-noise reduction. At training, a dictionary is learned that captures spatial and spectral characteristics of ego-noise. At testing, nonlinear classifiers are used to efficiently associate the current robot's motor state to relevant sets of entries in the learned dictionary. By this, computational load is reduced by one third in typical scenarios while achieving at least the same noise reduction performance. Moreover, we propose to train dictionaries on different microphone array geometries and use them for ego-noise reduction while the head to which the microphones are mounted is moving. In such scenarios, the motor guided approach results in significantly better performance values.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt8_03">15:20-15:35, Paper TuBT8.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0679.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('679'); return false" title="Click to show or hide the keywords and abstract">Semi-Automatic Bird Song Analysis by Spatial-Cue-Based Integration of Sound Source Detection, Localization, Separation, and Identification</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183594" title="Click to go to the Author Index">Kojima, Ryosuke</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#109917" title="Click to go to the Author Index">Sugiyama, Osamu</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195525" title="Click to go to the Author Index">Suzuki, Reiji</a></td><td class="r">Nagoya Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107173" title="Click to go to the Author Index">Nakadai, Kazuhiro</a></td><td class="r">Honda Res. Inst. Japan Co., Ltd</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196449" title="Click to go to the Author Index">Taylor, Charles</a></td><td class="r">UCLA</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab679" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a>, <a href="IROS16_KeywordIndexMedia.html#Model_Learning" title="Click to go to the Keyword Index">Model Learning</a></span><br>
                           <strong>Abstract:</strong> This paper addresses bird song analysis based on semi-automatic annotation. Research in animal behavior, especially with birds,	would be aided by automated (or semi-automated) systems that can localize sounds, measure their timing, and identify their source. This is difficult to achieve in real environments where several birds may be singing from different locations and at the same time. Analysis of recordings from the wild has in the past typically required manual annotation. Such annotation is not always accurate or even consistent, as it may vary both within or between observers. Here we propose a system that that uses automated methods from robot audition, including sound source detection, localization, separation and identification. In robot audition these technologies have typically been studied separately; combining them often leads to poor performance in real-time application from the wild. We suggest that integration is aided by placing a primary focus on spatial cues, then combining other features within a Bayesian framework. A second problem has been that supervised machine learning methods typically requires a pre-trained model that may require a large training set of annotated labels. We have employed a semi-automatic annotation approach that requires much less pre-annotation. Preliminary experiments with recordings of bird songs from the wild revealed that for identification accuracy our system outperformed a method based on convention robot audition.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt8_04">15:35-15:50, Paper TuBT8.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1325.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1325'); return false" title="Click to show or hide the keywords and abstract">Probabilistic 3D Sound Sources Mapping Using Moving Microphone Array</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105000" title="Click to go to the Author Index">Sasaki, Yoko</a></td><td class="r">National Inst. of Advanced Industrial Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196653" title="Click to go to the Author Index">Tanabe, Ryo</a></td><td class="r">Tokyo Univ. of Science</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#121123" title="Click to go to the Author Index">Takemura, Hiroshi</a></td><td class="r">Tokyo Univ. of Science</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1325" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Fusion" title="Click to go to the Keyword Index">Sensor Fusion</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a></span><br>
                           <strong>Abstract:</strong> The paper proposes a system for mapping the 3D location of a sound source using data from a microphone array, each of which gives an independent estimate of the direction. LiDAR is used to generate a 3D map of the environment and to estimate the location of the sensor in six degrees of freedom (6-DoF). By combing these modules, our system determines the direction to a sound source in 3D global space for a moving robot. From evaluation of the time series of the tracked sound stream, the position of the sound source is estimated by using the Monte Carlo localization approach. When estimated from a moving robot, an approach based on triangulation does not always adequately locate a sound source, because the estimate depends on the relationship between the location of the source and that of the moving robot. The main advantage of the proposed system is that as an audible signal is perceived, it continuously estimates the location of the source. We evaluate the system by performing an experiment using a hand-held sensor.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt8_05">15:50-16:05, Paper TuBT8.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1449.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1449'); return false" title="Click to show or hide the keywords and abstract">Partially Shared Deep Neural Network in Sound Source Separation and Identification Using a UAV-Embedded Microphone Array</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196772" title="Click to go to the Author Index">Morito, Takayuki</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#109917" title="Click to go to the Author Index">Sugiyama, Osamu</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183594" title="Click to go to the Author Index">Kojima, Ryosuke</a></td><td class="r">Tokyo Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107173" title="Click to go to the Author Index">Nakadai, Kazuhiro</a></td><td class="r">Honda Res. Inst. Japan Co., Ltd</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1449" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a>, <a href="IROS16_KeywordIndexMedia.html#Search_and_Rescue_Robots" title="Click to go to the Keyword Index">Search and Rescue Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Recognition" title="Click to go to the Keyword Index">Recognition</a></span><br>
                           <strong>Abstract:</strong> This paper addresses sound source separation and identification for noise-contaminated acoustic signals recorded with a microphone array embedded in an Unmanned Aerial Vehicle (UAV), aiming at people's voice detection quickly and widely in a disaster situation. The key approach to achieve this is Deep Neural Network (DNN), but it is well known that training a DNN needs a huge dataset to improve its performance. In a practical application, building such a dataset is not often realistic owing to the cost of manual data annotation. Therefore, we propose a Partially-Shared Deep Neural Network (PS-DNN) which can learn multiple tasks at the same time with a small amount of annotated data. Preliminary results show that the PS-DNN outperforms conventional DNN-based approaches which require fully-annotated data in training in terms of identification accuracy. In addition, it maintains performance even when noise-suppressed signals are used for sound source separation training, and partially annotated data is used for sound source identification training.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubt9"><b>TuBT9</b></a></td>
               <td class="r">#204~205</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubt9" title="Click to go to the Program at a Glance"><b>Marine Robots 1</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#104298" title="Click to go to the Author Index">Kaess, Michael</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#102702" title="Click to go to the Author Index">Kruusmaa, Maarja</a></td><td class="r">Tallinn Univ. of Tech</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt9_01">14:50-15:05, Paper TuBT9.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0198.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('198'); return false" title="Click to show or hide the keywords and abstract">A Discrete Dipole Approximation Approach to Underwater Active Electrosense Problems</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195474" title="Click to go to the Author Index">Wang, Ke</a></td><td class="r">Curtin Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#120525" title="Click to go to the Author Index">Cui, Lei</a></td><td class="r">Curtin Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103290" title="Click to go to the Author Index">Do, Khac Duc</a></td><td class="r">Univ. of Western Australia</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab198" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Biomimetics" title="Click to go to the Keyword Index">Biomimetics</a></span><br>
                           <strong>Abstract:</strong> Weakly electric fish use self-established electric field to sense the underwater environment that may be cluttered and turbid. Previous works on building artificial counterparts are limited to simplest cases, as no analytical solutions exist under complex boundary conditions. Universal numerical approaches like Finite Element Method (FEM) and Boundary Element Method (BEM) suffer from lengthy meshing process and heavily computational burden. In this paper, discrete dipole approximation (DDA), which is widely used in light scattering and absorption problems, was for the first time proposed to be applied for underwater electrosense. This approach is lightweight, flexible and computationally efficient compared with FEM. It was simulated in electric fields excited by parallel-plate electrodes and spherical electrodes of a simplified robotic model. A constrained unscented Kalman filter (CUKF) was further utilized to localize the position and identify the size of an invading cube. Results comparison with FEM indicate the differences of a cuboidal object in two orthogonal positions were 7.10% and 10.46% respectively, and the difference in size was 11.82%. These results were achieved at a cost of less than 1% of the computational effort of the FEM. The proposed approach proved effective from the simulation results and laid a solid foundation for real-time underwater active electrosense in a more general environment.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt9_02">15:05-15:20, Paper TuBT9.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0366.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('366'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Planning Feasible and Safe Paths Online for Autonomous Underwater Vehicles in Unknown Environments</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169624" title="Click to go to the Author Index">Hernández, Juan David</a></td><td class="r">Univ. De Girona</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107016" title="Click to go to the Author Index">Moll, Mark</a></td><td class="r">Rice Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171944" title="Click to go to the Author Index">Vidal Garcia, Eduard</a></td><td class="r">Univ. De Girona</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105805" title="Click to go to the Author Index">Carreras, Marc</a></td><td class="r">Univ. De Girona</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102678" title="Click to go to the Author Index">Kavraki, Lydia</a></td><td class="r">Rice Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab366" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0366.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a></span><br>
                           <strong>Abstract:</strong> We present a framework for planning collision-free and safe paths online for autonomous underwater vehicles (AUVs) in unknown environments. We build up on our previous work and propose an improved approach. While preserving its main modules (mapping, planning and mission handler), the framework now considers motion constraints to plan feasible paths, ie those that meet vehicle's motion capabilities. The new framework also incorporates a risk function to avoid navigating close to nearby obstacles, and reuses the last best known solution to eliminate time-consuming pruning routines. To evaluate this approach, we use the SPARUS-II AUV, a torpedo-shaped vehicle performing autonomous missions in a 2-dimensional workspace. We validate the framework's new features by solving tasks in both simulation and real-world in-water trials and comparing results with our previous approach.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt9_03">15:20-15:35, Paper TuBT9.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0396.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('396'); return false" title="Click to show or hide the keywords and abstract">Motion Control Architecture of a 4-Fin U-CAT AUV Using DOF Prioritization</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#132109" title="Click to go to the Author Index">Salumae, Taavi</a></td><td class="r">Tallinn Univ. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117709" title="Click to go to the Author Index">Chemori, Ahmed</a></td><td class="r">Cnrs / Lirmm</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102702" title="Click to go to the Author Index">Kruusmaa, Maarja</a></td><td class="r">Tallinn Univ. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab396" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Biologically_Inspired_Robots" title="Click to go to the Keyword Index">Biologically-Inspired Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a></span><br>
                           <strong>Abstract:</strong> This paper demonstrates a novel motion control approach for biomimetic underwater vehicles with pitching fins. Even though these vehicles are highly maneuverable, the actuation of their different degrees of freedom (DOFs) is strongly coupled. To address this problem, we propose to use smooth DOF prioritization depending on which maneuver the vehicle is about to do. DOF prioritization has allowed us to develop a modular, easily applicable and extendable motion control architecture for U-CAT vehicle, which is meant for archaeological shipwreck penetration. We demonstrate the benefits of this architecture by developing an remotely operated vehicle autopilot for depth and yaw using a nonlinear state feedback controller. We also show the extensibility of the approach by controlling 3 DOFs of a fully autonomous U-CAT. The real-time experimental results show high position tracking precision (depth RMS error: 1.9 cm; yaw RMS error: 2.5 deg) Comparative experiments justify the use of DOF prioritization.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt9_04">15:35-15:50, Paper TuBT9.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0819.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('819'); return false" title="Click to show or hide the keywords and abstract">Supporting AUV Localisation through Next Generation Underwater Acoustic Networks: Results from the Field</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#124220" title="Click to go to the Author Index">Munafò, Andrea</a></td><td class="r">NATO STO Centre for Maritime Res. and Experimentation</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196285" title="Click to go to the Author Index">Furfaro, Thomas</a></td><td class="r">NATO STO Centre for Maritime Res. and Experimentation</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103339" title="Click to go to the Author Index">Ferri, Gabriele</a></td><td class="r">NATO Centre for Maritime Res. and Experimentation</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#156517" title="Click to go to the Author Index">Alves, Joao</a></td><td class="r">CMRE (Center for Marine Res. and Experimentation)</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab819" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Networks" title="Click to go to the Keyword Index">Sensor Networks</a></span><br>
                           <strong>Abstract:</strong> This work describes how the localisation of autonomous underwater vehicles can be supported through networked acoustic communication. The localisation approach includes timing information within acoustic messages to create an interrogation scheme similar to that of long-baseline methods, but realised at the application level of the network. In this way, the network itself is able to provide vehicle localisation information, reducing the needs for additional on-board sensors or dedicated deployed platforms/transponders. The aim of this work is to report at sea localisation results as obtained in two completely different application scenarios. The first one is represented by the FP7 MORPH project (HORTA15 sea trial) where the proposed approach has been applied to support navigation of a fleet of AUVs in a tight formation and in very shallow waters. The second application is represented by the NATO STO CMRE multistatic network demonstrator with the acoustic network used to support navigation of AUVs working in large operational areas. In this latter case, results are given from the COLLAB-NGAS14 experimental campaign.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt9_05">15:50-16:05, Paper TuBT9.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0982.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('982'); return false" title="Click to show or hide the keywords and abstract">Incremental Data Association for Acoustic Structure from Motion</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179401" title="Click to go to the Author Index">Huang, Tiffany A.</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104298" title="Click to go to the Author Index">Kaess, Michael</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab982" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#SLAM" title="Click to go to the Keyword Index">SLAM</a></span><br>
                           <strong>Abstract:</strong> We provide a novel incremental data association method to complement our previous work on acoustic structure from motion (ASFM), which recovers 3D scene structure from multiple 2D sonar images, while at the same time localizing the sonar. Given point features extracted from multiple overlapping sonar images, our algorithm automatically finds the correspondences between the features. Our data association method uses information about the geometric correlations of the entire set of landmarks to reject spurious measurements or false positives that might otherwise have been accepted. For each new sonar measurement, the algorithm uses a gating procedure to narrow the landmark match search space. Using the pruned surviving candidate correspondences, we identify the correct hypothesis based on a posterior compatibility cost, penalizing for null matches to avoid all measurements being declared new landmarks. Unlike other methods, ASFM does not require any planar scene assumptions and uses constraints from more than two images to increase accuracy in both mapping and localization. We evaluate our algorithm in simulation and demonstrate successful data association results on real sonar images.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tubt10"><b>TuBT10</b></a></td>
               <td class="r">#206~208</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tubt10" title="Click to go to the Program at a Glance"><b>Humanoid Robots 2</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#103420" title="Click to go to the Author Index">Bicchi, Antonio</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#117307" title="Click to go to the Author Index">Yamamoto, Ko</a></td><td class="r">Univ. of Tokyo</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt10_01">14:50-15:05, Paper TuBT10.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0129.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('129'); return false" title="Click to show or hide the keywords and abstract">Walking-Wheeling Dual Mode Strategy for Humanoid Robot, DRC-HUBO+</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158412" title="Click to go to the Author Index">Bae, HyoIn</a></td><td class="r">KAIST, HuboLab</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#144839" title="Click to go to the Author Index">Lee, In Ho</a></td><td class="r">KAIST, HUBO Lab</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#188494" title="Click to go to the Author Index">Jung, Taejin</a></td><td class="r">KAIST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102941" title="Click to go to the Author Index">Oh, Jun Ho</a></td><td class="r">Korea Advanced Inst. of Sci. and Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab129" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Wheeled_Robots" title="Click to go to the Keyword Index">Wheeled Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Robotics_in_Hazardous_Fields" title="Click to go to the Keyword Index">Robotics in Hazardous Fields</a></span><br>
                           <strong>Abstract:</strong> For robots, a flexible approach is very important. Even when the robot is a humanoid, it does not have to perfectly mimic human motion. If necessary, the humanoid robot can adapt to other technologies even when these technologies make it different from humans. In a real environment or a disaster environment, humanoid robots are a good solution to assist humans. However, the stability problem of biped locomotion remains. In this paper, we introduce a new strategy of attaching a wheel to our humanoid robot in order to achieve mobility efficiency with a bipedal walking and wheel driven mode. Additionally, this strategy can enhance the stability. Through the proposed method, the humanoid robot can traverse uneven terrain and climb stairs with a walking motion, and can move safely with wheels on the ground. In addition, a method will be presented that can enhance the performance of wheel movement by using redundant joints and sensors on the humanoid robot. Various experiments and the DARPA Robotics Challenge results validate the efficiency and the feasibility of the proposed strategy.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt10_02">15:05-15:20, Paper TuBT10.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0236.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('236'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Balance and Impedance Optimization Control for Compliant Humanoid Stepping</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#155847" title="Click to go to the Author Index">Spyrakos-Papastavridis, Emmanouil</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100179" title="Click to go to the Author Index">Caldwell, Darwin G.</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105401" title="Click to go to the Author Index">Tsagarakis, Nikos</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab236" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0236.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Compliance_and_Impedance_Control" title="Click to go to the Keyword Index">Compliance and Impedance Control</a></span><br>
                           <strong>Abstract:</strong> The work presented herein, attempts to address the problem of designing stepping recovery controllers for compliantly actuated humanoid robots. Based on the decomposition of the stepping procedure into three distinct phases, which are characterized by unique combinations of configurations and impedance levels, the contrivance of a Linear Quadratic Regulator (LQR) optimization process allows for the production of a corresponding number of controllers. The penalties associated with the proposed cost functions, which account for compliant dynamics and balance-related parameters alike, are selected in a systematic manner that facilitates the generation of the appropriate impedance levels required for each particular phase of the stepping motion. Subsequently, the superimposition of gravity compensation control, onto the original LQR controllers, renders them nonlinear and theoretically capable of tracking referential stepping trajectories. The associated referential motor positions are then generated by exploiting a formula relating the Centre-of-Pressure (CoP) to the compliant ankle dynamics, thereby satisfying the balancing constraints whilst also accounting for the system’s inherent under-actuation. Thus, the technique’s novelty stems from its explicit consideration of flexible joint dynamics and deflection torques, for the design of the desired impedance levels and joint stepping trajectories. The Series Elastic Actuator (SEA) powered COmpliant huMANoid (COMAN), has served as an avatar of the stepping recovery methodology that is expounded in the paper.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt10_03">15:20-15:35, Paper TuBT10.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0284.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('284'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Real-Time Skating Motion Control of Humanoid Robots for Acceleration and Balancing</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195618" title="Click to go to the Author Index">Takasugi, Noriaki</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#165253" title="Click to go to the Author Index">Kojima, Kunio</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117479" title="Click to go to the Author Index">Nozawa, Shunichi</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#135552" title="Click to go to the Author Index">Kakiuchi, Yohei</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106350" title="Click to go to the Author Index">Okada, Kei</a></td><td class="r">The Univ. of Tokyo</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106348" title="Click to go to the Author Index">Inaba, Masayuki</a></td><td class="r">The Univ. of Tokyo</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab284" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0284.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a></span><br>
                           <strong>Abstract:</strong> In this paper, we propose a real-time control method for skating motion of humanoid robots. There are three problems for skating motion: (1) keeping dynamic balance, (2) adequately controlling foot force to suppress slipping at the foot, (3) controlling full-body motion in real-time. For solving these problems, we propose the Skating Motion Generator and Skating Motion Stabilizer. In the Skating Motion Generator, we separate the slip suppression from motion generation for (3). The separation enables us to generate skating motions in real time. In the Skating Motion Stabilizer, we adjust the sole pressure distribution of each foot to solve the contradiction between (1) and (2). We show the effectiveness of the proposed controller through the experiments, in which life-sized humanoid HRP-2 pushes the ground and skates on the skateboard. Applying the proposed controller, HRP-2 could successfully accelerate and skate on the skateboard at 0.5[m/s].
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt10_04">15:35-15:50, Paper TuBT10.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0668.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('668'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Resolved COG Viscoelasticity Control of a Humanoid</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117307" title="Click to go to the Author Index">Yamamoto, Ko</a></td><td class="r">Univ. of Tokyo</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab668" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0668.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Redundant_Robots" title="Click to go to the Keyword Index">Redundant Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Humanoid_and_Bipedal_Locomotion" title="Click to go to the Keyword Index">Humanoid and Bipedal Locomotion</a></span><br>
                           <strong>Abstract:</strong> This paper proposes the concept of the center of gravity (COG) viscoelasticity to associate joint viscoelasticity with the COG-zero moment point (ZMP) model of humanoid dynamics. Although COG viscoelasticity is based on the well-known kinematic relationship between joint stiffness and end-effector stiffness, it provides practical advantages for humanoid motion control. Once the feedback gain in the COG-ZMP model is designed using the control theory, COG viscoelasticity can be applied to directly transform it to joint viscoelasticity. The author names this method as resolved COG viscoelasticity control (RCVC). In particular, this paper proposes RCVC in which the null-space of the COG Jacobian is employed. The validity of the RCVC is verified by simulating whole-body dynamics.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tubt10_05">15:50-16:05, Paper TuBT10.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0961.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('961'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Toward Whole-Body Loco-Manipulation: Experimental Results on Multi-Contact Interaction with the Walk-Man Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#150828" title="Click to go to the Author Index">Farnioli, Edoardo</a></td><td class="r">Univ. Di Pisa</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118836" title="Click to go to the Author Index">Gabiccini, Marco</a></td><td class="r">Univ. of Pisa</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103420" title="Click to go to the Author Index">Bicchi, Antonio</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab961" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0961.VI.mpg" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Humanoid_Interaction" title="Click to go to the Keyword Index">Humanoid Interaction</a></span><br>
                           <strong>Abstract:</strong> In this paper a quasi-static framework for optimally controlling the contact force distribution is experimentally verified with the full-size compliant humanoid robot Walk-Man. The proposed approach is general enough to cope with multi-contact scenarios, i.e. robot-environment interactions occurring on feet and hands, up to the more general case of whole-body loco-manipulation, in which the robot is in contact with the environment also with the internal limbs, with a consequent loss of contact force controllability. Experimental tests were conducted with the Walk-Man robot (i) standing on flat terrain, (ii) standing on uneven terrain and (iii) interacting with the environment with both feet and a hand touching a vertical wall. Moreover, the influence of unmodeled weight on the robot, and the combination with a higher priority Cartesian tasks are shown. Results are presented also in the attached video.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuspt12"><b>TuSpT12</b></a></td>
               <td class="r">Grand Ballroom</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuspt12" title="Click to go to the Program at a Glance"><b>Special Forum 1: AI/Deep Learning</b></a></td>
               <td class="r">Special Forum</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#106613" title="Click to go to the Author Index">Lee, Daniel D.</a></td><td class="r">Univ. of Pennsylvania</td></tr>

</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tut31"><b>TuT31</b></a></td>
               <td class="r">#111</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tut31" title="Click to go to the Program at a Glance"><b>Human-Robot Interaction/Planning</b></a></td>
               <td class="r">Teaser Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#180494" title="Click to go to the Author Index">Scherbatyuk, Alexander</a></td><td class="r">Far Eastern Federal Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#107736" title="Click to go to the Author Index">Howard, Thomas</a></td><td class="r">Univ. of Rochester</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_01">16:25-16:26, Paper TuT31.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0030.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('30'); return false" title="Click to show or hide the keywords and abstract">Kernel Density Estimation Based Self-Learning Sampling Strategy for Motion Planning of Repetitive Tasks</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#194908" title="Click to go to the Author Index">Iversen, Thomas Fridolin</a></td><td class="r">Univ. of Southern Denmark</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110542" title="Click to go to the Author Index">Ellekilde, Lars-Peter</a></td><td class="r">Univ. of Southen Denmark</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab30" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Manipulation_Planning_and_Control" title="Click to go to the Keyword Index">Manipulation Planning and Control</a>, <a href="IROS16_KeywordIndexMedia.html#Industrial_Robots" title="Click to go to the Keyword Index">Industrial Robots</a></span><br>
                           <strong>Abstract:</strong> This paper introduces a new sampling strategy and shows that superior performance can be obtained for a range of sampling based robotic motion planners, used in scenarios with low task variance, as found in many vision guided pick and place operations. The strategy uses kernel density estimation to identify regions with high probability of containing configurations being part of feasible solutions, and use the estimation to bias sampling. The kernel densities are initialized with a uniform distribution and are continuously updated, whenever paths are successfully planned and optimized. The system is thereby self-learning and improves performance over time. The sampler is tested on a variety of planners and against other sampling methods in two different scenarios containing robotic arms with 6 degrees of freedom and compared to a state-of-the-art optimization based planning algorithm. Tests show that the sampler learns fast and improves both time taken for solving problems and the quality of the resulting paths compared to other samplers.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_02">16:26-16:27, Paper TuT31.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0051.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('51'); return false" title="Click to show or hide the keywords and abstract">Group Navigation and Control for Marine Autonomous Robotic Complex Based on Hydroacoustic Communication</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180494" title="Click to go to the Author Index">Scherbatyuk, Alexander</a></td><td class="r">Far Eastern Federal Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195117" title="Click to go to the Author Index">Dubrovin, Fedor</a></td><td class="r">Inst. for Marine Tech. Problems</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195119" title="Click to go to the Author Index">Unru, Petr</a></td><td class="r">Far Eastern Federal Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195121" title="Click to go to the Author Index">Rodionov, Alexander</a></td><td class="r">Far Eastern Federal Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab51" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a></span><br>
                           <strong>Abstract:</strong> This paper is devoted to the collective control and group navigation based on hydroacoustic communication intended for marine autonomous robotic complex (MARC) including autonomous underwater vehicle (AUV) and autonomous surface vehicle (ASV). The structures of control and navigation systems are considered and some results of the systems operation in the real marine environment are supplemented. The contribution of this paper is in review of current hydroacoustic (HA) modems for marine robots, implementation details of MARC structure and HA communication and some field test results.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_03">16:27-16:28, Paper TuT31.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0063.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('63'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Overapproximative Arm Occupancy Prediction for Human-Robot Co-Existence Built from Archetypal Movements</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178796" title="Click to go to the Author Index">Pereira, Aaron</a></td><td class="r">Tech. Univ. München</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114506" title="Click to go to the Author Index">Althoff, Matthias</a></td><td class="r">Tech. Univ. München</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab63" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0063.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Formal_Methods_in_Robotics_and_Automation" title="Click to go to the Keyword Index">Formal Methods in Robotics and Automation</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Safety" title="Click to go to the Keyword Index">Robot Safety</a>, <a href="IROS16_KeywordIndexMedia.html#Physical_Human_Robot_Interaction" title="Click to go to the Keyword Index">Physical Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> Human motion is fast and hard to predict. To implement a provably safe collision-avoidance strategy for robots in collaborative spaces with humans, an overapproximative prediction of the occupancy of the human is required, which needs to be calculated faster than real time. We present a method for computing volumes containing the entire possible future occupancy of the human, given its state, faster than real time. The dynamic model of the human is built from analysing a set of archetypal movements performed by test subjects. The occupancy prediction is tested on a publicly available database of motion capture data, and shown to be overapproximative for all movements relating to everyday activities, sport and dance. Our novel algorithm is useful to guarantee safety in human-robot collaboration scenarios.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_04">16:28-16:29, Paper TuT31.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0075.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('75'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Sampled Differential Dynamic Programming</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#194943" title="Click to go to the Author Index">Rajamäki, Joose</a></td><td class="r">Aalto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160068" title="Click to go to the Author Index">Naderi, Kourosh</a></td><td class="r">Aalto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105044" title="Click to go to the Author Index">Kyrki, Ville</a></td><td class="r">Aalto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195194" title="Click to go to the Author Index">Hämäläinen, Perttu Juho</a></td><td class="r">Aalto Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab75" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0075.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Nonholonomic_Motion_Planning" title="Click to go to the Keyword Index">Nonholonomic Motion Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a></span><br>
                           <strong>Abstract:</strong> We present SaDDP, a sampled version of the widely used differential dynamic programming (DDP) control algorithm. We contribute through establishing a novel connection between two major branches of robotics control research, that is, gradient-based methods such as DDP, and Monte Carlo methods such as path integral control (PI) that utilize random simulated trajectory rollouts. One of our key observations is that the Taylor-expansion, central to DDP, can be reformulated in terms of second-order statistics computed from the sampled trajectories. SaDDP makes few assumptions about the controlled system and works with black-box dynamics simulations with non-smooth contacts. Our simulation results show that the method outperforms PI and CMA-ES in both a simple linear-quadratic problem, and a multilink arm reaching task with obstacles.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_05">16:29-16:30, Paper TuT31.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0354.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('354'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Ballistic Motion Planning</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184359" title="Click to go to the Author Index">Campana, Mylène</a></td><td class="r">LAAS-CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101849" title="Click to go to the Author Index">Laumond, Jean-Paul</a></td><td class="r">LAAS-CNRS</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab354" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0354.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Animation_and_Simulation" title="Click to go to the Keyword Index">Animation and Simulation</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a></span><br>
                           <strong>Abstract:</strong> This paper addresses the motion planning problem of a jumping point-robot. Each jump consists in a ballistic motion linking two positions in contact with obstacle surfaces. A solution path is thus a sequence of parabola arcs. The originality of the approach is to consider non-sliding constraints at contact points: slipping avoidance is handled by constraining takeoff and landing velocity vectors to 3D friction cones. Furthermore the magnitude of these velocities is bounded. The ballistic motion lying in a vertical plane, we transform the 3D problem into a 2D one. We then solve the motion equations. The solution gives rise to an exact steering method computing a jump path between two contact points while respecting all constraints. The method is integrated into a standard probabilistic roadmap planner. Probabilistic completeness is proven. Simulations illustrate the performance of the approach.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_06">16:30-16:31, Paper TuT31.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0395.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('395'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Collision-Free Trajectory Planning on Lissajous Curves for Repeated Multi-Agent Coverage and Target Detection</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#194308" title="Click to go to the Author Index">Borkar, Aseem</a></td><td class="r">Indian Inst. of Tech. Bombay</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#173856" title="Click to go to the Author Index">Sinha, Arpita</a></td><td class="r">Indian Insitute of Tech. Bombay</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#134390" title="Click to go to the Author Index">Vachhani, Leena</a></td><td class="r">Indian Inst. of Tech. Bombay</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#194309" title="Click to go to the Author Index">Arya, Hemendra</a></td><td class="r">Indian Inst. of Tech. Bombay</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab395" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0395.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Multi_Robot_Systems" title="Click to go to the Keyword Index">Multi-Robot Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Surveillance_Systems" title="Click to go to the Keyword Index">Surveillance Systems</a></span><br>
                           <strong>Abstract:</strong> This paper proposes a trajectory planning strategy to simultaneously address multiple surveillance objectives such as complete area coverage, periodic surveillance, and guaranteed detection of a rogue element attempting to exit the area of interest. For agents with identical sensing capability, the proposed strategy defines a time-varying multi-agent formation on a Lissajous curve, which completes the aforementioned tasks in finite time with collision free paths for all agents in a 2-D rectangular region. This obviates the need for sensing and communication for collision avoidance among agents. A sufficient upper limit on agent dimensions that ensures collision free motion is derived. An algorithm is developed for choosing the number of agents and optimal Lissajous curve for a prescribed rectangle and agents' sensing capability. The optimal Lissajous curve is chosen to minimize the time period for repeated coverage and maximize the upper bound on agent size. The proposed algorithm is validated through computer simulations and experiments using differential drive robots.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_07">16:31-16:32, Paper TuT31.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0533.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('533'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Motion Planning with Diffusion Maps</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180431" title="Click to go to the Author Index">Chen, Yufan</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#139022" title="Click to go to the Author Index">Liu, Shih-Yuan</a></td><td class="r">U.C. Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191248" title="Click to go to the Author Index">Liu, Miao</a></td><td class="r">MIT</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192912" title="Click to go to the Author Index">Miller, Justin</a></td><td class="r">MIT</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104610" title="Click to go to the Author Index">How, Jonathan Patrick</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab533" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0533.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Detection_and_Tracking" title="Click to go to the Keyword Index">Human Detection and Tracking</a></span><br>
                           <strong>Abstract:</strong> Many robotic applications require repeated, on-demand motion planning in mapped environments. In addition, the presence of other dynamic agents, such as people, often induces frequent, dynamic changes in the environment. Having a potential function that encodes pairwise cost-to-go can be useful for improving the computational speed of finding feasible paths, and for guiding local searches around dynamic obstacles. However, since storing pairwise potential can be impractical given the O(|V|^2) memory requirement, existing work often needs to compute a potential function for each query to a new goal, which would require a substantial online computation. This work addresses the problem by using diffusion maps, a machine learning algorithm, to learn the map's geometry and develop a memory-efficient parametrization (O(|V|)) of pairwise potentials. Specially, each state in the map is transformed to a diffusion coordinate, in which pairwise Euclidean distance is shown to be a meaningful similarity metric. We develop diffusion-based motion planning algorithms and, through extensive numerical evaluation, show that the proposed algorithms find feasible paths of similar quality with orders of magnitude improvement in computational speed compared with single-query methods. The proposed algorithms are implemented on hardware to enable real-time autonomous navigation in an indoor environment with frequent interactions with pedestrians.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_08">16:32-16:33, Paper TuT31.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0683.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('683'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Multi-Contact Bilateral Telemanipulation Using Wearable Haptics</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#165894" title="Click to go to the Author Index">Meli, Leonardo</a></td><td class="r">Univ. of Siena</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#138048" title="Click to go to the Author Index">Salvietti, Gionata</a></td><td class="r">Univ. of Siena</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#147770" title="Click to go to the Author Index">Gioioso, Guido</a></td><td class="r">Univ. Degli Studi Di Siena</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#149511" title="Click to go to the Author Index">Malvezzi, Monica</a></td><td class="r">Univ. of Siena</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105260" title="Click to go to the Author Index">Prattichizzo, Domenico</a></td><td class="r">Univ. of Siena</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab683" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0683.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Networked_Teleoperation" title="Click to go to the Keyword Index">Networked Teleoperation</a>, <a href="IROS16_KeywordIndexMedia.html#Wearable_Robots" title="Click to go to the Keyword Index">Wearable Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a></span><br>
                           <strong>Abstract:</strong> Bilateral telemanipulation refers to frameworks in which a human operator manipulates a master robotic device, and a slave robotic device emulates the behavior of the master, while haptic feedback is provided to the operator. For multi-contact bilateral teleoperation we intend master and slave systems that can establish multiple contact points with the user and with the environment. A paradigmatic example can be a multi-fingered robotic hand teleoperated by the human hand. Two of the most critical issues in this context are: (i) how to provide haptic feedback in multiple point of the human hand; (ii) how to solve the correspondence problem between the human hand and the robotic slave device. In this work, we propose finger-worn devices able to apply a three dimensional vector of force at a specific contact point to solve the multi-contact feedback problem. For the correspondence problem, we propose an object-based mapping procedure. The approach is based on two virtual objects, defined both at the master and slave sides, to capture the human hand motion and to compute the related force feedback. The proposed approach has been tested in a telemanipulation framework where the master side was composed of a Leap Motion sensor used to track the hand plus three wearable haptic devices, while a robotic hand/arm system performed a manipulation task as slave.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_09">16:33-16:34, Paper TuT31.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0751.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('751'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Foresighted Navigation through Cluttered Environments</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#193043" title="Click to go to the Author Index">Regier, Peter</a></td><td class="r">Univ. of Bonn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#132486" title="Click to go to the Author Index">Osswald, Stefan</a></td><td class="r">Univ. of Bonn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#186171" title="Click to go to the Author Index">Karkowski, Philipp</a></td><td class="r">Univ. of Bonn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#113220" title="Click to go to the Author Index">Bennewitz, Maren</a></td><td class="r">Univ. of Bonn</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab751" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0751.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Service_Robots" title="Click to go to the Keyword Index">Service Robots</a></span><br>
                           <strong>Abstract:</strong> In this paper, we introduce an approach to efficient robot navigation through cluttered indoor environments. We propose to estimate local obstacle densities based on already detected objects and use them to predict traversal costs corresponding to potential obstacles in regions not yet observable by the robot's sensors. By taking into account the predicted costs for path planning, the robot is then able to navigate in a more foresighted manner and reduces the risk of getting stuck in cluttered regions. We thoroughly evaluated our approach in simulated and real-world experiments. As the experimental results demonstrate, our method enables the robot to efficiently navigate through environments containing cluttered regions and achieves significantly shorter completion times compared to a standard approach not using any prediction.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_10">16:34-16:35, Paper TuT31.10</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0836.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('836'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Trust-Based Human-Robot Interaction for Multi-Robot Symbolic Motion Planning</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190220" title="Click to go to the Author Index">Spencer, David</a></td><td class="r">Clemson Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#135735" title="Click to go to the Author Index">Wang, Yue</a></td><td class="r">Clemson Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192463" title="Click to go to the Author Index">Humphrey, Laura</a></td><td class="r">Air Force Res. Lab</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab836" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0836.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Formal_Methods_in_Robotics_and_Automation" title="Click to go to the Keyword Index">Formal Methods in Robotics and Automation</a>, <a href="IROS16_KeywordIndexMedia.html#Path_Planning_for_Multiple_Mobile_Robots_or_Agents" title="Click to go to the Keyword Index">Path Planning for Multiple Mobile Robots or Agents</a></span><br>
                           <strong>Abstract:</strong> Symbolic motion planning for robots is the process of specifying and planning robot tasks in a discrete space, then carrying them out in a continuous space in a manner that preserves the discrete-level task specifications. Despite progress in symbolic motion planning, many challenges remain, including addressing scalability for multi-robot systems and improving solutions by incorporating human intelligence in an adaptive fashion. In this paper, we use local communication, observation, control protocols, and compositional reasoning approaches to decompose the planning problem to address scalability. To address solution quality and adaptability, we use a dynamic and computational trust model to aid this decomposition and to implement real-time switching between automated and human motion planning. A simulation is provided demonstrating the successful implementation of these methods.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_11">16:35-16:36, Paper TuT31.11</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0854.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('854'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Hybrid Teleoperation Control Scheme for a Single-Arm Mobile Manipulator with Omnidirectional Wheels</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195680" title="Click to go to the Author Index">Pepe, Alberto</a></td><td class="r">ALMA MATER STUDIORUM - Univ. of Bologna</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195541" title="Click to go to the Author Index">Chiaravalli, Davide</a></td><td class="r">Alma Mater Studiorum, Univ. of Bologna</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103935" title="Click to go to the Author Index">Melchiorri, Claudio</a></td><td class="r">Univ. of Bologna</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab854" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0854.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Telerobotics" title="Click to go to the Keyword Index">Telerobotics</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Mobile_Manipulation" title="Click to go to the Keyword Index">Mobile Manipulation</a></span><br>
                           <strong>Abstract:</strong> In this paper, an hybrid position-position and position-velocity teleoperation control scheme for a generic mobile manipulator is presented and discussed. The mobile manipulator is composed by a mobile platform and a 5 dof arm, and the proposed control scheme allows the simultaneous control of both the devices by means of a single haptic device characterized by an open kinematic chain and not specifically designed for mobile manipulators teleoperation (e.g. a Phantom Omni). The proposed teleoperation controller overcomes the mismatch of the control signals to be sent to the arm (position) and to the mobile platform (velocity) through a proper partition of the master device workspace. Tests have been performed both by simulation and with a real setup. The setup is composed by a 6 dof Phantom Omni haptic device acting as master, and a single-arm Kuka youBot omnidirectional manipulator acting as slave. Experimental results related to a pick and place task, performed on the real setup and involving the motion of both the arm and the platform are reported and commented.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_12">16:36-16:37, Paper TuT31.12</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0872.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('872'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Gaussian Random Paths for Real-Time Motion Planning</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#163618" title="Click to go to the Author Index">Choi, Sungjoon</a></td><td class="r">Seoul National Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179481" title="Click to go to the Author Index">Lee, Kyungjae</a></td><td class="r">Seoul National Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#119971" title="Click to go to the Author Index">Oh, Songhwai</a></td><td class="r">Seoul National Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab872" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0872.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a>, <a href="IROS16_KeywordIndexMedia.html#Manipulation_Planning_and_Control" title="Click to go to the Keyword Index">Manipulation Planning and Control</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a></span><br>
                           <strong>Abstract:</strong> In this paper, we propose Gaussian random paths by defining a probability distribution over continuous paths interpolating a finite set of anchoring points using Gaussian process regression. By utilizing the generative property of Gaussian random paths, a Gaussian random path planner is developed to safely steer a robot to a goal position. The Gaussian random path planner can be used in a number of applications, including local path planning for a mobile robot and trajectory optimization for a whole body motion planning. We have conducted an extensive set of simulations and experiments, showing that the proposed planner outperforms look-ahead planners which use a pre-defined subset of egocentric trajectories in terms of collision rates and trajectory lengths. Furthermore, we apply the proposed method to existing trajectory optimization methods as an initialization step and demonstrate that it can help produce more cost-efficient trajectories.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_13">16:37-16:38, Paper TuT31.13</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0955.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('955'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Expressing Homotopic Requirements for Mobile Robot Navigation through Natural Language Instructions</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#181378" title="Click to go to the Author Index">Yi, Daqing</a></td><td class="r">Brigham Young Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107736" title="Click to go to the Author Index">Howard, Thomas</a></td><td class="r">Univ. of Rochester</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122763" title="Click to go to the Author Index">Goodrich, Michael A.</a></td><td class="r">Brigham Young Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196511" title="Click to go to the Author Index">Seppi, Kevin</a></td><td class="r">Brigham Young Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab955" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0955.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Integrated_Task_and_Motion_Planning" title="Click to go to the Keyword Index">Integrated Task and Motion Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> Allowing a human to express topological requirements to a robot in language enables untrained users to guide robot movement without requiring the human to understand sophisticated robot algorithms. By using a homotopy class or classes to represent one or more topological requirements, we build a framework that helps a robot understand a human's intent. This paper reviews a homotopic decomposition method that is used to convert any path into a string, which allows homotopic path equivalence to be performed by comparing strings. We then integrate a graphical model (HDCG) to infer the homotopic constraint in the format of strings from a language instruction. Finally, we use a homotopic path-planning algorithm that finds the optimal paths for a given objective and homotopic constraint. Experiment results show how a language instruction is converted into a path driven by an implicit topological requirement.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_14">16:38-16:39, Paper TuT31.14</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1030.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1030'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Optimizing the Use of Power in Wave Based Bilateral Teleoperation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158070" title="Click to go to the Author Index">Ferraguti, Federica</a></td><td class="r">Univ. Degli Studi Di Modena E Reggio Emilia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111622" title="Click to go to the Author Index">Fantuzzi, Cesare</a></td><td class="r">Univ. Di Modena E Reggio Emilia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110186" title="Click to go to the Author Index">Secchi, Cristian</a></td><td class="r">Univ. of Modena & Reggio Emilia</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1030" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1030.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Telerobotics" title="Click to go to the Keyword Index">Telerobotics</a>, <a href="IROS16_KeywordIndexMedia.html#Networked_Teleoperation" title="Click to go to the Keyword Index">Networked Teleoperation</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a></span><br>
                           <strong>Abstract:</strong> Because of their simplicity, wave variables have become almost a standard strategy for stabilizing delayed bilateral teleoperation systems. However, the price to pay for a stable behavior is a degradation in the performance of the teleoperation system. Recently, more flexible and transparency oriented bilateral architectures have been proposed (e.g. TDPN, PSPM, Two-Layer approach) but they are complex to implement and to tune. In [1], a strategy for blending the high performance of the new control methodologies with the simplicity of wave based bilateral teleoperation has been proposed. Nevertheless, while appealing in terms of simplicity, this method is conservative in terms of the transparency that can be achieved. In this paper, we extend the architecture in [1] in order to optimize the use of the energy and for achieving a coupling that is as close as possible to the desired one while preserving the passivity of the overall system.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_15">16:39-16:40, Paper TuT31.15</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1054.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1054'); return false" title="Click to show or hide the keywords and abstract">Expressive Path Shape (Swagger): Simple Features That Illustrate a Robot’s Attitude Toward Its Goal in Real Time</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#128348" title="Click to go to the Author Index">Knight, Heather</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196477" title="Click to go to the Author Index">Theilstrom, Ravenna</a></td><td class="r">Swarthmore Coll</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105852" title="Click to go to the Author Index">Simmons, Reid</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1054" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Companions_and_Social_Human_Robot_Interaction" title="Click to go to the Keyword Index">Robot Companions and Social Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Service_Robots" title="Click to go to the Keyword Index">Service Robots</a></span><br>
                           <strong>Abstract:</strong> Expressive motion can situate a robot’s attitude in its task motions, illustrating real-time reactions. Inspired by acting movement training, we construct path shape features that layer expression into a mobile robot’s motion traversal. Our video-study results show that simple variations of path shape and orientation can influence human perceptions of a robot’s task, focus, and confidence. We further find that sequencing path features is a useful way to create expressions that are pinpointed in time without requiring changes in velocity. Our quantitative features represent the Laban Space Effort: using path shape and orientation along the path to communicate the direct or indirect attitude of the robot toward its target destination. These features illustrate expressive or stylistic aspects of the robot’s inner state, filling a gap in the pre-existing literature that has mostly focused on task legibility. Our future work will evaluate temporal and spatial robot motion features in explicit interaction contexts.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_16">16:40-16:41, Paper TuT31.16</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1222.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1222'); return false" title="Click to show or hide the keywords and abstract">Enhancing Bilateral Teleoperation Using Camera-Based Online Virtual Fixtures Generation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196606" title="Click to go to the Author Index">Selvaggio, Mario</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191619" title="Click to go to the Author Index">Notomista, Gennaro</a></td><td class="r">Univ. Degli Studi Di Napoli "Federico II"</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#132532" title="Click to go to the Author Index">Chen, Fei</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196605" title="Click to go to the Author Index">Gao, Boyang</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#202512" title="Click to go to the Author Index">Trapani, Francesco</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100179" title="Click to go to the Author Index">Caldwell, Darwin G.</a></td><td class="r">Istituto Italiano Di Tecnologia</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1222" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Telerobotics" title="Click to go to the Keyword Index">Telerobotics</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a></span><br>
                           <strong>Abstract:</strong> In this paper we present an interactive system to enhance bilateral teleoperation through online virtual fixtures generation and task switching. This is achieved using a stereo camera system which provides accurate information of the surrounding environment of the robot and of the tasks that have to be performed in it. The use of the proposed approach aims at improving the performances of bilateral teleoperation systems by reducing the human operator workload and increasing both the implementation and the execution efficiency. In fact, using our method virtual guidances do not need to be programmed a priori but they can be instead automatically generated and updated making the system suitable for unstructured environments. We strengthen the proposed method using passivity control in order to safely switch between different tasks while teleoperating under active constraints. A series of experiments emulating real industrial scenarios are used to show that the switch between multiple tasks can be passively and safely achieved and handled by the system.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_17">16:41-16:42, Paper TuT31.17</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1235.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1235'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Inferring Human Intent from Video by Sampling Hierarchical Plans</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#194849" title="Click to go to the Author Index">Holtzen, Steven</a></td><td class="r">Univ. of California, Los Angeles</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168910" title="Click to go to the Author Index">Zhao, Yibiao</a></td><td class="r">UCLA</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196269" title="Click to go to the Author Index">Gao, Tao</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#202511" title="Click to go to the Author Index">Tenenbaum, Joshua</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#170029" title="Click to go to the Author Index">Zhu, Song-Chun</a></td><td class="r">UCLA</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1235" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1235.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Task_Planning" title="Click to go to the Keyword Index">Task Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Planning__Scheduling_and_Coordination" title="Click to go to the Keyword Index">Planning, Scheduling and Coordination</a></span><br>
                           <strong>Abstract:</strong> This paper presents a method which allows robots to infer a human’s hierarchical intent from partially observed RGBD videos by imagining how the human will behave in the future. This capability is critical for creating robots which can interact socially or collaboratively with humans. We represent intent as a novel hierarchical, compositional, and probabilistic And-Or graph structure which describes a relationship between actions and plans. We infer human intent by reverse-engineering a human’s decision-making and action planning processes under a Bayesian probabilistic programming framework. We present experiments from a 3D environment which demonstrate that the inferred human intent (1) matches well with human judgment, and (2) provides useful contextual cues for object tracking and action recognition.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_18">16:42-16:43, Paper TuT31.18</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1295.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1295'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Point-To-Point Safe Navigation of a Mobile Robot Using Stigmergy and RFID Technology</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#170111" title="Click to go to the Author Index">Khaliq, Ali Abdul</a></td><td class="r">Örebro Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#135594" title="Click to go to the Author Index">Pecora, Federico</a></td><td class="r">Örebro Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104413" title="Click to go to the Author Index">Saffiotti, Alessandro</a></td><td class="r">Orebro Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1295" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1295.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Service_Robots" title="Click to go to the Keyword Index">Service Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a></span><br>
                           <strong>Abstract:</strong> Reliable autonomous navigation is still a challenging problem for robots with simple and inexpensive hardware. A key difficulty is the need to maintain an internal map of the environment and an accurate estimate of the robot’s position in this map. Recently, a stigmergic approach has been proposed in which a navigation map is stored into the environment, on a grid of RFID tags, and robots use it to optimally reach predefined goal points without the need for internal maps. While effective, this approach is limited to a predefined set of goal points. In this paper, we extend this approach to enable robots to travel to any point on the RFID floor, even if it was not previously identified as a goal location, as well as to keep a safe distance from any given critical location. Our approach produces safe, repeatable and quasi-optimal trajectories without the use of internal maps, self localization, or path planning. We report experiments run in a real apartment equipped with an RFID floor, in which a service robot either reaches or avoids a user who wears slippers equipped with an RFID tag reader.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_19">16:43-16:44, Paper TuT31.19</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1323.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1323'); return false" title="Click to show or hide the keywords and abstract">Admittance Shaping in Delayed Bilateral Teleoperation Control</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160580" title="Click to go to the Author Index">Kristalny, Maxim</a></td><td class="r">Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#148953" title="Click to go to the Author Index">Cho, Jang Ho</a></td><td class="r">Korea Inst. of Machinery & Materials</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1323" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Telerobotics" title="Click to go to the Keyword Index">Telerobotics</a>, <a href="IROS16_KeywordIndexMedia.html#Haptics_and_Haptic_Interfaces" title="Click to go to the Keyword Index">Haptics and Haptic Interfaces</a>, <a href="IROS16_KeywordIndexMedia.html#Networked_Teleoperation" title="Click to go to the Keyword Index">Networked Teleoperation</a></span><br>
                           <strong>Abstract:</strong> In this paper we consider a problem of delayed bilateral teleoperation control. We exploit a recent parameterization of all feasible teleoperator admittance matrices to propose a control synthesis procedure based on admittance shaping. This leads to an intuitive performance-oriented controller design that reveals important trade-offs inherent to bilateral teleoperaton. In particular, this allows a trade-off between performance and passivity, that is not accessible while using classical passivity-based methods. The proposed controller design is illustrated with simulations and a simple 1 DOF experimental case study.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_20">16:44-16:45, Paper TuT31.20</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1334.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1334'); return false" title="Click to show or hide the keywords and abstract">Log-Space Harmonic Function Path Planning</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#164029" title="Click to go to the Author Index">Wray, Kyle</a></td><td class="r">Univ. of Massachusetts Amherst</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118732" title="Click to go to the Author Index">Ruiken, Dirk</a></td><td class="r">Univ. of Massachusetts</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101349" title="Click to go to the Author Index">Grupen, Rod</a></td><td class="r">Univ. of Massachusetts</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179562" title="Click to go to the Author Index">Zilberstein, Shlomo</a></td><td class="r">Univ. of Massachusetts</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1334" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a></span><br>
                           <strong>Abstract:</strong> We propose a log-space solution for robotic path planning with harmonic functions that solves the long-standing numerical precision problem. We prove that this algorithm: (1) performs the correct computations in log-space, (2) returns the true equivalent path using the log-space mapping, and (3) has a strong error bound given its convergence criterion. We evaluate the algorithm on 7 problem domains. A Graphics Processing Unit (GPU) implementation is also shown to greatly improve performance. We also provide an open source library entitled epic with extensive ROS support and demonstrate this method on a real humanoid robot: the uBot-6. Experiments demonstrate that the log-space solution rapidly produces smooth obstacle-avoiding trajectories, and supports planning in exponentially larger real-world robotic applications.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_21">16:45-16:46, Paper TuT31.21</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1354.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1354'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Interpretation of Uncertain Information in Mobile Service Robots by Analyzing Surrounding Spatial Arrangement Based on Occupied Density Variation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191819" title="Click to go to the Author Index">Muthugala, Muthugala Arachchige Viraj Jagathpriya</a></td><td class="r">Univ. of Moratuwa</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191981" title="Click to go to the Author Index">Jayasekara, A.G.B.P.</a></td><td class="r">Univ. of Moratuwa</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1354" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1354.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a>, <a href="IROS16_KeywordIndexMedia.html#Service_Robots" title="Click to go to the Keyword Index">Service Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Centered_Robotics" title="Click to go to the Keyword Index">Human Centered Robotics</a></span><br>
                           <strong>Abstract:</strong> Service robots are being developed as a supportive aid for elderly people. Those robots are operated by non-expert users in heterogeneous domestic environments. Hence,the ability of a robot to be operated in a more natural human friendly manner enhances the overall satisfaction of the user. Humans prefer to use voice in order to convey instructions. Those voice instructions often include uncertain terms such as “little” and “far”. Therefore, the robotic assistants should possess the competency to appropriately interpret the quantitative meanings of such terms. The quantitative meaning of uncertain terms related to the spatial information depends on the spatial arrangement of the environment. This paper proposes a method in order to evaluate the uncertain information in user commands by replicating the natural tendencies of humans about the spatial arrangement of the environment. A module called Occupied Density analyzer has been deployed to analyze the occupied density distribution. A function has been defined to estimate the perceptive distance based on the occupied density distribution. The perception of the uncertain terms is adjusted according to the perceptive distance of that particular scenario. Particulars on rationale behind the proposed method are explained with due attention to the natural human tendencies. Experiments have been carried out in order to evaluate the performance and the behaviors of the proposed system.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_22">16:46-16:47, Paper TuT31.22</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1399.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1399'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Development of the Human Interactive Autonomy for the Shared Teleoperation of Mobile Robots</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196716" title="Click to go to the Author Index">Lee, Kwang-Hyun</a></td><td class="r">Korea Univ. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179770" title="Click to go to the Author Index">Mehmood, Usman</a></td><td class="r">Korea Univ. of Tech. and Education</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102369" title="Click to go to the Author Index">Ryu, Jee-Hwan</a></td><td class="r">Korea Univ. of Tech. and Education</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1399" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1399.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Telerobotics" title="Click to go to the Keyword Index">Telerobotics</a>, <a href="IROS16_KeywordIndexMedia.html#Semi_Autonomous_Robots" title="Click to go to the Keyword Index">Semi-Autonomous Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> Manipulation complexity of teleoperation increases the necessity of shared autonomy. However, designing an autonomy while keeping in mind the environmental uncertainty is challenging. This paper proposes a human interactive autonomy for a shared teleoperation. The central concept is a novel methodology of combining the innate cognitive ability of a human operator to arrive at an accurate, and precise solution of an autonomous system for mobile robot shared teleoperation. The resulting system is more efficient and less fatigued than direct teleoperation. This combined approach transfers the human operator’s intention, shaped by his cognition about remote environment, to the autonomous system. The intention information input from the operator helps autonomous system perform the given teleoperation tasks more efficiently. For initial feasibility and proof of concept, the intention information is provided to the autonomous system in the form of a path which is acquired by our proposed sketch method. Our novel proposed sketch method allows the operator to sketch the path on the visual feedback image of the remote environments and the information is then transmitted to the autonomous system. Experimental results provide the feasibility and effectiveness of proposed method against prevalent control methods.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_23">16:47-16:48, Paper TuT31.23</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1540.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1540'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Scene-Based Dependable Indoor Navigation System</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#155260" title="Click to go to the Author Index">Ko, Dong Wook</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180510" title="Click to go to the Author Index">Kim, Yong Nyeon</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#154706" title="Click to go to the Author Index">Lee, Jin Han</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102631" title="Click to go to the Author Index">Suh, Il Hong</a></td><td class="r">Hanyang Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1540" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1540.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a></span><br>
                           <strong>Abstract:</strong> We propose a topology-based Bayesian visual navigation framework, with which we represent the world environment as a collection of scenes. The proposed topological map consists of nodes represented as a bag of visual line words and edges represented as both an adjacency list and relative motion information to perform the transition between topological nodes. Our proposed Bayesian localization framework uses two measurement models: a visual line word-based place model and a path-matching model. To enable a mobile robot to reach to a desired destination, a coastal path is planned in such a way that maximizes the possibility of following the reference path defined as a sequence of scenes encountered when building the topological map, while avoiding possible collisions by leveraging a local grid map constructed in real time from Kinect depth information. To show the validity of our proposed framework, we provide several extensive experimental results in several indoor environments under different conditions, such as illumination changes and visual occlusions.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_24">16:48-16:49, Paper TuT31.24</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1632.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1632'); return false" title="Click to show or hide the keywords and abstract">A Humanoid Doing an Artistic Work - Graffiti on the Wall</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140101" title="Click to go to the Author Index">Jun, Youngbum</a></td><td class="r">Univ. of Nevada Las Vegas</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122358" title="Click to go to the Author Index">Jang, Giho</a></td><td class="r">Hanyang Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#109773" title="Click to go to the Author Index">Cho, Baek-Kyu</a></td><td class="r">Kookmin Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192422" title="Click to go to the Author Index">Trubatch, Joel</a></td><td class="r">Univ. of Nevada Las Vegas</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#132642" title="Click to go to the Author Index">Kim, Inhyeok</a></td><td class="r">NAVER</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192229" title="Click to go to the Author Index">Seo, Sang-Duck</a></td><td class="r">Univ. of Nevada Las Vegas</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100181" title="Click to go to the Author Index">Oh, Paul Y.</a></td><td class="r">Univ. of Nevada, Las Vegas (UNLV)</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1632" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Entertainment_Robotics" title="Click to go to the Keyword Index">Entertainment Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Integrated_Planning_and_Control" title="Click to go to the Keyword Index">Integrated Planning and Control</a></span><br>
                           <strong>Abstract:</strong> Graffiti work using a humanoid with artistic technique can convey the value of artists' work to people. Previous work that focused on drawing an image on a canvas accurately has not contained artistic processes like performance, drawing skills and other artists' intents at the time of creation. To combine such artistic processes, the work in this paper utilizes whole-body motion of a humanoid to paint an image on a wall using Pointillism. However, a biped humanoid consists of high Degree Of Freedom (DOF) system and is very sensitive to internal and external disturbances from interaction with environments. As is the case when graffitiing on a wall. Most notably, the vibration from impact contacts and mechanical uncertainties limit the humanoid in graffitiing properly. This paper presents an approach to realize drawing a large image on a wall through real-time motion planning for printing, artificial compliance, and a disturbance controller.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut31_25">16:49-16:50, Paper TuT31.25</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1693.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1693'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Efficient Learning of Stand-Up Motion for Humanoid Robots with Bilateral Symmetry</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169561" title="Click to go to the Author Index">Jeong, Heejin</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106613" title="Click to go to the Author Index">Lee, Daniel D.</a></td><td class="r">Univ. of Pennsylvania</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1693" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1693.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Reinforcement_Learning" title="Click to go to the Keyword Index">Robot Reinforcement Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Humanoid_Robots" title="Click to go to the Keyword Index">Humanoid Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Learning" title="Click to go to the Keyword Index">Robot Learning</a></span><br>
                           <strong>Abstract:</strong> Standing up after falling is an essential ability for humanoid robots in order to resume their tasks without help from humans. Although many humanoid robots, especially small-size humanoid robots, have their own stand-up motions, there has not been a generalized method to automatically learn flexible stand-up motions for humanoid robots which can be applied to various fallen positions. In this research, we propose a method for learning stand-up motions for humanoid robots using Q-learning making use of their bilateral symmetry. We implemented this method on DarwIn-OP humanoid robots and learned an optimal policy in simulation. We compared the resulting stand-up motion with manually designed stand-up motions and with stand-up motions learned without considering bilateral symmetry. Both in simulation and on the real robot, the new stand-up motion was successful in most trials while other motions took longer or were not as robust.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tut32"><b>TuT32</b></a></td>
               <td class="r">#112</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tut32" title="Click to go to the Program at a Glance"><b>Unmanned Aerial Vehicle</b></a></td>
               <td class="r">Teaser Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#103488" title="Click to go to the Author Index">Sanfeliu, Alberto</a></td><td class="r">Univ. Pol. De Cataluyna</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#104207" title="Click to go to the Author Index">Lee, Dongjun</a></td><td class="r">Seoul National Univ</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_01">16:25-16:26, Paper TuT32.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0095.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('95'); return false" title="Click to show or hide the keywords and abstract">Minimum-Time Trajectories for Quadrotor UAVs in Complex Environments</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195266" title="Click to go to the Author Index">Jamieson, Jonathan</a></td><td class="r">Univ. of Strathclyde</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195267" title="Click to go to the Author Index">Biggs, James</a></td><td class="r">Dipartimento Di Scienze E Tecnologie Aerospaziali, Pol. D</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab95" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a></span><br>
                           <strong>Abstract:</strong> This paper presents a heuristic framework that generates trajectories for quadrotor UAVs in complex environments that smoothly follow a series of waypoints with desired boundary states on the derivatives. The waypoints can be chosen manually or found using a sampling-based path planner such as RRT for complex environments with obstacles. A trajectory in the virtual domain is found using polynomials parametrised by an abstract argument that are numerically optimised to give the shortest geometrical path length. A mapping function is used to ensure the kinodynamic limits on the velocity and acceleration throughout the trajectory are satisfied. The mapping function is found heuristically with an algorithm that minimises the trajectory time.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_02">16:26-16:27, Paper TuT32.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0101.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('101'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Vision-Based Unmanned Aerial Vehicle Detection and Tracking for Sense and Avoid Systems</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#167961" title="Click to go to the Author Index">Sapkota, Krishna</a></td><td class="r">École Pol. Fédérale De Lausanne</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160392" title="Click to go to the Author Index">Roelofsen, Steven</a></td><td class="r">EPFL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#194954" title="Click to go to the Author Index">Rozantsev, Artem</a></td><td class="r">EPFL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107767" title="Click to go to the Author Index">Lepetit, Vincent</a></td><td class="r">EPFL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#120956" title="Click to go to the Author Index">Gillet, Denis</a></td><td class="r">Swiss Federal Inst. of Tech. in Lausanne (EPFL)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#133878" title="Click to go to the Author Index">Fua, Pascal</a></td><td class="r">EPFL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102253" title="Click to go to the Author Index">Martinoli, Alcherio</a></td><td class="r">EPFL</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab101" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0101.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Visual_Tracking" title="Click to go to the Keyword Index">Visual Tracking</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a></span><br>
                           <strong>Abstract:</strong> We propose an approach for on-line detection of small Unmanned Aerial Vehicles (UAVs) and estimation of their relative positions and velocities in the 3D environment from a single moving camera in the context of sense and avoid systems. This problem is challenging both from a detection point of view, as there are no markers on the targets available, and from a tracking perspective, due to misdetection and false positives. Furthermore, the methods need to be computationally light, despite the complexity of computer vision algorithms, to be used on UAVs with limited payload. To address these issues we propose a multi-staged framework that incorporates fast object detection using an AdaBoost-based approach, coupled with an on-line visual-based tracking algorithm and a recent sensor fusion and state estimation method. Our framework allows for achieving real-time performance with accurate object detection and tracking without any need of markers and customized, high-performing hardware resources.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_03">16:27-16:28, Paper TuT32.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0104.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('104'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Persistent Aerial Tracking System for UAVs</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191244" title="Click to go to the Author Index">Mueller, Matthias</a></td><td class="r">KAUST</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191246" title="Click to go to the Author Index">Sharma, Gopal</a></td><td class="r">Indian Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191247" title="Click to go to the Author Index">Smith, Neil</a></td><td class="r">King Abdullah Univ. of Science & Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#172792" title="Click to go to the Author Index">Ghanem, Bernard</a></td><td class="r">King Abdullah Univ. of Science and Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab104" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0104.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Servoing" title="Click to go to the Keyword Index">Visual Servoing</a>, <a href="IROS16_KeywordIndexMedia.html#Surveillance_Systems" title="Click to go to the Keyword Index">Surveillance Systems</a></span><br>
                           <strong>Abstract:</strong> In this paper, we propose a persistent, robust and autonomous object tracking system for unmanned aerial vehicles (UAVs) called Persistent Aerial Tracking (PAT). A computer vision and control strategy is applied to a diverse set of moving objects (e.g. humans, animals, cars, boats, etc.) integrating multiple UAVs with a stabilized RGB camera. A novel strategy is employed to successfully track objects over a long period, by ’handing over the camera’ from one UAV to another. We evaluate several state-of-the-art trackers on the VIVID aerial video dataset and additional sequences that are specifically tailored to low altitude UAV target tracking. Based on the evaluation, we select the leading tracker and improve upon it by optimizing for both speed and performance, integrate the complete system into an off-the-shelf UAV, and obtain promising results showing the robustness of our solution in real-world aerial scenarios.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_04">16:28-16:29, Paper TuT32.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0213.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('213'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Design, Modeling and Control of Omni-Directional Aerial Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#166827" title="Click to go to the Author Index">Park, Sangyul</a></td><td class="r">Seoul National Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179376" title="Click to go to the Author Index">Her, Jongbeom</a></td><td class="r">Seoul National Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195520" title="Click to go to the Author Index">Kim, Juhyeok</a></td><td class="r">Seoul National Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104207" title="Click to go to the Author Index">Lee, Dongjun</a></td><td class="r">Seoul National Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab213" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0213.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Space_Robotics_and_Automation" title="Click to go to the Keyword Index">Space Robotics and Automation</a></span><br>
                           <strong>Abstract:</strong> We propose a novel multi-rotor flying platform, ODAR (omni-directional aerial robot), which is fully-actuated (i.e., can assume arbitrary motion in SE(3) or generate arbitrary control wrench in se(3)) by six opportunistically distributed rotors, each driven by reversible ESC (electronic speed controller) for bi-directional thrust generation. Due to this omni-directional wrench generation, the ODAR system can realize such powerful behaviors impossible with conventional multi-rotor flying platforms as: 360^{circ} photo/video shooting while holding its position for VR scene generation; or resisting side-way gust while keeping its attitude and exerting downward pushing force larger than its own weight for aerial manipulation applications. This paper presents optimal mechanical design, modeling and control, hardware and software implementation, and experimental verification of the ability of the ODAR system to attain those behaviors impossible with the conventional drones as stated above.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_05">16:29-16:30, Paper TuT32.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0436.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('436'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Drone with Insect-Inspired Folding Wings</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195512" title="Click to go to the Author Index">Dufour, Louis</a></td><td class="r">Ec. Pol. Fédérale De Lausanne -EPFL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195513" title="Click to go to the Author Index">Owen, Kevin</a></td><td class="r">École Pol. Fédérale</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#148776" title="Click to go to the Author Index">Mintchev, Stefano</a></td><td class="r">École Pol. Fédérale De Lausanne</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103183" title="Click to go to the Author Index">Floreano, Dario</a></td><td class="r">Ec. Pol. Federal, Lausanne</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab436" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0436.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Biologically_Inspired_Robots" title="Click to go to the Keyword Index">Biologically-Inspired Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Search_and_Rescue_Robots" title="Click to go to the Keyword Index">Search and Rescue Robots</a></span><br>
                           <strong>Abstract:</strong> Flying robots are increasingly adopted in search and rescue missions because of their capability to quickly collect and stream information from remote and dangerous areas. To further enhance their use, we are investigating the development of a new class of drones, foldable sensorized hubs that can quickly take off from rescuers’ hands as soon as they are taken out of a pocket or a backpack. With this aim, this paper presents the development of a foldable wing inspired by insects. The wing can be packaged for transportation or deployed for flight in half a second with a simple action from the user. The wing is manufactured as a thick origami structure with a foldable multi-layer material. The prototype of the foldable wing is experimentally characterized and validated in flight on a mini-drone.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_06">16:30-16:31, Paper TuT32.6</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0447.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('447'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Real-Time Path Generation for Multicopters in Environments with Obstacles</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192023" title="Click to go to the Author Index">Nguyen, Dong Hai Phuong</a></td><td class="r">DIBRIS, Univ. of Genova</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#155018" title="Click to go to the Author Index">Recchiuto, Carmine Tommaso</a></td><td class="r">Univ. of Genova</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#108312" title="Click to go to the Author Index">Sgorbissa, Antonio</a></td><td class="r">Univ. of Genova</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab447" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0447.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Search_and_Rescue_Robots" title="Click to go to the Keyword Index">Search and Rescue Robots</a></span><br>
                           <strong>Abstract:</strong> The article proposes a solution allowing a multicopter to generate and follow a path while taking into account the obstacles in the environment. Specifically, we introduce a method for path definition that describes a curve as the intersection of two surfaces. Then, the article proposes a computationally efficient strategy allowing to modify either surface, and hence the resulting path, to take into account the presence of obstacles perceived in real–time. The algorithm has been implemented and embedded in a software package to control the flight of a fully autonomous AscTec Firefly hexacopter with two cameras and onboard processing capabilities.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_07">16:31-16:32, Paper TuT32.7</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0459.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('459'); return false" title="Click to show or hide the keywords and abstract">Learning the Hidden Human Knowledge of UAV Pilots When Navigating in a Cluttered Environment for Improving Path Planning</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195781" title="Click to go to the Author Index">Alzugaray, Ignacio</a></td><td class="r">Univ. Pol. De Cataluyna</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103488" title="Click to go to the Author Index">Sanfeliu, Alberto</a></td><td class="r">Univ. Pol. De Cataluyna</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab459" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Learning_from_Demonstration" title="Click to go to the Keyword Index">Learning from Demonstration</a></span><br>
                           <strong>Abstract:</strong> We propose in this work a new model of how the hidden human knowledge (HHK) of UAV pilots can be incorporated in the UAVs path planning generation. We intuitively know that human’s pilots barely manage or even attempt to drive the UAV through a path that is optimal attending to some criteria as an optimal planner would suggest. Although human pilots might get close but not reach the optimal path proposed by some planner that optimizes over time or distance, the final effect of this differentiation could be not only surprisingly better, but also desirable. In the best scenario for optimality, the path that human pilots generate would deviate from the optimal path as much as the hidden knowledge that its perceives is injected into the path. The aim of our work is to use real human pilot paths to learn the hidden knowledge using repulsion fields and to incorporate this knowledge afterwards in the environment obstacles as cause of the deviation from optimality. We present a strategy of learning this knowledge based on attractor and repulsors, the learning method and a modified RRT* that can use this knowledge for path planning. Finally we do real-life tests and we compare the resulting paths with and without this knowledge.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_08">16:32-16:33, Paper TuT32.8</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0593.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('593'); return false" title="Click to show or hide the keywords and abstract">Aerial Torsional Manipulation Employing Multirotor Flying Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183275" title="Click to go to the Author Index">Shimahara, Syohei</a></td><td class="r">Ritsumeikan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183405" title="Click to go to the Author Index">Leewiwatwong, Suphachart</a></td><td class="r">Ritsumeikan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183294" title="Click to go to the Author Index">Ladig, Robert</a></td><td class="r">Ritsumeikan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#161409" title="Click to go to the Author Index">Shimonomura, Kazuhiro</a></td><td class="r">Ritsumeikan Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab593" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Mobile_Manipulation" title="Click to go to the Keyword Index">Mobile Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a></span><br>
                           <strong>Abstract:</strong> We describe an implementation of torsional work at high altitude employing an aerial robot equipped with an upward directed hand on top of a hexarotor airframe, considering its workspace as above the airframe. The aerial robot in this study consists of a small hexarotor platform and a robotic hand module including a gripper. After grasping the object, the torsional work is executed by using yaw rotation of the body of the robot. In order to increase a torque for the torsional task, an impact mechanism is embedded in the base of the hand module. A maximum instant torque generated with this impact mechanism was more than 4 N·m. In addition, number of the rotation of the body is measured through images captured by on-board camera mounted beside the hand. Through experiments, we verified the feasibility of the present robot by successfully unscrewing a light bulb set to the socket above the robot and safely landing with the detached bulb.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_09">16:33-16:34, Paper TuT32.9</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0610.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('610'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Real-Time Dense Surface Reconstruction for Aerial Manipulation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196025" title="Click to go to the Author Index">Karrer, Marco</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192445" title="Click to go to the Author Index">Kamel, Mina</a></td><td class="r">Autonomous Systems Lab, ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100088" title="Click to go to the Author Index">Siegwart, Roland</a></td><td class="r">ETH Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#115127" title="Click to go to the Author Index">Chli, Margarita</a></td><td class="r">ETH Zurich</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab610" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0610.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a></span><br>
                           <strong>Abstract:</strong> With robotic systems reaching considerable maturity in basic self-localization and environment mapping, new research avenues open up pushing for interaction of a robot with its surroundings for added autonomy. However, the transition from traditionally sparse feature-based maps to dense and accurate scene-estimation imperative for realistic manipulation is not straightforward. Moreover, achieving this level of scene perception in real-time from a computationally constrained and highly shaky and agile platform, such as a small an Unmanned Aerial Vehicle (UAV) is perhaps the most challenging scenario for perception for manipulation. Drawing inspiration from otherwise computationally constraining Computer Vision techniques, we present a system combining visual, inertial and depth information to achieve dense, local scene reconstruction of high precision in real-time. Our evaluation testbed is formed using ground-truth not only in the pose of the sensor-suite, but also the scene reconstruction using a highly accurate laser scanner, offering unprecedented comparisons of scene estimation to ground-truth using real sensor data. Given the lack of any real, ground-truth datasets for environment reconstruction, our V4RL Dense Surface Reconstruction dataset is publicly available.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_10">16:34-16:35, Paper TuT32.10</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0698.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('698'); return false" title="Click to show or hide the keywords and abstract">SUAV: Q - an Improved Design for a Transformable Solar-Powered UAV</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#132022" title="Click to go to the Author Index">D'Sa, Ruben</a></td><td class="r">Univ. of Minnesota</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191800" title="Click to go to the Author Index">Jenson, Devon</a></td><td class="r">Cse, Umn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196163" title="Click to go to the Author Index">Henderson, Travis</a></td><td class="r">Cse, Umn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196168" title="Click to go to the Author Index">Kilian, Jack</a></td><td class="r">Cse, Umn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196169" title="Click to go to the Author Index">Schulz, Bobby</a></td><td class="r">Cse, Umn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196170" title="Click to go to the Author Index">Calvert, Michael</a></td><td class="r">Cse, Umn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196171" title="Click to go to the Author Index">Heller, Thaine</a></td><td class="r">Cse, Umn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101709" title="Click to go to the Author Index">Papanikolopoulos, Nikos</a></td><td class="r">Univ. of Minnesota</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab698" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> Throughout the wide range of aerial robot related applications, selecting a particular airframe is often a tradeoff. Fixed-wing small-scale unmanned aerial vehicles (UAVs) typically have difficulty surveying at low altitudes while quadrotor UAVs, having more maneuverability, suffer from limited flight time. Recent prior work [1] proposes a solar-powered small-scale aerial vehicle designed to transform between fixed-wing and quad-rotor configurations. Surplus energy collected and stored while in a fixed-wing configuration is utilized while in a quad-rotor configuration. This paper presents an improvement to the robot’s design in [1] by pursuing a modular airframe, an optimization of the hybrid propulsion system, and solar power electronics. Two prototypes of the robot have been fabricated for independent testing of the airframe in fixed-wing and quad-rotor states. Validation of the solar power electronics and hybrid propulsion system designs were demonstrated through a combination of simulation and empirical data from prototype hardware.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_11">16:35-16:36, Paper TuT32.11</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0731.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('731'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Efficient Multi-Camera Visual-Inertial SLAM for Micro Aerial Vehicles</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169352" title="Click to go to the Author Index">Houben, Sebastian</a></td><td class="r">Univ. of Bonn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192242" title="Click to go to the Author Index">Quenzel, Jan</a></td><td class="r">Univ. of Bonn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191335" title="Click to go to the Author Index">Krombach, Nicola</a></td><td class="r">Univ. of Bonn</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107752" title="Click to go to the Author Index">Behnke, Sven</a></td><td class="r">Univ. of Bonn</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab731" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0731.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#SLAM" title="Click to go to the Keyword Index">SLAM</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Tracking" title="Click to go to the Keyword Index">Visual Tracking</a></span><br>
                           <strong>Abstract:</strong> Visual SLAM is an area of vivid research and bears countless applications for moving robots. In particular, micro aerial vehicles benefit from visual sensors due to their low weight. Their motion is, however, often faster and more complex than that of ground-based robots which is why systems with multiple cameras are currently evaluated and deployed. This, in turn, drives the computational demand for visual SLAM algorithms.<p>We present an extension of the recently introduced monocular ORB-SLAM for multiple cameras alongside an inertial measurement unit (IMU). Our main contributions are: Embedding the multi-camera setup into the underlying graph SLAM approach that defines the upcoming sparse optimization problems on several adjusted subgraphs, integration of an IMU filter that supports visual tracking, and enhancements of the original algorithm in local map estimation and keyframe creation. The SLAM system is evaluated on a public stereo SLAM dataset for flying robots and on a new dataset with three mounted cameras.<p>The main advantages of the proposed method are its restricted computational load, high positional accuracy, and low number of parameters.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_12">16:36-16:37, Paper TuT32.12</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0757.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('757'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Cooperative Transportation of a Payload Using Quadrotors: A Reconfigurable Cable-Driven Parallel Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#131403" title="Click to go to the Author Index">Masone, Carlo</a></td><td class="r">Max Planck Inst. for Biological Cybernetics</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#115597" title="Click to go to the Author Index">Buelthoff, Heinrich H.</a></td><td class="r">Max Planck Inst. for Biol. Cybernetics</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122526" title="Click to go to the Author Index">Stegagno, Paolo</a></td><td class="r">Max Planck Inst. for Biological Cybernetics</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab757" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0757.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Kinematics" title="Click to go to the Keyword Index">Kinematics</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Parallel_Robots" title="Click to go to the Keyword Index">Parallel Robots</a></span><br>
                           <strong>Abstract:</strong> This paper addresses the problem of cooperative aerial transportation of an object using a team of quadrotors. The approach presented to solve this problem accounts for the full dynamics of the system and it is inspired by the literature on reconfigurable cable-driven parallel robots (RCDPR). Using the modelling convention of RCDPR it is derived a direct relation between the motion of the quadrotors and the motion of the payload. This relation makes explicit the available internal motion of the system, which can be used to automatically achieve additional tasks. The proposed method does not require to specify a priory the forces in the cables and uses a tension distribution algorithm to optimally distribute them among the robots. The presented framework is also suitable for online teleoperation. Physical simulations with a human-in-the-loop validate the proposed approach.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_13">16:37-16:38, Paper TuT32.13</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0786.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('786'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>High Accuracy Visual Servoing for Aerial Manipulation Using a 7 Degrees of Freedom Industrial Manipulator</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#165944" title="Click to go to the Author Index">Laiacker, Maximilian</a></td><td class="r">German Aerospace Center (DLR)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#151327" title="Click to go to the Author Index">Huber, Felix</a></td><td class="r">German Aerospace Center</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103966" title="Click to go to the Author Index">Kondak, Konstantin</a></td><td class="r">German Aerospace Center</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab786" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0786.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Perception_for_Grasping_and_Manipulation" title="Click to go to the Keyword Index">Perception for Grasping and Manipulation</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Servoing" title="Click to go to the Keyword Index">Visual Servoing</a></span><br>
                           <strong>Abstract:</strong> This paper is devoted to the performance optimization of an aerial manipulation system composed of a Flettner-helicopter and 7 DoF manipulator. With experiments we demonstrate that the time delays in signal propagation between perception and actuation modules play an important role for the overall performance of an aerial manipulator system using visual servoing. We present an approach for estimation of the perception-action time delay and its active compensation based on the predicted motion of the manipulator end-effector. Experiments show that compensating these delays improve the manipulation performance even more than elaborated methods for cooperative arm-helicopter control. The proposed approach should not be considered as a replacement of armhelicopter coordinated control but as an extension. Additionally the reliability of the visual servoing is improved by implementing a multi object localization that is robust to occlusions of the target object. The accuracy and robustness of the proposed visual servoing and active compensation algorithms are demonstrated in inand outdoor experiments With the proposed algorithm the aerial manipulator is able to repeatedly grasp an object with an accuracy better than 2cm.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_14">16:38-16:39, Paper TuT32.14</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0825.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('825'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>The Flying Anemometer: Unified Estimation of Wind Velocity from Aerodynamic Power and Wrenches</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#152365" title="Click to go to the Author Index">Tomic, Teodor</a></td><td class="r">German Aerospace Center (DLR)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#152540" title="Click to go to the Author Index">Schmid, Korbinian</a></td><td class="r">Roboception GmbH</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#152541" title="Click to go to the Author Index">Lutz, Philipp</a></td><td class="r">German Aerospace Center (DLR)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196300" title="Click to go to the Author Index">Mathers, Andrew</a></td><td class="r">WindEEE Res. Inst. Western Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#108317" title="Click to go to the Author Index">Haddadin, Sami</a></td><td class="r">Leibniz Univ. Hanover</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab825" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0825.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Model_Learning" title="Click to go to the Keyword Index">Model Learning</a></span><br>
                           <strong>Abstract:</strong> We consider the problem of estimating the wind velocity perceived by a flying multicopter, from data acquired by onboard sensors and knowledge of its aerodynamics model only. We employ two complementary methods. The first is based on the estimation of the external wrench (force and torque) due to aerodynamics acting on the robot in flight. Wind velocity is obtained by inverting an identified model of the aerodynamic forces. The second method is based on the estimation of the propeller aerodynamic power, and provides an estimate independent of other sensors. We show how to calculate components of the wind velocity using multiple aerodynamic power measurements, when the poses between them are known. The method uses the motor current and angular velocity as measured by the electronic speed controllers, essentially using the propellers as wind sensors. Verification of the methods and model identification were done using measurements acquired during autonomous flights in a 3D wind tunnel.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_15">16:39-16:40, Paper TuT32.15</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0965.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('965'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Self-Organized UAV Traffic in Realistic Environments</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169750" title="Click to go to the Author Index">Virágh, Csaba</a></td><td class="r">Eötvös Univ. of Budapest</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196091" title="Click to go to the Author Index">Nagy, Mate</a></td><td class="r">Max Planck Inst. of Ornithology; Univ. Konstanz; MTA-EL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196089" title="Click to go to the Author Index">Gershenson, Carlos</a></td><td class="r">Univ. Nacional Autonoma De Mexico</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168540" title="Click to go to the Author Index">Vásárhelyi, Gábor</a></td><td class="r">Eötvös Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab965" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0965.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Self_Organised_Robot_Systems" title="Click to go to the Keyword Index">Self-Organised Robot Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Planning__Scheduling_and_Coordination" title="Click to go to the Keyword Index">Planning, Scheduling and Coordination</a></span><br>
                           <strong>Abstract:</strong> We investigated different dense multirotor UAV traffic simulation scenarios in open 2D and 3D space, under realistic environments with the presence of sensor noise, communication delay, limited communication range, limited sensor update rate and finite inertia.We implemented two fundamental self-organized algorithms: one with constant direction and one with constant velocity preference to reach a desired target. We performed evolutionary optimization on both algorithms in five basic traffic scenarios and tested the optimized algorithms under different vehicle densities. We provide optimal algorithm and parameter selection criteria and compare the maximal flux and collision risk of each solution and situation. We found that i) different scenarios and densities require different algorithmic approaches, i.e., UAVs have to behave differently in sparse and dense environments or when they have common or different targets; ii) a slower-is-faster effect is implicitly present in our models, i.e., the maximal flux is achieved at densities where the average speed is far from maximal; iii) communication delay is the most severe destabilizing environmental condition that has a fundamental effect on performance and needs to be taken into account when designing algorithms to be used in real life.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_16">16:40-16:41, Paper TuT32.16</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0996.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('996'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Vision Based Collaborative Localization for Multirotor Vehicles</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190651" title="Click to go to the Author Index">Vemprala, Sai</a></td><td class="r">Arizona State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102948" title="Click to go to the Author Index">Saripalli, Srikanth</a></td><td class="r">Arizona State Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab996" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0996.VI.mpg" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a></span><br>
                           <strong>Abstract:</strong> We present a framework for vision based localization for two or more multirotor aerial vehicles relative to each other. This collaborative localization technique is built upon a relative pose estimation strategy between two or more cameras with the capability of estimating accurate metric poses between each other even through fast motion and continually changing environments. Through synchronized feature detection and tracking with a robust outlier rejection process, classical multiple view geometry concepts have been utilized for obtaining scale-ambiguous relative poses, which are then refined through reconstruction and pose optimization to provide a metric estimate. Furthermore, we present the implementation details of this technique followed by a set of results which involves evaluation of the accuracy of the pose estimates through test cases in both simulated and real experiments. Test cases include keeping one camera stationary as the other is mounted on a quadrotor which is then flown through various types of trajectories. We also perform a quantitative comparison with a GPS/IMU localization technique to demonstrate the accuracy of our method.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_17">16:41-16:42, Paper TuT32.17</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0999.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('999'); return false" title="Click to show or hide the keywords and abstract">Long-Range GPS-Denied Aerial Inertial Navigation with LIDAR Localization</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#132885" title="Click to go to the Author Index">Hemann, Garrett</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101807" title="Click to go to the Author Index">Singh, Sanjiv</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104298" title="Click to go to the Author Index">Kaess, Michael</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab999" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a></span><br>
                           <strong>Abstract:</strong> Despite significant progress in GPS-denied autonomous flight, long-distance traversals (> 100 km) in the absence of GPS remain elusive. This paper demonstrates a method capable of accurately estimating the aircraft state over a 218 km flight with a final position error of 27 m, 0.012% of the distance traveled. Our technique efficiently captures the full state dynamics of the air vehicle with semi-intermittent global corrections using LIDAR measurements matched against an a priori Digital Elevation Model (DEM). Using an error-state Kalman filter with IMU bias estimation, we are able to maintain a high-certainty state estimate, reducing the computation time to search over a global elevation map. A sub region of the DEM is scanned with the latest LIDAR projection providing a correlation map of landscape symmetry. The optimal position is extracted from the correlation map to produce a position correction that is applied to the state estimate in the filter. This method provides a GPS-denied state estimate for long range drift-free navigation. We demonstrate this method on two flight data sets from a full-sized helicopter, showing significantly longer flight distances over the current state of the art.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_18">16:42-16:43, Paper TuT32.18</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1003.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1003'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Aerial Robots with Rigid/Elastic-Joint Arms: Single-Joint Controllability Study and Preliminary Experiments</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169311" title="Click to go to the Author Index">Yuksel, Burak</a></td><td class="r">Max Planck Inst. for Biological Cybernetics</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180805" title="Click to go to the Author Index">Staub, Nicolas</a></td><td class="r">LAAS-CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104988" title="Click to go to the Author Index">Franchi, Antonio</a></td><td class="r">LAAS-CNRS</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1003" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1003.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a>, <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a></span><br>
                           <strong>Abstract:</strong> We present the dynamic modeling, analysis, and control design of a Planar-Vertical Take-Off and Landing (PVTOL) underactuated aerial vehicle equipped either with a rigid- or an elastic-joint arm. We prove that in both cases the system is exactly linearizable with a dynamic feedback and differentially flat for the same set of outputs (but different controllers). We compare the two cases with extensive and realistic simulations, which show that the rigid-joint case outperforms the elastic-joint case for aerial grasping tasks while the converse holds for link-velocity amplification tasks. We present preliminary experimental results using a actuated joint with variable stiffness (VSA) on a quadrotor platform.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_19">16:43-16:44, Paper TuT32.19</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1174.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1174'); return false" title="Click to show or hide the keywords and abstract">Human-Interpretable Diagnostic Information for Robotic Planning Systems</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192462" title="Click to go to the Author Index">Feng, Lu</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192463" title="Click to go to the Author Index">Humphrey, Laura</a></td><td class="r">Air Force Res. Lab</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169096" title="Click to go to the Author Index">Lee, Insup</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#131686" title="Click to go to the Author Index">Topcu, Ufuk</a></td><td class="r">Univ. of Pennsylvania</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1174" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Planning__Scheduling_and_Coordination" title="Click to go to the Keyword Index">Planning, Scheduling and Coordination</a>, <a href="IROS16_KeywordIndexMedia.html#Formal_Methods_in_Robotics_and_Automation" title="Click to go to the Keyword Index">Formal Methods in Robotics and Automation</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Centered_Robotics" title="Click to go to the Keyword Index">Human Centered Robotics</a></span><br>
                           <strong>Abstract:</strong> Advances in automation have the potential to reduce the workload required for human planning and execution of missions carried out by robotic systems such as unmanned aerial vehicles (UAVs). However, automation can also result in an increase in system complexity and a corresponding decrease in system transparency, which makes identifying and reasoning about errors in mission plans more difficult. To help explain errors in robotic planning systems, we define a notion of structured probabilistic counterexamples, which provide human- interpretable diagnostic information that captures requirements violations resulting from complex probabilistic robotic behavior. We propose an approach for generating such counterexamples using mixed integer linear programming and demonstrate the usefulness of our approach via a case study of UAV mission planning based on the AMASE multi-UAV simulator.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_20">16:44-16:45, Paper TuT32.20</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1190.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1190'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Swarm of Flying Smartphones</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142369" title="Click to go to the Author Index">Loianno, Giuseppe</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160721" title="Click to go to the Author Index">Mulgaonkar, Yash</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178030" title="Click to go to the Author Index">Brunner, Chris</a></td><td class="r">Qualcomm</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183700" title="Click to go to the Author Index">Ahuja, Dheeraj</a></td><td class="r">Qualcomm Tech. Inc</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183699" title="Click to go to the Author Index">Ramanandan, Arvind</a></td><td class="r">Qualcomm Res</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183707" title="Click to go to the Author Index">Chari, Murali</a></td><td class="r">Qualcomm Tech. Inc</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183744" title="Click to go to the Author Index">Diaz, Serafin</a></td><td class="r">Qualcomm Tech. Inc</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104342" title="Click to go to the Author Index">Kumar, Vijay</a></td><td class="r">Univ. of Pennsylvania</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1190" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1190.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a></span><br>
                           <strong>Abstract:</strong> In the last decade, consumer electronic devices such as smartphones, are packaged with small cameras, gyroscopes, and accelerometers, all sensors allowing autonomous deployment of aerial robots in GPS-denied environments. Our previous work [1], demonstrated the feasibility of using smartphones for autonomous flight. In many applications, there is a large interest to the use multiple autonomous aerial vehicles in a cooperative manner to speed up the operation of the mission. In this work, we present the first fully autonomous smartphonebased swarm of quadrotors. Multiple vehicles are able to plan safe trajectories avoiding inter-robot collisions, optimizing at the same time a given task and concurrently building in a cooperative manner a 3-D map of the environment. The sensing, sensor fusion, control, and planning are all done on an off-the-shelf Samsung Galaxy S5 smartphone using just the single camera and IMU available on the phone. The work allows any consumer with multiple smartphones to autonomously drive a swarm of multiple vehicles without GPS, by downloading an app, and have the swarm cooperatively map a 3-D environment.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_21">16:45-16:46, Paper TuT32.21</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1302.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1302'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Modeling and Control of FAST-Hex: A Fully-Actuated by Synchronized-Tilting Hexarotor</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#145863" title="Click to go to the Author Index">Ryll, Markus</a></td><td class="r">Lab. for Analysis and Architecture of Systems</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196402" title="Click to go to the Author Index">Bicego, Davide</a></td><td class="r">LAAS-CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104988" title="Click to go to the Author Index">Franchi, Antonio</a></td><td class="r">LAAS-CNRS</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1302" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1302.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> We present FAST-Hex, a novel UAV concept which is able to smoothly change its configuration from underactuated to fully actuated by using only one additional motor that tilts all propellers at the same time. FAST-Hex can adapt to the task at hand by finely tuning its configuration from the efficient (but underactuated) flight (typical of coplanar multi-rotor platforms) to the full-pose-tracking (but less efficient) flight, which is attainable by non-coplanar multi-rotors. We also introduce a novel full-pose geometric controller for generic multi-rotors (not only the FAST-Hex) that outperforms classical inverse dynamics approaches. The controller receives as input any reference pose in R3xSO(3) (3D position + 3D orientation). Exact tracking is achieved if the reference pose is feasible with respect to the propeller spinning rate saturations. In case of unfeasibility a new feasible desired trajectory is generated online giving priority to the positional part. The new controller is tested with the FAST-Hex but can be used for many other multi-rotor platforms: underactuated, slightly fully-actuated and completely fully-actuated.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_22">16:46-16:47, Paper TuT32.22</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1373.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1373'); return false" title="Click to show or hide the keywords and abstract">Two Meter Solar UAV: Design Approach and Performance Prediction for Autonomous Sensing Applications</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#157529" title="Click to go to the Author Index">Morton, Scott</a></td><td class="r">Univ. of Minnesota</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101709" title="Click to go to the Author Index">Papanikolopoulos, Nikos</a></td><td class="r">Univ. of Minnesota</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1373" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> This work focuses on the design and predicted performance of a two meter wingspan solar powered unmanned aerial vehicle (UAV). Such a platform would be ideal for distributed robotics applications because it combines the portability and deployment simplicity of a small airframe with the long flight time of a solar UAV. Methods to design and predict properties of a two meter solar UAV are described including airframe type selection, mass estimation, and propulsion requirements. A simplified approach to predict flight time is presented as well as an improved metric for quantifying multi-day flight robustness. Maximum flight time for the two meter airframe considered is estimated to be greater than ten hours which is an order of magnitude improvement over reported commercially available capabilities. In terms of multi-day flight capability, total mass is predicted to be within the bounds of a realizable aircraft based on extrapolation from larger experimentally tested multi-day solar UAVs.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_23">16:47-16:48, Paper TuT32.23</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1375.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1375'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Takeoff and Landing on Slopes Via Inclined Hovering with a Tethered Aerial Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179792" title="Click to go to the Author Index">Tognon, Marco</a></td><td class="r">LAAS-CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196416" title="Click to go to the Author Index">Testa, Andrea</a></td><td class="r">Univ. Del Salento</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196414" title="Click to go to the Author Index">Rossi, Enrica</a></td><td class="r">LAAS-CNRS</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104988" title="Click to go to the Author Index">Franchi, Antonio</a></td><td class="r">LAAS-CNRS</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1375" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1375.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a></span><br>
                           <strong>Abstract:</strong> In this paper we face the challenging problem of takeoff and landing on sloped surfaces for a VTOL aerial vehicle. We define the general conditions for a safe and robust maneuver and we analyze and compare two classes of methods to fulfill these conditions: free-flight vs. passively-tethered. Focusing on the less studied tethered method, we show its advantages w.r.t. the free-flight method thanks to the possibility of inclined hovering equilibria. We prove that the tether configuration and the inclination of the aerial vehicle w.r.t. the slope are flat outputs of the system and we design a hierarchical nonlinear controller based on this property. We then show how this controller can be used to land and takeoff in a robust way without the need of either a planner or a perfect tracking. The validity and applicability of the method in the real world is shown by experiments with a quadrotor that is able perform a safe landing and takeoff on a sloped surface.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_24">16:48-16:49, Paper TuT32.24</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1462.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1462'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Improvement of UAV's Flight Performance by Reducing the Drag Force of Spherical Shell</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184202" title="Click to go to the Author Index">Salaan, Carl John</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110231" title="Click to go to the Author Index">Okada, Yoshito</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196809" title="Click to go to the Author Index">Hozumi, Koichi</a></td><td class="r">Japan AerospaceTechnology Foundation</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107167" title="Click to go to the Author Index">Ohno, Kazunori</a></td><td class="r">Tohoku Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100118" title="Click to go to the Author Index">Tadokoro, Satoshi</a></td><td class="r">Tohoku Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1462" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1462.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> In recent years, several researchers focused their work on the mechanisms to protect the UAV from the dangerous collision with obstacles particularly the application of a spherical shell. However, this mechanism has some drawbacks when used in real-world mission especially in an outdoor environment. In the presence of wind, the spherical shell will experience significant air drag that will affect the flight performance of the UAV.<p>In this paper, we focused our study on improving the flight performance of the UAV by reducing the drag force caused mainly by the spherical shell. We analyzed its structure and components to minimize the unwanted drag force. We evaluated two spherical structure, namely the 2V geodesic and fullerene. We also evaluated the spherical shell's component so-called joint by applying airfoil shape and compared it to a flat-plate design. CFD simulation and wind tunnel experiment were used as an evaluation tool to obtain a quantitative result.<p>Based on our evaluation, changing from flat-plate to airfoil shape decrease the drag force of the joints by 72.31 %. Likewise, changing the structure from 2V geodesic to fullerene reduced the drag force of the connections by 12.42 %. The combination of fullerene structure and airfoil joints reduced the overall drag force by 34.74 %. <p>An actual flight test in the bridge in the presence of wind further verifies the performance of the system by using the spherical shell with fullerene structure and airfoil joint.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tut32_25">16:49-16:50, Paper TuT32.25</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1690.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1690'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Implementation of Varied Particle Container for Smoothed Particle Hydrodynamics – Based Aggregation for Unmanned Aerial Vehicle Quadrotor Swarm</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#167999" title="Click to go to the Author Index">Bandala, Argel</a></td><td class="r">De La Salle Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196741" title="Click to go to the Author Index">Faelden, Gerard Ely</a></td><td class="r">De La Salle Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196745" title="Click to go to the Author Index">Maningo, Jose Martin</a></td><td class="r">De La Salle Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196746" title="Click to go to the Author Index">Nakano, Reiichiro Christian</a></td><td class="r">De La Salle Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168005" title="Click to go to the Author Index">Vicerra, Ryan Rhay</a></td><td class="r">De La Salle Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#125300" title="Click to go to the Author Index">Dadios, Elmer P</a></td><td class="r">Dlsu</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1690" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1690.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Swarm_Robotics" title="Click to go to the Keyword Index">Swarm Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Distributed_Robot_Systems" title="Click to go to the Keyword Index">Distributed Robot Systems</a></span><br>
                           <strong>Abstract:</strong> The property of the Smoothed Particle Hydrodynamics (SPH) method of being mesh free, adaptable and suitable for tracking of individual particles makes it appropriate for approximating swarm behaviors for multi-agent robotics applications. The researchers modeled each of the swarm robots as SPH particles and then subjected them to external forces to exhibit aggregation and force certain formations. The external forces subjected to the SPH particles are gravity forces and container constraints. The containers explored in the study are simple geometrical primitives: sphere and cube. Computer simulations were done to show how SPH can facilitate in forcing swarm formations with the help of bounding primitives. Algorithm benchmarking was done to show how well SPH performs. Results show that SPH performs better than the benchmark algorithm by a margin of 0.703 and 1.016 units for the two set-ups, respectively. Actual robot implementation was also done to verify the effectivity and viability of the proposed algorithm in exhibiting the aggregation behavior. After 15 seconds of system run time, the interparticle distance and motion accuracy reached 96.93% and 91.14%, respectively.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuci1"><b>TuCI1</b></a></td>
               <td class="r">#111</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuci1" title="Click to go to the Program at a Glance"><b>Interactive Session: Human-Robot Interaction/Planning</b></a></td>
               <td class="r">Interactive Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#180494" title="Click to go to the Author Index">Scherbatyuk, Alexander</a></td><td class="r">Far Eastern Federal Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#107736" title="Click to go to the Author Index">Howard, Thomas</a></td><td class="r">Univ. of Rochester</td></tr>
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuci2"><b>TuCI2</b></a></td>
               <td class="r">#112</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuci2" title="Click to go to the Program at a Glance"><b>Interactive Session: Unmanned Aerial Vehicle</b></a></td>
               <td class="r">Interactive Session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#103488" title="Click to go to the Author Index">Sanfeliu, Alberto</a></td><td class="r">Univ. Pol. De Cataluyna</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#104207" title="Click to go to the Author Index">Lee, Dongjun</a></td><td class="r">Seoul National Univ</td></tr>
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuct1"><b>TuCT1</b></a></td>
               <td class="r">#101</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuct1" title="Click to go to the Program at a Glance"><b>Motion Planning and Obstacle Avoidance</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#119971" title="Click to go to the Author Index">Oh, Songhwai</a></td><td class="r">Seoul National Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#102253" title="Click to go to the Author Index">Martinoli, Alcherio</a></td><td class="r">EPFL</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct1_01">16:55-17:10, Paper TuCT1.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0040.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('40'); return false" title="Click to show or hide the keywords and abstract">Graph-Based Distributed Control for Adaptive Multi-Robot Patrolling through Local Formation Transformation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183017" title="Click to go to the Author Index">Wasik, Alicja</a></td><td class="r">EPFL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#148769" title="Click to go to the Author Index">Pereira, Jose Nuno</a></td><td class="r">Epfl Enac Iie Disal</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122994" title="Click to go to the Author Index">Ventura, Rodrigo</a></td><td class="r">Inst. Superior Técnico</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107437" title="Click to go to the Author Index">Lima, Pedro U.</a></td><td class="r">Inst. Superior Técnico - Inst. for Systems and Robotics</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102253" title="Click to go to the Author Index">Martinoli, Alcherio</a></td><td class="r">EPFL</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab40" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Cooperating_Robots" title="Click to go to the Keyword Index">Cooperating Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Distributed_Robot_Systems" title="Click to go to the Keyword Index">Distributed Robot Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Multi_Robot_Systems" title="Click to go to the Keyword Index">Multi-Robot Systems</a></span><br>
                           <strong>Abstract:</strong> Multi-robot cooperative navigation in real-world environments is essential in many applications, including surveillance and search-and-rescue missions. State-of-the-art methods for cooperative navigation are often tested in ideal laboratory conditions and not ready to be deployed in real-world environments, which are often cluttered with static and dynamic obstacles. In this work, we explore a graph-based framework to achieve control of real robot formations moving in a world cluttered with a variety of obstacles by introducing a new distributed algorithm for reconfiguring the formation shape. We systematically validate the reconfiguration algorithm using three real robots in scenarios of increasing complexity.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct1_02">17:10-17:25, Paper TuCT1.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0082.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('82'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Towards 3-D Distributed Odor Source Localization: An Extended Graph-Based Formation Control Algorithm for Plume Tracking</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160182" title="Click to go to the Author Index">Soares, Jorge M.</a></td><td class="r">École Pol. Fédérale De Lausanne</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117599" title="Click to go to the Author Index">Marjovi, Ali</a></td><td class="r">EPFL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190153" title="Click to go to the Author Index">Giezendanner, Jonathan</a></td><td class="r">EPFL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#190154" title="Click to go to the Author Index">Kodiyan, Anil</a></td><td class="r">EPFL</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#119420" title="Click to go to the Author Index">Aguiar, A. Pedro</a></td><td class="r">Faculty of Engineering, Univ. of Porto (FEUP)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#124047" title="Click to go to the Author Index">Pascoal, Antonio</a></td><td class="r">Inst. Superior Tecnico</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102253" title="Click to go to the Author Index">Martinoli, Alcherio</a></td><td class="r">EPFL</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab82" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0082.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Distributed_Robot_Systems" title="Click to go to the Keyword Index">Distributed Robot Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Sensor_Networks" title="Click to go to the Keyword Index">Sensor Networks</a>, <a href="IROS16_KeywordIndexMedia.html#Robotics_in_Hazardous_Fields" title="Click to go to the Keyword Index">Robotics in Hazardous Fields</a></span><br>
                           <strong>Abstract:</strong> The large number of potential applications for robotic odor source localization has motivated the development of a variety of plume tracking algorithms, the majority of which work in restricted two-dimensional scenarios.<p>In this paper, we introduce a distributed algorithm for 3-D plume tracking using a system of ground and aerial robots in formation. We propose an algorithm that takes advantage of spatially distributed measurements to track the plume in 3-D and lead the robots to the source by integrating three behaviors -- upwind movement, plume centering, and Laplacian feedback formation control.<p>We evaluate this strategy in simulation and with real robots in a wind tunnel. For a source close to the ground, results show that a team of robots running our algorithm reaches the source with low lateral error while also tracing the horizontal and vertical plume shape.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct1_03">17:25-17:40, Paper TuCT1.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0529.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('529'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Robust Modeling and Prediction in Dynamic Environments Using Recurrent Flow Networks</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#163618" title="Click to go to the Author Index">Choi, Sungjoon</a></td><td class="r">Seoul National Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179481" title="Click to go to the Author Index">Lee, Kyungjae</a></td><td class="r">Seoul National Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#119971" title="Click to go to the Author Index">Oh, Songhwai</a></td><td class="r">Seoul National Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab529" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0529.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Collision_Detection_and_Avoidance" title="Click to go to the Keyword Index">Collision Detection and Avoidance</a>, <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a></span><br>
                           <strong>Abstract:</strong> To enable safe motion planning in a dynamic environment, it is vital to anticipate and predict object movements. In practice, however, an accurate object identification among multiple moving objects is extremely challenging, making it infeasible to accurately track and predict individual objects. Furthermore, even for a single object, its appearance can vary significantly due to external effects, such as occlusions, varying perspectives, or illumination changes. In this paper, we propose a novel recurrent network architecture called a recurrent flow network that can infer the velocity of each cell and the probability of future occupancy from a sequence of occupancy grids which we refer to as an occupancy flow. The parameters of the recurrent flow network are optimized using Bayesian optimization. The proposed method outperforms three baseline optical flow methods, Lucas-Kanade, Lucas-Kanade with Tikhonov regularization, and HornSchunck methods, and a Bayesian occupancy grid filter in terms of both prediction accuracy and robustness to noise.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct1_04">17:40-17:55, Paper TuCT1.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0782.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('782'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Introspective Perception: Learning to Predict Failures in Vision Systems</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179528" title="Click to go to the Author Index">Daftry, Shreyansh</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179760" title="Click to go to the Author Index">Zeng, Sam</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106456" title="Click to go to the Author Index">Bagnell, James</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100325" title="Click to go to the Author Index">Hebert, Martial</a></td><td class="r">CMU</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab782" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0782.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Failure_Detection_and_Recovery" title="Click to go to the Keyword Index">Failure Detection and Recovery</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Navigation" title="Click to go to the Keyword Index">Visual Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a></span><br>
                           <strong>Abstract:</strong> As robots aspire for long-term autonomous operations in complex dynamic environments, the ability to reliably take mission-critical decisions in ambiguous situations becomes critical. This motivates the need to build systems that have situational awareness to assess how qualified they are at that moment to make a decision. We call this self-evaluating capability as introspection. In this paper, we take a small step in this direction and propose a generic framework for introspective behavior in perception systems. Our goal is to learn a model to reliably predict failures in a given system, with respect to a task, directly from input sensor data. We present this in the context of vision-based autonomous MAV flight in outdoor natural environments, and show that it effectively handles uncertain situations.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct1_05">17:55-18:10, Paper TuCT1.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0962.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('962'); return false" title="Click to show or hide the keywords and abstract">Performance Level Profiles: A Formal Language for Describing the Expected Performance of Functional Modules</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#182504" title="Click to go to the Author Index">Brafman, Ronen</a></td><td class="r">Ben-Gurion Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196406" title="Click to go to the Author Index">Bar-Sinai, Michael</a></td><td class="r">Ben-Gurion Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196411" title="Click to go to the Author Index">Ashkenazi, Maor</a></td><td class="r">Ben-Gurion Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab962" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Failure_Detection_and_Recovery" title="Click to go to the Keyword Index">Failure Detection and Recovery</a>, <a href="IROS16_KeywordIndexMedia.html#Software_and_Architecture" title="Click to go to the Keyword Index">Software and Architecture</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Safety" title="Click to go to the Keyword Index">Robot Safety</a></span><br>
                           <strong>Abstract:</strong> Despite the existence of powerful formal languages for writing robot controllers, most existing functional modules are written using standard programming languages. The existence of such a code base raises critical challenges: 1. How to enable automated analysis, monitoring, and reuse of existing code given that reasoning directly about code fragments is impractical. 2. How to convey to users the expected level of performance of an autonomous robot? 3. Perhaps most crucial: how to quickly identify abnormal behaviour of autonomous robots? This is a key impediment to the deployment of such platforms in open environments. We address these issues through the use of performance-level profiles (PLPs), a formal, yet intuitive, language for specifying the expected properties of functional modules, designed with the above aims in mind. PLPs are motivated by action specification languages, such as PDDL 2.1, but add novel elements important for robotic applications, such as update frequency, run-time statistics, progress measures, and trigger conditions, and take into account the different roles modules can play. PLPs have been used to support monitoring in two projects: an autonomous compact track loader, and a service robot. Additionally, we developed a number of tools for automated monitoring-code generation from PLPs.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuct2"><b>TuCT2</b></a></td>
               <td class="r">#102</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuct2" title="Click to go to the Program at a Glance"><b>Robot Vision</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#187730" title="Click to go to the Author Index">Ling, Yonggen</a></td><td class="r">The Hong Kong Univ. of Science and Tech</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#107990" title="Click to go to the Author Index">Sandini, Giulio</a></td><td class="r">Italian Inst. of Tech</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct2_01">16:55-17:10, Paper TuCT2.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0659.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('659'); return false" title="Click to show or hide the keywords and abstract">Fast Joint Compatibility Branch and Bound for Feature Cloud Matching</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#167502" title="Click to go to the Author Index">Shen, Xiaotong</a></td><td class="r">National Univ. of Singapore</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107110" title="Click to go to the Author Index">Frazzoli, Emilio</a></td><td class="r">Massachusetts Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101388" title="Click to go to the Author Index">Rus, Daniela</a></td><td class="r">MIT</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100760" title="Click to go to the Author Index">Ang Jr, Marcelo H</a></td><td class="r">National Univ. of Singapore</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab659" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a>, <a href="IROS16_KeywordIndexMedia.html#SLAM" title="Click to go to the Keyword Index">SLAM</a></span><br>
                           <strong>Abstract:</strong> In this work, we address the problem of robust data association for feature cloud matching. For matching two feature clouds observed at two different poses, we discover that the covariance matrix of the measurement prediction error can be written as the sum of a low rank matrix and a block diagonal matrix, if we assume that the features are observed independently at each pose. This special structure of the covariance matrix allows us to compute its inverse analytically and efficiently. Together with a good bookkeeping strategy, the complexity of the Joint Compatibility (JC) test is reduced to O(1). Contrary to the approximated JC test, ours is both exact and fast. Based on the efficient JC test algorithm and a branch and bound search procedure, we devise an algorithm, called Fast Joint Compatibility Branch and Bound (FastJCBB), to quickly obtain robust data association. <p>The FastJCBB algorithm is essentially modified from the conventional Joint Compatibility Branch and Bound (JCBB) algorithm and both of these algorithms are able to produce exactly the same data association results. However, with the substantial improvement in the efficiency of JC tests, our FastJCBB algorithm is much faster than the conventional JCBB, especially when matching two large feature clouds. It is reported that our FastJCBB algorithm is more than 740 times faster than the conventional JCBB in carrying out one million JC tests when matching two clouds with about 100 features each. Since both FastJCBB and JCBB share the same branch and bound procedure in exploring the interpretation tree, the search complexity remains exponential. Our main contribution is the significant improvement in the efficiency of exploring each node of the interpretation tree.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct2_02">17:10-17:25, Paper TuCT2.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0726.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('726'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Rigid Scene Flow for 3D LiDAR Scans</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#155157" title="Click to go to the Author Index">Dewan, Ayush</a></td><td class="r">Univ. of Freibug</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#186166" title="Click to go to the Author Index">Caselitz, Tim</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103074" title="Click to go to the Author Index">Tipaldi, Gian Diego</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101785" title="Click to go to the Author Index">Burgard, Wolfram</a></td><td class="r">Univ. of Freiburg</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab726" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0726.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Autonomous_Agents" title="Click to go to the Keyword Index">Autonomous Agents</a></span><br>
                           <strong>Abstract:</strong> The perception of the dynamic aspects of the environment is a highly relevant precondition for the realization of autonomous robot system acting in the real world. In this paper, we propose a novel method for estimating dense rigid scene flow in 3D LiDAR scans. We formulate the problem as an energy minimization problem, where we assume local geometric constancy and incorporate regularization for smooth motion fields. Analyzing the dynamics at point level helps in inferring the fine-grained details of motion. We show results on multiple sequences of the KITTI odometry dataset, where we seamlessly estimate multiple motions pertaining to different dynamic objects. Furthermore, we test our approach on a dataset with pedestrians to show how our method adapts to a case with non-rigid motion. For comparison we use the ground truth from KITTI and show how our method outperforms different ICP-based methods.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct2_03">17:25-17:40, Paper TuCT2.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0771.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('771'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>High-Precision Online Markerless Stereo Extrinsic Calibration</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#187730" title="Click to go to the Author Index">Ling, Yonggen</a></td><td class="r">The Hong Kong Univ. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142354" title="Click to go to the Author Index">Shen, Shaojie</a></td><td class="r">Hong Kong Univ. of Science and Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab771" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0771.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Calibration_and_Identification" title="Click to go to the Keyword Index">Calibration and Identification</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Navigation" title="Click to go to the Keyword Index">Visual Navigation</a></span><br>
                           <strong>Abstract:</strong> Stereo cameras and dense stereo matching algorithms are core components for many robotic applications due to their abilities to directly obtain dense depth measurements and their robustness against changes in lighting conditions. However, the performance of dense depth estimation relies heavily on accurate stereo extrinsic calibration. In this work, we present a real-time markerless approach for obtaining high-precision stereo extrinsic calibration using a novel 5-DOF (degrees-of-freedom) and nonlinear optimization on a manifold, which captures the observability property of vision-only stereo calibration. Our method minimizes epipolar errors between spatial per-frame sparse natural features. It does not require temporal feature correspondences, making it not only invariant to dynamic scenes and illumination changes, but also able to run significantly faster than standard bundle adjustment-based approaches. We introduce a principled method to determine if the calibration converges to the required level of accuracy, and show through online experiments that our approach achieves a level of accuracy that is comparable to offline marker-based calibration methods. Our method refines stereo extrinsic to the accuracy that is sufficient for block-matching-based dense disparity computation. It provides a cost-effective way to improve the reliability of stereo vision systems for long-term autonomy.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct2_04">17:40-17:55, Paper TuCT2.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1345.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1345'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Self-Supervised Monocular Distance Learning on a Lightweight Micro Air Vehicle</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196554" title="Click to go to the Author Index">Lamers, Kevin</a></td><td class="r">Delft Univ. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#169724" title="Click to go to the Author Index">Tijmons, Sjoerd</a></td><td class="r">Delft Univ. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#131961" title="Click to go to the Author Index">De Wagter, Christophe</a></td><td class="r">Delft Univ. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#131959" title="Click to go to the Author Index">de Croon, Guido</a></td><td class="r">TU Delft / ESA</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1345" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1345.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Learning" title="Click to go to the Keyword Index">Visual Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a></span><br>
                           <strong>Abstract:</strong> Obstacle detection by monocular vision is challenging because a single camera does not provide a direct measure for absolute distances to objects. A self-supervised learning approach is proposed that combines a camera and a very small short-range proximity sensor to find the relation between the appearance of objects in camera images and their corresponding distances. The method is efficient enough to run real time on a small camera system that can be carried onboard a lightweight MAV of 19 g. The effectiveness of the method is demonstrated by computer simulations and by experiments with the real platform in flight.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct2_05">17:55-18:10, Paper TuCT2.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1476.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1476'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Scene Flow Propagation for Semantic Mapping and Object Discovery in Dynamic Street Scenes</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196709" title="Click to go to the Author Index">Kochanov, Deyvid</a></td><td class="r">RWTH Aachen Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180164" title="Click to go to the Author Index">Osep, Aljosa</a></td><td class="r">RWTH Aachen Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#114207" title="Click to go to the Author Index">Stückler, Jörg</a></td><td class="r">RWTH Aachen Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#115270" title="Click to go to the Author Index">Leibe, Bastian</a></td><td class="r">RWTH Aachen Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1476" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1476.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a></span><br>
                           <strong>Abstract:</strong> Scene understanding is an important prerequisite for vehicles and robots that operate autonomously in dynamic urban street scenes. For navigation and high-level behavior planning, the robots not only require a persistent 3D model of the static surroundings - equally important, they need to perceive and keep track of dynamic objects. In this paper, we propose a method that incrementally fuses stereo frame observations into temporally consistent semantic 3D maps. In contrast to previous work, our approach uses scene flow to propagate dynamic objects within the map. Our method provides a persistent 3D occupancy as well as semantic belief on static as well as moving objects. This allows for advanced reasoning on objects despite noisy single-frame observations and occlusions. We develop a novel approach to discover object instances based on the temporally consistent shape, appearance, motion, and semantic cues in our maps. We evaluate our approaches to dynamic semantic mapping and object discovery on the popular KITTI benchmark and demonstrate improved results compared to single-frame methods.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuct3"><b>TuCT3</b></a></td>
               <td class="r">#103</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuct3" title="Click to go to the Program at a Glance"><b>Sensor-Based Planning</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#141359" title="Click to go to the Author Index">Indelman, Vadim</a></td><td class="r">Tech. - Israel Inst. of Tech</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#152116" title="Click to go to the Author Index">Bayat, Behzad</a></td><td class="r">EPFL | École Pol. Fédérale De Lausanne</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct3_01">16:55-17:10, Paper TuCT3.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0131.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('131'); return false" title="Click to show or hide the keywords and abstract">Computationally Efficient Decision Making under Uncertainty in High-Dimensional State Spaces</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195276" title="Click to go to the Author Index">Kopitkov, Dmitry</a></td><td class="r">Tech. - Israel Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#141359" title="Click to go to the Author Index">Indelman, Vadim</a></td><td class="r">Tech. - Israel Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab131" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Sensor_based_Planning" title="Click to go to the Keyword Index">Sensor-based Planning</a>, <a href="IROS16_KeywordIndexMedia.html#SLAM" title="Click to go to the Keyword Index">SLAM</a>, <a href="IROS16_KeywordIndexMedia.html#Autonomous_Agents" title="Click to go to the Keyword Index">Autonomous Agents</a></span><br>
                           <strong>Abstract:</strong> We develop a novel approach for decision making under uncertainty in high-dimensional state spaces, considering both active unfocused and focused inference, where in the latter case reducing the uncertainty of only a subset of variables is of interest. State of the art approaches typically first calculate the posterior information (or covariance) matrix, followed by determinant calculation of thereof, and do so separately for each candidate action. In contrast, using the generalized matrix determinant lemma, we avoid calculating these posteriors and determinants of large matrices. Furthermore, as our key contribution we introduce the concept of calculation re-use, performing a one-time computation that depends on state dimensionality and system sparsity, after which evaluating the impact of each candidate action no longer depends on state dimensionality. Such a concept is derived for both active focused and unfocused inference, leading to general, non-myopic and exact approaches that are faster by orders of magnitude compared to the state of the art. We verify our approach experimentally in two scenarios, sensor deployment (focused and unfocused) and measurement selection in visual SLAM, and show its superiority over standard techniques.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct3_02">17:10-17:25, Paper TuCT3.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0222.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('222'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Optimal Search Strategies for Pollutant Source Localization</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#152116" title="Click to go to the Author Index">Bayat, Behzad</a></td><td class="r">EPFL | École Pol. Fédérale De Lausanne</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195535" title="Click to go to the Author Index">Crasta, Naveena</a></td><td class="r">Inst. Superior Tecnico (IST), Univ. of Lisbon</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118320" title="Click to go to the Author Index">Li, Howard</a></td><td class="r">Univ. of New Brunswick</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105018" title="Click to go to the Author Index">Ijspeert, Auke</a></td><td class="r">EPFL</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab222" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0222.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Reactive_and_Sensor_Based_Planning" title="Click to go to the Keyword Index">Reactive and Sensor-Based Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a></span><br>
                           <strong>Abstract:</strong> This paper is aimed at developing optimal motion planning for a single autonomous surface vehicle (ASV) equipped with an on-board pollutant sensor that will maximize the sensor-related information available for source seeking. The ASV uses a nonlinear diffusion model of the pollutant source to estimate the intensity/level of the pollution at the present ASV location. The rate of detection of particles depends on the relative distance between the ASV and the source. First, we use a probabilistic map of the source location built through the sensor information for a dynamic motion planning of source seeking based on an entropy reduction formulation, where an appropriately defined Fisher information matrix (FIM) is used for entropy reduction or information gain. We derive the FIM for the set-up and investigate optimal trajectories. Next, we present an online nonlinear Monte Carlo algorithm that uses the obtained sensor information about pollutant at different vehicle locations to update a probabilistic uncertainty map of pollutant source location. As the mission unfolds the ASV motion is computed by considering a moving-horizon interval of decision, which will allow for the inclusion of new information available for optimal motion planning. The proposed motion planning approach is extended to take into account external disturbances and it is able to minimize the uncertainty in the pollutant source. Finally, we provide two case studies to demonstrate efficacy of the proposed motion planning algorithm.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct3_03">17:25-17:40, Paper TuCT3.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0271.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('271'); return false" title="Click to show or hide the keywords and abstract">Sampling-Based View Planning for 3D Visual Coverage Task with Unmanned Aerial Vehicle</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191291" title="Click to go to the Author Index">Jing, Wei</a></td><td class="r">Carnegie Mellon Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#162889" title="Click to go to the Author Index">Polden, Joseph</a></td><td class="r">Singapore Inst. of Manufacturing Tech. (SIMTech)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100227" title="Click to go to the Author Index">Lin, Wei</a></td><td class="r">SIMTech, A*STAR</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122166" title="Click to go to the Author Index">Shimada, Kenji</a></td><td class="r">Carnegie Mellon Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab271" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Sensor_based_Planning" title="Click to go to the Keyword Index">Sensor-based Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Task_Planning" title="Click to go to the Keyword Index">Task Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a></span><br>
                           <strong>Abstract:</strong> The view planning problem is the problem that involves finding suitable viewpoints for vision-related tasks such as inspection or reconstruction. In this paper, we propose a novel view planning algorithm for a camera-equipped Unmanned Aerial Vehicle (UAV) acquiring visual geometric information of target objects in its surrounding environment. The proposed model-based approach makes use of iterative random sampling and a probabilistic potential-field method to generate candidate viewpoints in a non-deterministic manner. Combinatorial optimization is then applied to select the most suitable subset of these candidate viewpoints to complete the given visual inspection or shape reconstruction task. The effectiveness of the proposed method is demonstrated through a number of computational tests that compare its overall performance against two previous methods. A field-test is also performed to demonstrate the method's applicability in a real world UAV-based shape reconstruction task of an outdoor statue.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct3_04">17:40-17:55, Paper TuCT3.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0864.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('864'); return false" title="Click to show or hide the keywords and abstract">Information-Theoretic Exploration with Bayesian Optimization</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#148801" title="Click to go to the Author Index">Bai, Shi</a></td><td class="r">Stevens Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191879" title="Click to go to the Author Index">Wang, Jinkun</a></td><td class="r">Stevens Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196322" title="Click to go to the Author Index">Chen, Fanfei</a></td><td class="r">Stevens Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#123501" title="Click to go to the Author Index">Englot, Brendan</a></td><td class="r">Stevens Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab864" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Reactive_and_Sensor_Based_Planning" title="Click to go to the Keyword Index">Reactive and Sensor-Based Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a>, <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a></span><br>
                           <strong>Abstract:</strong> We consider an autonomous exploration problem in which a mobile robot is guided by an information-based controller through an a priori unknown environment, choosing to collect its next measurement at the location estimated to be the most informative within its current field of view. We propose a novel approach to predict mutual information (MI) using Bayesian optimization. Over several iterations, candidate sensing actions are suggested by Bayesian optimization and added to a committee that repeatedly trains a Gaussian process (GP). The GP estimates MI throughout the robot's action space, serving as the basis for an acquisition function used to select the next candidate. The best sensing action in the committee is executed by the robot. This approach is compared over several environments with two batch methods, one which chooses the most informative action from a set of pseudo-random samples whose MI is explicitly evaluated, and one that applies GP regression to this sample set. Our computational results demonstrate that the proposed method provides not only computational efficiency and rapid map entropy reduction, but also robustness in comparison with competing approaches.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct3_05">17:55-18:10, Paper TuCT3.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1305.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1305'); return false" title="Click to show or hide the keywords and abstract">Mutual Information Based Communication Aware Path Planning: A Game Theoretic Perspective</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195828" title="Click to go to the Author Index">Ramaswamy, Vinod</a></td><td class="r">Univ. of Colorado, Boulder</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196500" title="Click to go to the Author Index">Moon, Sangwoo</a></td><td class="r">Univ. of Colorado Boulder</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102708" title="Click to go to the Author Index">Frew, Eric W.</a></td><td class="r">Univ. of Colorado</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#111113" title="Click to go to the Author Index">Ahmed, Nisar</a></td><td class="r">Univ. of Colorado Boulder</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1305" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Sensor_based_Planning" title="Click to go to the Keyword Index">Sensor-based Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Agent_Based_Systems" title="Click to go to the Keyword Index">Agent-Based Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Networked_Robots" title="Click to go to the Keyword Index">Networked Robots</a></span><br>
                           <strong>Abstract:</strong> This paper examines the problem of distributed path planning for a mobile sensor network comprised of communication-aware robots performing general information gathering missions. Mutual information is derived for distributed sensing over packet erasure channels that model multi-hop communication. We model distributed path planning as a non-cooperative game and derive utility functions that are optimized locally by each robot. Each robot computes the control input in a distributed manner that results in a combined action that can be bounded by the optimal centralized result by utilizing sub-modularity in certain cases. It is shown that when the communication model includes multi-hop communication to expand the coverage of the sensor network, the property of sub-modularity is lost. We further show that the additional global knowledge required for the local computation of utility functions can be learned by simple consensus approaches. Finally, we discuss a sampling approach to approximate the proposed utility functions in order to reduce the associated computational requirements.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuct4"><b>TuCT4</b></a></td>
               <td class="r">#104</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuct4" title="Click to go to the Program at a Glance"><b>Medical Robot and Systems 3</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#101785" title="Click to go to the Author Index">Burgard, Wolfram</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#168203" title="Click to go to the Author Index">Haouchine, Nazim</a></td><td class="r">INRIA</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct4_01">16:55-17:10, Paper TuCT4.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0055.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('55'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>An Egocentric Computer Vision Based Co-Robot Wheelchair</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#187326" title="Click to go to the Author Index">Li, Haoxiang</a></td><td class="r">Adobe Res</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191084" title="Click to go to the Author Index">Kutbi, Mohammed</a></td><td class="r">Stevens Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191009" title="Click to go to the Author Index">Li, Xin</a></td><td class="r">Stevens Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191083" title="Click to go to the Author Index">Cai, Changjiang</a></td><td class="r">Stevens Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#115227" title="Click to go to the Author Index">Mordohai, Philippos</a></td><td class="r">Stevens Inst. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180967" title="Click to go to the Author Index">Hua, Gang</a></td><td class="r">Stevens Inst. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab55" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0055.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Systems__Healthcare__and_Assisted_Living" title="Click to go to the Keyword Index">Medical Systems, Healthcare, and Assisted Living</a>, <a href="IROS16_KeywordIndexMedia.html#Wheeled_Robots" title="Click to go to the Keyword Index">Wheeled Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Robot_Interaction" title="Click to go to the Keyword Index">Human-Robot Interaction</a></span><br>
                           <strong>Abstract:</strong> Motivated by the emerging needs to improve the quality of life for the elderly and disabled individuals who rely on wheelchairs for mobility, and who might have limited or no hand functionality at all, we propose an egocentric computer vision based co-robot wheelchair to enhance their mobility without hand usage. The co-robot wheelchair is built upon a typical commercial power wheelchair. The user can access 360 degrees of motion direction as well as a continuous range of speed without the use of hands via the egocentric computer vision based control we developed. The user wears an egocentric camera and collaborates with the robotic wheelchair by conveying the motion commands with head motions. Compared with previous sip-n-puff, chin-control and tongue-operated solutions to hands-free mobility, this egocentric computer vision based control system provides a more natural human robot interface. Our experiments show that this design is of higher usability and users can quickly learn to control and operate the wheelchair. Besides its convenience in manual navigation, the egocentric camera also supports novel user-robot interaction modes by enabling autonomous navigation towards a detected person or object of interest. User studies demonstrate the usability and efficiency of the proposed egocentric computer vision co-robot wheelchair.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct4_02">17:10-17:25, Paper TuCT4.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0116.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('116'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Making Robots Mill Bone More Like Human Surgeons: Using Bone Density and Anatomic Information to Mill Safely and Efficiently</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192902" title="Click to go to the Author Index">Dillon, Neal P</a></td><td class="r">Vanderbilt Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#157389" title="Click to go to the Author Index">Fichera, Loris</a></td><td class="r">Vanderbilt Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195387" title="Click to go to the Author Index">Wellborn, Patrick</a></td><td class="r">Vanderbilt Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#127466" title="Click to go to the Author Index">Labadie, Robert F</a></td><td class="r">Vanderbilt Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101891" title="Click to go to the Author Index">Webster III, Robert James</a></td><td class="r">Vanderbilt Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab116" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0116.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Surgical_Robotics" title="Click to go to the Keyword Index">Surgical Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a></span><br>
                           <strong>Abstract:</strong> Surgeons and robots typically use different approaches for bone milling. Surgeons adjust their speed and tool incidence angle constantly, which enables them to efficiently mill porous bone. Surgeons also adjust milling parameters such as speed and depth of cut throughout the procedure based on proximity to sensitive structures like nerves and blood vessels. In this paper we use image-based bone density estimates and segmentations of vital anatomy to make a robot mill more like a surgeon and less like an industrial computer numeric controlled (CNC) milling machine. We produce patient-specific plans optimizing velocity and incidence angles for spherical cutting burrs. These plans are particularly useful in bones of variable density and porosity like the human temporal bone. They result in fast milling in non-critical areas, reducing overall procedure time, and lower forces near vital anatomy. We experimentally demonstrate the algorithm on temporal bone phantoms and show that it reduces mean forces near vital anatomy by 63% and peak forces by 50% in comparison to a CNC-type path, without adding time to the procedure.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct4_03">17:25-17:40, Paper TuCT4.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0940.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('940'); return false" title="Click to show or hide the keywords and abstract">Automatic Channel Selection in Neural Microprobes: A Combinatorial Multi-Armed Bandit Approach</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#186156" title="Click to go to the Author Index">Gordillo Chaves, Camilo Andres</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#113202" title="Click to go to the Author Index">Frank, Barbara</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171975" title="Click to go to the Author Index">Istvan, Ulbert</a></td><td class="r">Univ. of Budapest</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#149992" title="Click to go to the Author Index">Paul, Oliver</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171974" title="Click to go to the Author Index">Ruther, Patrick</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101785" title="Click to go to the Author Index">Burgard, Wolfram</a></td><td class="r">Univ. of Freiburg</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab940" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Neurorobotics" title="Click to go to the Keyword Index">Neurorobotics</a></span><br>
                           <strong>Abstract:</strong> State-of-the-art neural microprobes contain hundreds of electrodes within a single shaft. Due to hardware and wiring restrictions, it is usually only possible to measure a small subset of the available electrodes simultaneously. The selection of the best channels is typically performed offline either manually or automatically. However, having a fixed selection for long-term observation does not allow the system to react to changes in the neural activity, and may therefore lead to the loss of important information. In this paper, we formulate the process of autonomously selecting the best subset of electrodes as a combinatorial multi-armed bandit problem with non-stationary rewards, thus allowing the probe to adapt its selection policies online. In order to minimize exploratory actions of the probe, we furthermore take advantage of the existing dependencies between neighboring channels. Our approach is an adaptation of the discounted upper confidence bounds (D-UCB) algorithm, and identifies the electrodes providing the largest amount of non-redundant information. To the best of our knowledge, this is the first online approach for the problem of electrode selection. In extensive experiments, we demonstrate that our solution is not only able to converge towards an average optimal selection policy, but it is also able to react to changes in the neural activity or to damages of the recording electrodes.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct4_04">17:40-17:55, Paper TuCT4.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1342.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1342'); return false" title="Click to show or hide the keywords and abstract">A Probabilistic Approach Based on Random Forests to Estimating Similarity of Human Motion in the Context of Parkinson’s Disease</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196363" title="Click to go to the Author Index">Kuhner, Andreas</a></td><td class="r">Univ. Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#173321" title="Click to go to the Author Index">Schubert, Tobias</a></td><td class="r">AIS Univ. Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#153610" title="Click to go to the Author Index">Cenciarini, Massimo</a></td><td class="r">Univ. Medical Center Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#182977" title="Click to go to the Author Index">Maurer, Christoph</a></td><td class="r">Univ. of Freiburg Medical Center</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101785" title="Click to go to the Author Index">Burgard, Wolfram</a></td><td class="r">Univ. of Freiburg</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1342" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Robots_and_Systems" title="Click to go to the Keyword Index">Medical Robots and Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Motor_Skill_Learning" title="Click to go to the Keyword Index">Motor Skill Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Medical_Systems__Healthcare__and_Assisted_Living" title="Click to go to the Keyword Index">Medical Systems, Healthcare, and Assisted Living</a></span><br>
                           <strong>Abstract:</strong> The objective characterization of human motion is required in a variety of fields including competitive sports, rehabilitation and the detection of motor deficits. Nowadays, typically human experts evaluate the motor behavior. These evaluations are based on their individual experience which leads to a low inter- and intra-expert reliability. Standardized tests improve on the reliability but are still prone to subjective ratings and require human expert knowledge. This paper presents a novel method to characterize the motor state of Parkinson patients using full body motion capturing data based on a combination of multiple metrics. Our approach merges various metrics with a Random Forest and uses a probabilistic formulation to compute a one-dimensional measure for the performed motion. We present an application of our approach to the problem of relating subject motion to different classes like healthy subjects and Parkinson disease patients with deep brain stimulation switched on or off. In the experimental session we show that our measure leads to high classification rates and high entropy values for real-world data. Besides, we show that our method discriminates between Parkinson's subjects (with and without stimulation) and healthy persons as good as the Unified Parkinson's Disease Rating Scale (UPDRS).
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct4_05">17:55-18:10, Paper TuCT4.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1637.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1637'); return false" title="Click to show or hide the keywords and abstract">AutoHydrate: A Wearable Hydration Monitoring System</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196859" title="Click to go to the Author Index">Mengistu, Yehenew</a></td><td class="r">Oklahoma State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#184555" title="Click to go to the Author Index">Pham, Minh</a></td><td class="r">Oklahoma State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160370" title="Click to go to the Author Index">Do, Ha Manh</a></td><td class="r">Oklahoma State Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100881" title="Click to go to the Author Index">Sheng, Weihua</a></td><td class="r">Oklahoma State Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1637" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Medical_Systems__Healthcare__and_Assisted_Living" title="Click to go to the Keyword Index">Medical Systems, Healthcare, and Assisted Living</a>, <a href="IROS16_KeywordIndexMedia.html#Human_Detection_and_Tracking" title="Click to go to the Keyword Index">Human Detection and Tracking</a></span><br>
                           <strong>Abstract:</strong> Water is a highly abundant nutrient in the human body and monitoring of its regulation is essential to keep the body hydrated. A number of critical health conditions including swelling of the brain and short/long term memory loss are associated with poor or excessive drinking habits. This can be prevented with the use of a real time hydration monitoring system. In this paper we presented AutoHydrate, a wearable hydration monitoring system which continuously monitors the drinking activities and daily fluid requirements of the user through automatic detection of drinking and body activities. The system is built using a throat microphone for collecting acoustic signals, a smartwatch for collecting body activity, an embedded computer for processing the signals and sending recommendation to a smartphone app in real time for an interactive information display. After different time, frequency and cepstral domain features are extracted from the signals, drinking activities are classified using Support Vector Machine (SVM) and body activity is classified using Gradient Boosting Decision Tree algorithm. The Dietary Reference Intake standard is followed for recommending the amount of fluid required using our detection. Based on our experimental results on 8 subjects, a Drinking detection accuracy of 91.5% and Body activity classification accuracy of 89.12% are obtained. Results show that our system is feasible for real time monitoring of body hydration.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuct5"><b>TuCT5</b></a></td>
               <td class="r">#105</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuct5" title="Click to go to the Program at a Glance"><b>Multiple Robot Path Planning</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#102604" title="Click to go to the Author Index">Dudek, Gregory</a></td><td class="r">McGill Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#103311" title="Click to go to the Author Index">Lodi Rizzini, Dario</a></td><td class="r">Univ. of Parma</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct5_01">16:55-17:10, Paper TuCT5.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0245.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('245'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Occlusion-Aware Multi-Robot 3D Tracking</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#154015" title="Click to go to the Author Index">Hausman, Karol</a></td><td class="r">Univ. of Southern California</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#170054" title="Click to go to the Author Index">Kahn, Gregory</a></td><td class="r">Univ. of California, Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#123971" title="Click to go to the Author Index">Patil, Sachin</a></td><td class="r">Univ. of California Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#115624" title="Click to go to the Author Index">Mueller, Joerg</a></td><td class="r">Robert Bosch LLC</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107102" title="Click to go to the Author Index">Goldberg, Ken</a></td><td class="r">UC Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107568" title="Click to go to the Author Index">Abbeel, Pieter</a></td><td class="r">UC Berkeley</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101856" title="Click to go to the Author Index">Sukhatme, Gaurav</a></td><td class="r">Univ. of Southern California</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab245" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0245.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a>, <a href="IROS16_KeywordIndexMedia.html#Path_Planning_for_Multiple_Mobile_Robots_or_Agents" title="Click to go to the Keyword Index">Path Planning for Multiple Mobile Robots or Agents</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a></span><br>
                           <strong>Abstract:</strong> We introduce an optimization-based control approach that enables a team of robots to cooperatively track a target using onboard sensing. In this setting, the robots are required to estimate their own positions as well as concurrently track the target. Our probabilistic method generates controls that minimize the expected uncertainty of the target. Additionally, our method efficiently reasons about occlusions between robots and takes them into account for the control generation. We evaluate our approach in a number of experiments in which we simulate a team of quadrotor robots flying in three-dimensional space to track a moving target on the ground. We compare our method to other state-of-the-art approaches represented by the random sampling technique, lattice planning method, and our previous method. Our experimental results indicate that our method achieves up to 8 times smaller maximum tracking error and up to 2 times smaller average tracking error than the next best approach in the presented scenarios.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct5_02">17:10-17:25, Paper TuCT5.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0422.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('422'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Dynamic Multi-Target Coverage with Robotic Cameras</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183179" title="Click to go to the Author Index">Hoenig, Wolfgang</a></td><td class="r">Univ. of Southern California</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#113756" title="Click to go to the Author Index">Ayanian, Nora</a></td><td class="r">Univ. of Southern California</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab422" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0422.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Path_Planning_for_Multiple_Mobile_Robots_or_Agents" title="Click to go to the Keyword Index">Path Planning for Multiple Mobile Robots or Agents</a>, <a href="IROS16_KeywordIndexMedia.html#Multi_Robot_Systems" title="Click to go to the Keyword Index">Multi-Robot Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Cooperating_Robots" title="Click to go to the Keyword Index">Cooperating Robots</a></span><br>
                           <strong>Abstract:</strong> When tracking multiple targets with autonomous cameras for 3D scene reconstruction, e.g., in sports, a significant challenge is handling the unpredictable nature of the targets' motion. Such a monitoring system must reposition according to the targets' movements and maintain satisfactory coverage of the targets. We propose an approximate, centralized approach for maximizing the visible boundary of dynamic targets using mobile cameras in a bounded 2D environment. Targets and obstacles translate, rotate, and deform independently, and cameras are only aware of the current position and shape of the targets and obstacles. Using current information, the environment is searched for better viewing positions, then cameras navigate to those positions while avoiding collisions with targets and obstacles. We present a benchmark and metrics to evaluate the performance of our method, and compare our approach to a simple gradient-based local method in several real-time simulations.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct5_03">17:25-17:40, Paper TuCT5.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0761.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('761'); return false" title="Click to show or hide the keywords and abstract">Multi-Robot Search for a Moving Target: Integrating World Modeling, Task Assignment and Context</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#183108" title="Click to go to the Author Index">Riccio, Francesco</a></td><td class="r">Sapienza Univ. of Rome</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196248" title="Click to go to the Author Index">Borzi, Emanuele</a></td><td class="r">Sapienza Univ. of Rome</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#170408" title="Click to go to the Author Index">Gemignani, Guglielmo</a></td><td class="r">Sapienza Univ. Rome</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#137880" title="Click to go to the Author Index">Nardi, Daniele</a></td><td class="r">Sapienza Univ. of Rome</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab761" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Multi_Robot_Systems" title="Click to go to the Keyword Index">Multi-Robot Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Distributed_Robot_Systems" title="Click to go to the Keyword Index">Distributed Robot Systems</a></span><br>
                           <strong>Abstract:</strong> In this paper, we address coordination within a team of cooperative autonomous robots that need to accomplish a common goal. Our survey of the vast literature on the subject highlights two directions to further improve the performance of a multi-robot team. In particular, in a dynamic environment, coordination needs to be adapted to the different situations at hand (for example, when there is a dramatic loss of performance due to unreliable communication network). To this end, we contribute a novel approach for coordinating robots. Such an approach allows a robotic team to exploit environmental knowledge to adapt to various circumstances encountered, enhancing its overall performance. This result is achieved by dynamically adapting the underlying task assignment and distributed world representation, based on the current state of the environment. We demonstrate the effectiveness of our coordination system by applying it to the problem of locating a moving, non-adversarial target. In particular, we report on experiments carried out with a team of humanoid robots in a soccer scenario and a team of mobile bases in an office environment.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct5_04">17:40-17:55, Paper TuCT5.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0811.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('811'); return false" title="Click to show or hide the keywords and abstract">Fast and Efficient Rendezvous in Street Networks</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#133857" title="Click to go to the Author Index">Meghjani, Malika</a></td><td class="r">McGill Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#166102" title="Click to go to the Author Index">Manjanna, Sandeep</a></td><td class="r">McGill Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102604" title="Click to go to the Author Index">Dudek, Gregory</a></td><td class="r">McGill Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab811" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Path_Planning_for_Multiple_Mobile_Robots_or_Agents" title="Click to go to the Keyword Index">Path Planning for Multiple Mobile Robots or Agents</a>, <a href="IROS16_KeywordIndexMedia.html#Planning__Scheduling_and_Coordination" title="Click to go to the Keyword Index">Planning, Scheduling and Coordination</a>, <a href="IROS16_KeywordIndexMedia.html#Cooperating_Robots" title="Click to go to the Keyword Index">Cooperating Robots</a></span><br>
                           <strong>Abstract:</strong> We address the problem of rendezvous between two agents in urban street networks. Specifically, we consider the case where the agents have variable speeds and they need to schedule a rendezvous or a meeting under uncertainty in their travel times. Examples of such a scenario range from everyday life where two people would like to coordinate a meeting while going from office to home; to a futuristic case where automated taxis would like to meet each other for load balancing passengers. The scheduling for such scenarios can easily become challenging with uncertainties such as delayed departures, road blocks due to construction or traffic congestion. Any solution for such a task is required to minimize the waiting time and the planning overhead. In this paper, we propose an algorithm that optimizes the total travel time and the waiting time for two agents to complete their respective paths from start to rendezvous and from rendezvous to goal locations subject to delays along their paths. We validate our approach with a street network database which has a cost associated with every query made to the database server. Thus our algorithm intelligently optimizes for rendezvous trajectories that effectively mitigate the scourge of traffic delays, while simultaneously limiting the number of queries through careful analysis of the informative value of each potential query.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct5_05">17:55-18:10, Paper TuCT5.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1466.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1466'); return false" title="Click to show or hide the keywords and abstract">Safe and Complete Trajectory Generation for Robot Teams with Higher-Order Dynamics</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#160582" title="Click to go to the Author Index">Tang, Sarah</a></td><td class="r">Univ. of Pennsylvania</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104342" title="Click to go to the Author Index">Kumar, Vijay</a></td><td class="r">Univ. of Pennsylvania</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1466" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Path_Planning" title="Click to go to the Keyword Index">Motion and Path Planning</a>, <a href="IROS16_KeywordIndexMedia.html#Path_Planning_for_Multiple_Mobile_Robots_or_Agents" title="Click to go to the Keyword Index">Path Planning for Multiple Mobile Robots or Agents</a></span><br>
                           <strong>Abstract:</strong> In this work, we consider the labeled multi-robot planning problem. In this paradigm, a team of robots at fixed start positions must navigate to pre-specified and non-interchangable goal positions. While many algorithms have been proposed for finding optimal solutions to this problem, most methods assume that the robots are kinematic agents, whereas in reality, robots often have high-order dynamics that must be respected by their trajectories. Here, we propose a centralized method for generating trajectories for teams of robots with general nth-order dynamics navigating to labeled goals. Our algorithm is safe and complete and additionally allows for decoupled optimization of each robot's trajectory as a Quadratic Program with linear constraints. We present simulation results for teams of up to 20 robots.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuct6"><b>TuCT6</b></a></td>
               <td class="r">#106</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuct6" title="Click to go to the Program at a Glance"><b>Localization</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#102606" title="Click to go to the Author Index">Rekleitis, Ioannis</a></td><td class="r">Univ. of South Carolina</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#103133" title="Click to go to the Author Index">Beetz, Michael</a></td><td class="r">Univ. of Bremen</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct6_01">16:55-17:10, Paper TuCT6.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0115.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('115'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Active Localization with Dynamic Obstacles</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#156679" title="Click to go to the Author Index">Quattrini Li, Alberto</a></td><td class="r">Univ. of South Carolina</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192519" title="Click to go to the Author Index">Xanthidis, Marios</a></td><td class="r">Univ. of South Carolina</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104313" title="Click to go to the Author Index">O'Kane, Jason</a></td><td class="r">Univ. of South Carolina</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102606" title="Click to go to the Author Index">Rekleitis, Ioannis</a></td><td class="r">Univ. of South Carolina</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab115" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0115.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a></span><br>
                           <strong>Abstract:</strong> This paper addresses the problem of robot global localization in a known environment, in the presence of many dynamic obstacles. Deploying a robot in crowded spaces such as museums, shopping malls, department stores, or university campuses is especially challenging because the moving people occlude the static parts of the environment, such as walls and doorways, making the robot essentially blind. A new weighting function is proposed for a particle filter state estimation algorithm that accounts for the presence of dynamic obstacles and avoids population depletion. An active localization strategy is employed which guides the robot to locations that resolve ambiguities and eliminate hypotheses in a systematic manner. Experimental results from multiple simulations and from real robot deployments validate the localization improvements achieved by the proposed method.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct6_02">17:10-17:25, Paper TuCT6.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0463.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('463'); return false" title="Click to show or hide the keywords and abstract">Keyframe Based Large-Scale Indoor Localisation Using Geomagnetic Field and Motion Pattern</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#159943" title="Click to go to the Author Index">Wang, Sen</a></td><td class="r">Univ. of Oxford</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195733" title="Click to go to the Author Index">Wen, Hongkai</a></td><td class="r">Univ. of Oxford</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195761" title="Click to go to the Author Index">Clark, Ronald</a></td><td class="r">Univ. of Oxford</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104367" title="Click to go to the Author Index">Trigoni, Niki</a></td><td class="r">Univ. of Oxford</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab463" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a></span><br>
                           <strong>Abstract:</strong> This paper studies indoor localisation problem by using low-cost and pervasive sensors. Most of existing indoor localisation algorithms rely on camera, laser scanner, floor plan or other pre-installed infrastructure to achieve sub-meter or sub-centimetre localisation accuracy. However, in some circumstances these required devices or information may be unavailable or too expensive in terms of cost or deployment. This paper presents a novel keyframe based Pose Graph Simultaneous Localisation and Mapping (SLAM) method, which correlates ambient geomagnetic field with motion pattern and employs low-cost sensors commonly equipped in mobile devices, to provide positioning in both unknown and known environments. Extensive experiments are conducted in large-scale indoor environments to verify that the proposed method can achieve high localisation accuracy similar to state-of-the-arts, such as vision based Google Project Tango.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct6_03">17:25-17:40, Paper TuCT6.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1221.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1221'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Bridging the Appearance Gap: Multi-Experience Localization for Long-Term Visual Teach & Repeat</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#173522" title="Click to go to the Author Index">Paton, Michael</a></td><td class="r">Univ. of Toronto</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#176318" title="Click to go to the Author Index">MacTavish, Kirk Andrew</a></td><td class="r">Univ. of Toronto</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#142721" title="Click to go to the Author Index">Warren, Michael</a></td><td class="r">Univ. of Toronto</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#115139" title="Click to go to the Author Index">Barfoot, Timothy</a></td><td class="r">Univ. of Toronto</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1221" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1221.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a></span><br>
                           <strong>Abstract:</strong> Vision-based, route-following algorithms enable autonomous robots to repeat manually taught paths over long distances using inexpensive vision sensors. However, these methods struggle with long-term, outdoor operation due to the challenges of environmental appearance change caused by lighting, weather, and seasons. While techniques exist to address appearance change by using multiple experiences over different environmental conditions, they either provide topological-only localization, require several manually taught experiences in different conditions, or require extensive offline mapping to produce metric localization. For real-world use, we would like to localize metrically to a single manually taught route and gather additional visual experiences during autonomous operations. Accordingly, we propose a novel multi-experience localization (MEL) algorithm developed specifically for route-following applications; it provides continuous, six-degree-of-freedom (6dof) localization with relative uncertainty to a privileged (manually taught) path using several experiences simultaneously. We validate our algorithm through two experiments: i) an offline performance analysis on a 9km subset of a challenging 27km route-traversal dataset and ii) an online field trial where we demonstrate autonomy on a small 250m loop over the course of a sunny day. Both exhibit significant appearance change due to lighting variation. Through these experiments we show that safe localization can be achieved by bridging the appearance gap.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct6_04">17:40-17:55, Paper TuCT6.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1246.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1246'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Monocular Camera Localization in 3D LiDAR Maps</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#186166" title="Click to go to the Author Index">Caselitz, Tim</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110224" title="Click to go to the Author Index">Steder, Bastian</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#122257" title="Click to go to the Author Index">Ruhnke, Michael</a></td><td class="r">Univ. of Freiburg</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101785" title="Click to go to the Author Index">Burgard, Wolfram</a></td><td class="r">Univ. of Freiburg</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1246" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1246.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Navigation" title="Click to go to the Keyword Index">Visual Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a></span><br>
                           <strong>Abstract:</strong> Localizing a camera in a given map is essential for vision-based navigation. In contrast to common methods for visual localization that use maps acquired with cameras, we propose a novel approach, which tracks the pose of monocular camera with respect to a given 3D LiDAR map. We employ a visual odometry system based on local bundle adjustment to reconstruct a sparse set of 3D points from image features. These points are continuously matched against the map to track the camera pose in an online fashion. Our approach to visual localization has several advantages. Since it only relies on matching geometry, it is robust to changes in the photometric appearance of the environment. Utilizing panoramic LiDAR maps additionally provides viewpoint invariance. Yet low-cost and lightweight camera sensors are used for tracking. We present real-world experiments demonstrating that our method accurately estimates the 6-DoF camera pose over long trajectories and under varying conditions.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct6_05">17:55-18:10, Paper TuCT6.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1337.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1337'); return false" title="Click to show or hide the keywords and abstract">FLAT2D: Fast Localization from Approximate Transformation into 2D</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#147572" title="Click to go to the Author Index">Goeddel, Robert</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191711" title="Click to go to the Author Index">Kershaw, Carl</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171438" title="Click to go to the Author Index">Serafin, Jacopo</a></td><td class="r">Univ. Sapienza of Rome</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107201" title="Click to go to the Author Index">Olson, Edwin</a></td><td class="r">Univ. of Michigan</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1337" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a></span><br>
                           <strong>Abstract:</strong> In this paper, we propose a fast method for localization based on 3D structure around the vehicle using a 2D representation. This representation retains many of the advantages of &quot;full&quot; matching in 3D, but comes with dramatically lower space and computational requirements. We also introduce a variation of Graph-SLAM tailored to support localization, allowing us to make use of max mixture representations in our localization estimate. Finally, we present real-world localization results for both an indoor mobile robotic platform and an autonomous golf cart, demonstrating that autonomous vehicles do not need full 3D matching to accurately localize in the environment.<p>Many autonomous vehicles require precise localization into a prior map in order to support planning and to leverage semantic information within those maps (e.g. that the right lane is a turn-only lane.) A popular approach in automotive systems is to use infrared intensity maps of the ground surface to localize, making them susceptible to failures when the surface is obscured by snow or when the road is repainted. An emerging alternative is to localize based on the 3D structure around the vehicle; these methods are robust to these types of changes, but the maps are costly both in terms of storage and the computational cost of matching.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuct7"><b>TuCT7</b></a></td>
               <td class="r">#107</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuct7" title="Click to go to the Program at a Glance"><b>Grasping</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#101767" title="Click to go to the Author Index">Brock, Oliver</a></td><td class="r">Tech. Univ. Berlin</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#102004" title="Click to go to the Author Index">Schaal, Stefan</a></td><td class="r">MPI Intelligent Systems & Univ. of Southern California</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct7_01">16:55-17:10, Paper TuCT7.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0813.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('813'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Synergy-Based Policy Improvement with Path Integrals for Anthropomorphic Hands</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#137686" title="Click to go to the Author Index">Ficuciello, Fanny</a></td><td class="r">Univ. Di Napoli Federico II</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196286" title="Click to go to the Author Index">Zaccara, Damiano</a></td><td class="r">Univ. Di Napoli Federico II</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#100153" title="Click to go to the Author Index">Siciliano, Bruno</a></td><td class="r">Univ. Napoli Federico II</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab813" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0813.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Reinforcement_Learning" title="Click to go to the Keyword Index">Robot Reinforcement Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Multifingered_Hands" title="Click to go to the Keyword Index">Multifingered Hands</a></span><br>
                           <strong>Abstract:</strong> In this work, a synergy-based reinforcement learning algorithm has been developed to confer autonomous grasping capabilities to anthropomorphic hands. In the presence of high degrees of freedom, classical machine learning techniques require a number of iterations that increases with the size of the problem, thus convergence of the solution is not ensured. The use of postural synergies determines dimensionality reduction of the search space and allows recent learning techniques, such as Policy Improvement with Path Integrals (PI2), becoming easily applicable. A key point is the adoption of a suitable reward function representing the goal of the task and ensuring onestep performance evaluation. Force closure quality of the grasp in the synergies subspace has been chosen as a cost function for performance evaluation. The experiments conducted on the SCHUNK 5-Finger Hand demonstrate the effectiveness of the algorithm showing skills comparable to human capabilities in learning new grasps and in performing wide variety from power to high precision grasps of very small objects.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct7_02">17:10-17:25, Paper TuCT7.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0952.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('952'); return false" title="Click to show or hide the keywords and abstract">Grasp Quality Evaluation in Underactuated Robotic Hands</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#192164" title="Click to go to the Author Index">Pozzi, Maria</a></td><td class="r">Univ. of Siena</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195153" title="Click to go to the Author Index">Sundaram, Ashok M.</a></td><td class="r">German Aerospace Center (DLR)</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#149511" title="Click to go to the Author Index">Malvezzi, Monica</a></td><td class="r">Univ. of Siena</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#105260" title="Click to go to the Author Index">Prattichizzo, Domenico</a></td><td class="r">Univ. of Siena</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104075" title="Click to go to the Author Index">Roa, Maximo A.</a></td><td class="r">German Aerospace Center, DLR</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab952" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Multifingered_Hands" title="Click to go to the Keyword Index">Multifingered Hands</a></span><br>
                           <strong>Abstract:</strong> Underactuated and synergy-driven hands are gaining attention in the grasping community mainly due to their simple kinematics, intrinsic compliance and versatility for grasping objects even in non structured scenarios. The evaluation of the grasping capabilities of such hands is a challenging task. This paper revisits some traditional quality measures developed for multi-fingered, fully actuated hands, and applies them to the case of underactuated hands. The extension of quality metrics for synergy-driven hands for the case of underactuated grasping is also presented. The performance of both types of measures is evaluated with simulated examples, concluding with a comparative discussion of their main features.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct7_03">17:25-17:40, Paper TuCT7.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0967.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('967'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>A Compact Representation of Human Single-Object Grasping</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178758" title="Click to go to the Author Index">Puhlmann, Steffen</a></td><td class="r">TU Berlin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178756" title="Click to go to the Author Index">Heinemann, Fabian</a></td><td class="r">TU Berlin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101767" title="Click to go to the Author Index">Brock, Oliver</a></td><td class="r">Tech. Univ. Berlin</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178754" title="Click to go to the Author Index">Maertens, Marianne</a></td><td class="r">Tech. Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab967" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0967.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Planning__Scheduling_and_Coordination" title="Click to go to the Keyword Index">Planning, Scheduling and Coordination</a>, <a href="IROS16_KeywordIndexMedia.html#Dexterous_Manipulation" title="Click to go to the Keyword Index">Dexterous Manipulation</a></span><br>
                           <strong>Abstract:</strong> Observations of human grasping reveal that the exploitation of environmental constraints is a key structural aspect for the robustness and versatility of human grasping behavior. We analyze 3,400 human grasping trials with 17 subjects grasping 25 objects to show that viewing environmental constraints as the central structural aspect of human grasping yields surprisingly simple representations of human grasping behavior. We present hypothesis-driven experiments that emphasize the centrality of environmental constraints in human grasping and extract from data a simple &quot;grasping plan&quot; that is a generative model for all of the human grasping trials we observed. This grasping plan can in principle be transferred to a robot system in an attempt to leverage environmental constraints to improve the performance of robotic grasping.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct7_04">17:40-17:55, Paper TuCT7.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0975.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('975'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Self-Supervised Regrasping Using Spatio-Temporal Tactile Features and Reinforcement Learning</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#171371" title="Click to go to the Author Index">Chebotar, Yevgen</a></td><td class="r">Univ. of Southern California</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#154015" title="Click to go to the Author Index">Hausman, Karol</a></td><td class="r">Univ. of Southern California</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140358" title="Click to go to the Author Index">Su, Zhe</a></td><td class="r">Univ. of Southern California</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101856" title="Click to go to the Author Index">Sukhatme, Gaurav</a></td><td class="r">Univ. of Southern California</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#102004" title="Click to go to the Author Index">Schaal, Stefan</a></td><td class="r">MPI Intelligent Systems & Univ. of Southern California</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab975" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0975.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Reinforcement_Learning" title="Click to go to the Keyword Index">Robot Reinforcement Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Tactile_Sensing" title="Click to go to the Keyword Index">Tactile Sensing</a></span><br>
                           <strong>Abstract:</strong> We introduce a framework for learning regrasping behaviors based on tactile data. First, we present a grasp stability predictor that uses spatio-temporal tactile features collected from the early-object-lifting phase to predict the grasp outcome with a high accuracy. Next, the trained predictor is used to supervise and provide feedback to a reinforcement learning algorithm that learns the required grasp adjustments based on tactile feedback. Our results gathered over more than 50 hours of real robot experiments indicate that the robot is able to predict the grasp outcome with 93% accuracy. In addition, the robot is able to improve the grasp success rate from 42% when randomly grasping an object to up to 97% when allowed to regrasp the object in case of a predicted failure.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct7_05">17:55-18:10, Paper TuCT7.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1148.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1148'); return false" title="Click to show or hide the keywords and abstract">Unscented Bayesian Optimization for Safe Robot Grasping</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196544" title="Click to go to the Author Index">Nogueira, Jose</a></td><td class="r">Inst. Superior Técnico</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104315" title="Click to go to the Author Index">Martinez-Cantin, Ruben</a></td><td class="r">Centro Univ. De La Defensa</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110801" title="Click to go to the Author Index">Bernardino, Alexandre</a></td><td class="r">IST - Técnico Lisboa</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#108793" title="Click to go to the Author Index">Jamone, Lorenzo</a></td><td class="r">Inst. Superior Tecnico</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1148" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Learning" title="Click to go to the Keyword Index">Robot Learning</a>, <a href="IROS16_KeywordIndexMedia.html#Grasping" title="Click to go to the Keyword Index">Grasping</a></span><br>
                           <strong>Abstract:</strong> Safe and robust grasping of unknown objects is a major challenge in robotics, which has no general solution yet. A promising approach relies on haptic exploration, where active optimization strategies can be employed to reduce the number of exploration trials. One critical problem is that certain optimal grasps discoverd by the optimization procedure may be very sensitive to small deviations of the parameters from their nominal values: we call these unsafe grasps because small errors during motor execution may turn optimal grasps into bad grasps. To reduce the risk of grasp failure, safe grasps should be favoured. Therefore, we propose a new algorithm, Unscented Bayesian Optimization, that performs efficient optimization while considering uncertainty in the input space, leading to the discovery of safe optima. The results highlight how our method outperforms the classical Bayesian Optimization both in synthetic problems and in realistic robot grasp simulations, finding robust and safe grasps after a few exploration trials.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuct8"><b>TuCT8</b></a></td>
               <td class="r">#108</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuct8" title="Click to go to the Program at a Glance"><b>Robot Audition</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#107173" title="Click to go to the Author Index">Nakadai, Kazuhiro</a></td><td class="r">Honda Res. Inst. Japan Co., Ltd</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#180148" title="Click to go to the Author Index">Bertin, Nancy</a></td><td class="r">Cnrs, Irisa</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct8_01">16:55-17:10, Paper TuCT8.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0008.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('8'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Online Simultaneous Localization and Mapping of Multiple Sound Sources and Asynchronous Microphone Arrays</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#182104" title="Click to go to the Author Index">Sekiguchi, Kouhei</a></td><td class="r">Kyoto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#165708" title="Click to go to the Author Index">Bando, Yoshiaki</a></td><td class="r">Kyoto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#128040" title="Click to go to the Author Index">Nakamura, Keisuke</a></td><td class="r">Honda Res. Inst. Japan Co., Ltd</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107173" title="Click to go to the Author Index">Nakadai, Kazuhiro</a></td><td class="r">Honda Res. Inst. Japan Co., Ltd</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#164754" title="Click to go to the Author Index">Itoyama, Katsutoshi</a></td><td class="r">Kyoto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110308" title="Click to go to the Author Index">Yoshii, Kazuyoshi</a></td><td class="r">Kyoto Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab8" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0008.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Multi_Robot_Systems" title="Click to go to the Keyword Index">Multi-Robot Systems</a></span><br>
                           <strong>Abstract:</strong> This paper presents an online method of simultaneous localization and mapping (SLAM) for estimating the positions of multiple moving sound sources and stationary robots and synchronizing microphone arrays attached to those robots. Since each robot with a microphone array can solely estimate the directions of sound sources, the 2-dimensional source positions can be estimated from the source directions estimated by multiple robots using a triangulation method. In addition, sound mixtures can be separated accurately by regarding distributed microphone arrays as one big array. To perform these tasks, some methods have been proposed for localizing and synchronizing microphone arrays. These methods, however, can be used only if a single sound source exists because the time differences of arrival (TDOAs) between microphones are assumed to be directly observed. To overcome this limitation, we propose a unified state-space model that encodes the source and robot positions and the time offsets between microphone arrays in a latent space. Given the TDOAs and directions of arrival (DOAs) estimated by separating observed mixture sounds into source sounds, the latent variables are estimated jointly in an online manner using a FastSLAM2.0 algorithm that can deal with an unknown time-varying number of moving sound sources.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct8_02">17:10-17:25, Paper TuCT8.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0177.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('177'); return false" title="Click to show or hide the keywords and abstract">Position Estimation of Sound Source on Ground by Multirotor Helicopter with Microphone Array</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195444" title="Click to go to the Author Index">Washizaki, Kai</a></td><td class="r">Kumamoto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195445" title="Click to go to the Author Index">Wakabayashi, Mizuho</a></td><td class="r">Kumamoto Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#110645" title="Click to go to the Author Index">Kumon, Makoto</a></td><td class="r">Graduate School of Science and Tech. Kumamoto</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab177" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a>, <a href="IROS16_KeywordIndexMedia.html#Unmanned_Aerial_Systems" title="Click to go to the Keyword Index">Unmanned Aerial Systems</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a></span><br>
                           <strong>Abstract:</strong> Multirotor helicopters are expected to be utilized various tasks including rescue missions and surveillance. For those missions, sensors are equipped with helicopters in order to recognize the environment, and auditory information is one of such information that can be utilized to find the target sound source even if it is occluded by objects. One of the difficulty comes from the fact that the noise generated by rotating rotors distorts the target signal significantly, which leads inaccurate estimate of the target source direction. Besides, the estimate of the attitude and the position of the helicopter is inaccurate, which is another technical issue. This paper proposes a robust sound source position estimation taking such uncertainty into account, and experiment with real flight tests showed that the helicopter was able to estimate the target position within about 2m accuracy.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct8_03">17:25-17:40, Paper TuCT8.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0849.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('849'); return false" title="Click to show or hide the keywords and abstract">Localizing an Intermittent and Moving Sound Source Using a Mobile Robot</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#196309" title="Click to go to the Author Index">Nguyen, Van Quan</a></td><td class="r">INRIA</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#124104" title="Click to go to the Author Index">Colas, Francis</a></td><td class="r">Inria Nancy Grand Est</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#156413" title="Click to go to the Author Index">Vincent, Emmanuel</a></td><td class="r">Inria</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#107554" title="Click to go to the Author Index">Charpillet, Francois</a></td><td class="r">INRIA, Loria</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab849" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a>, <a href="IROS16_KeywordIndexMedia.html#Localization" title="Click to go to the Keyword Index">Localization</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a></span><br>
                           <strong>Abstract:</strong> This paper addresses the problem of localizing and tracking one intermittent, moving sound source using a microphone array on a mobile robot. Robot motion provides a solution for estimating the distance to the source and avoiding front-back ambiguity. We propose a mixture Kalman filter (MKF) framework in order to fuse the robot motion information and the measurements taken at different poses of the robot. Experiments and statistical results demonstrate the ability of the proposed method to track one intermittent sound source in a reverberant environment where false measurements of the source angle of arrival (AoA) and the source activity often occur compared to a method that does not consider tracking source activity into account.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct8_04">17:40-17:55, Paper TuCT8.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0932.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('932'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Audio-Based Robot Control from Interchannel Level Difference and Absolute Sound Energy</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#178719" title="Click to go to the Author Index">Magassouba, Aly</a></td><td class="r">Univ. Rennes 1, Inria</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180148" title="Click to go to the Author Index">Bertin, Nancy</a></td><td class="r">Cnrs, Irisa</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101831" title="Click to go to the Author Index">Chaumette, Francois</a></td><td class="r">Inria Rennes-Bretagne Atlantique</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab932" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0932.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a>, <a href="IROS16_KeywordIndexMedia.html#Visual_Servoing" title="Click to go to the Keyword Index">Visual Servoing</a></span><br>
                           <strong>Abstract:</strong> This paper is a follow-up way to our previous works regarding audio-based control, that is an alternative method for auditory-based robot tasks. Conversely to classic methods oriented towards sound source localization, audio-based control is a sensor-based framework that does not localize the sound source. Instead, auditory features are used as inputs of a closed-loop control scheme. The audio-based control method presented in this paper relies on the sound signal energy measured by two microphones. By combining the interchannel level difference to the acoustic absolute energy level, the control scheme allows positioning the robot with respect to the sound source at a given distance and orientation. Moreover this method has the benefit of a low computation cost, since it only relies on the signal energy measurement. Experimental results conducted on a mobile robot validate the relevance and the robustness of this approach in dynamic and real world conditions.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct8_05">17:55-18:10, Paper TuCT8.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1363.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1363'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Split Conditional Independent Mapping for Sound Source Localisation with Inverse-Depth Parametrisation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#168487" title="Click to go to the Author Index">Su, Daobilige</a></td><td class="r">Univ. of Tech. Sydney</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103428" title="Click to go to the Author Index">Vidal-Calleja, Teresa A.</a></td><td class="r">Univ. of Tech. Sydney</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#106478" title="Click to go to the Author Index">Valls Miro, Jaime</a></td><td class="r">Univ. of Tech. Sydney</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1363" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1363.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Robot_Audition" title="Click to go to the Keyword Index">Robot Audition</a></span><br>
                           <strong>Abstract:</strong> In this paper, we propose a framework to map stationary sound sources while simultaneously localise a moving robot. Conventional methods for localisation and sound source mapping rely on a microphone array and either, 1) a proprioceptive sensor only (such as wheel odometry) or 2) an additional exteroceptive sensor (such as cameras or lasers) to get accurately the robot locations. Since odometry drifts over time and sound observations are bearing-only, sparse and extremely noisy, the former can only deal with relatively short trajectories before the whole map drifts. In comparison, the latter can get more accurate trajectory estimation over long distances and a better estimation of the sound source map as a result. However, in most of the work in the literature, trajectory estimation and sound source mapping are treated as uncorrelated, which means an update on the robot trajectory does not propagate properly to the sound source map. In this paper, we proposed an efficient method to correlate robot trajectory with sound source mapping by exploiting the conditional independence property between two maps estimated by two different Simultaneous Localisation and Mapping (SLAM) algorithms running in parallel. In our approach, the first map has the flexibility that can be built with any SLAM algorithm (filtering or optimisation) to estimate robot poses with an exteroceptive sensor. The second map is built by using a filtering-based SLAM algorithm locating all stationary sound sources parametrised with Inverse Depth Parametrisation (IDP). Robot locations used during IDP initialisation are the common features shared between the two SLAM maps, which allow to propagate information accordingly. Comprehensive simulations and experimental results show the effectiveness of the proposed method.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuct9"><b>TuCT9</b></a></td>
               <td class="r">#204~205</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuct9" title="Click to go to the Program at a Glance"><b>Marine Robots 2</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#118789" title="Click to go to the Author Index">Johnson-Roberson, Matthew</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#161725" title="Click to go to the Author Index">Duda, Alexander</a></td><td class="r">DFKI</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct9_01">16:55-17:10, Paper TuCT9.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0571.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('571'); return false" title="Click to show or hide the keywords and abstract">A Nonlinear Disturbance Observer Using Delayed Estimates -Its Application to Motion Control of an Underwater Vehicle-Manipulator System&#8213;</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195637" title="Click to go to the Author Index">Sugiyama, Noboru</a></td><td class="r">Tokyo Univ. of Marine Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103020" title="Click to go to the Author Index">Toda, Masayoshi</a></td><td class="r">Tokyo Univ. of Marine Science and Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab571" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a></span><br>
                           <strong>Abstract:</strong> This paper presents a control design method with a new nonlinear disturbance observer using delayed estimates formotioncontrolofanunderwatervehicle-manipulatorsystem (UVMS). In recent years, application of an underwater vehicle hasbeenexpectedinvarious&#64257;elds.Sincemodelingunknownhydrodynamic forces is dif&#64257;cult, nonlinear disturbance observers have been proposed in the aim of decreasing disturbances due to hydrodynamic forces. In this paper, we propose a new disturbance observer by combining a conventional disturbance observer and time delay control, and attempt to apply it to motion control of a UVMS. Further, an analysis of ultimate boundedness and numerical simulations are conducted to evaluate the proposed control system. The respective analysis and simulation results show that the proposed disturbance observer exhibits faster convergence and less estimation errors than the conventional one and hence the control system based on it can perform successful motion control.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct9_02">17:10-17:25, Paper TuCT9.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0934.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('934'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Towards Real-Time Underwater 3D Reconstruction with Plenoptic Cameras</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180427" title="Click to go to the Author Index">Skinner, Katherine A.</a></td><td class="r">Univ. of Michigan</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#118789" title="Click to go to the Author Index">Johnson-Roberson, Matthew</a></td><td class="r">Univ. of Michigan</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab934" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0934.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Mapping" title="Click to go to the Keyword Index">Mapping</a>, <a href="IROS16_KeywordIndexMedia.html#SLAM" title="Click to go to the Keyword Index">SLAM</a></span><br>
                           <strong>Abstract:</strong> Achieving real-time perception is critical to developing a fully autonomous system that can sense, navigate, and interact with its environment. Perception tasks such as online 3D reconstruction and mapping have been intensely studied for terrestrial robotics applications. However, characteristics of the underwater domain such as light attenuation and light scattering violate the brightness constancy constraint, which is an underlying assumption in methods developed for land-based applications. Furthermore, the complex nature of light propagation underwater limits or even prevents subsea use of real-time depth sensors used in state-of-the-art terrestrial mapping techniques. There have been recent advances in the development of plenoptic (also known as light field) cameras, which use an array of micro lenses capturing both intensity and ray direction to enable color and depth measurement from a single passive sensor. This paper presents an end-to-end system to harness these cameras to produce real-time 3D reconstructions underwater. Our system builds upon the state-of-the-art in online terrestrial 3D reconstruction, transferring these approaches to the underwater domain by gathering real-time color and depth (RGB-D) data underwater using a plenoptic camera, and performing dense 3D reconstruction while compensating for attenuation effects of the underwater environment simultaneously, using a graphics processing unit (GPU) to achieve real-time performance. Results are presented for data gathered in a water tank and the proposed technique is validated quantitatively through comparison with a ground truth 3D model gathered in air to demonstrate that the proposed approach can generate accurate 3D models of objects underwater in real-time.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct9_03">17:25-17:40, Paper TuCT9.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0981.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('981'); return false" title="Click to show or hide the keywords and abstract">Refractive Forward Projection for Underwater Flat Port Cameras</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#161725" title="Click to go to the Author Index">Duda, Alexander</a></td><td class="r">DFKI</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#156125" title="Click to go to the Author Index">Gaudig, Christopher</a></td><td class="r">DFKI (German Res. Center for Artificial Intelligence)</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab981" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Robot_Vision" title="Click to go to the Keyword Index">Robot Vision</a></span><br>
                           <strong>Abstract:</strong> Geometric distortion associated with underwater flat port housings considerably affects the accuracy of scene depth and visual pose estimation. To compensate for this, a new solution for the forward projection is proposed based on the refractive camera model and Taylor expansion. This allows for an easy integration of the flat refractive camera model into non-linear optimization problems like bundle adjustment.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct9_04">17:40-17:55, Paper TuCT9.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1040.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1040'); return false" title="Click to show or hide the keywords and abstract">A Preliminary Survey of Underwater Robotic Vehicle Design and Navigation for Under-Ice Operations</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195168" title="Click to go to the Author Index">Barker, Laughlin David Laird</a></td><td class="r">Johns Hopkins Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101958" title="Click to go to the Author Index">Whitcomb, Louis</a></td><td class="r">The Johns Hopkins Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1040" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Navigation" title="Click to go to the Keyword Index">Navigation</a>, <a href="IROS16_KeywordIndexMedia.html#Field_Robots" title="Click to go to the Keyword Index">Field Robots</a></span><br>
                           <strong>Abstract:</strong> This paper reviews the motivation, development, and use of underwater robotic vehicles designed for use in ice-covered waters, with special attention paid to the navigation systems employed for under-ice deployments. The scientific need for routine access under fixed and moving ice by underwater robotic vehicles is briefly reviewed. The challenges of under-ice vehicle navigation are summarized. The paper then reviews all known under-ice robotic vehicles and their associated navigation systems, categorizing them by vehicle type (tethered, untethered, hybrid, and glider) and by the type of ice they were designed for (fixed land-fast sea ice and moving sea ice).
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct9_05">17:55-18:10, Paper TuCT9.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1130.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1130'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>An Underwater Laser Vision System for Relative 3-D Posture Estimation to Mesh-Like Targets</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180995" title="Click to go to the Author Index">Constantinou, Christos</a></td><td class="r">Cyprus Univ. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103500" title="Click to go to the Author Index">Loizou, Savvas</a></td><td class="r">Cyprus Univ. of Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#170462" title="Click to go to the Author Index">Georgiades, George</a></td><td class="r">Cyprus Univ. of Tech</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1130" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1130.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Marine_Robotics" title="Click to go to the Keyword Index">Marine Robotics</a>, <a href="IROS16_KeywordIndexMedia.html#Range_Sensing" title="Click to go to the Keyword Index">Range Sensing</a>, <a href="IROS16_KeywordIndexMedia.html#Calibration_and_Identification" title="Click to go to the Keyword Index">Calibration and Identification</a></span><br>
                           <strong>Abstract:</strong> In this paper we described the development of both the hardware and the algorithms for a novel laser vision system suitable for measuring distances from both solid and mesh-like targets in underwater environments. The system was developed as a part of the AQUABOT project that developed an underwater robotic system for autonomous inspection of offshore aquaculture installation. The system takes into account the hemispherical optics typical in underwater vehicle designs and implements an array of line-lasers to ensure that mesh-like targets provide reflections in a consistent manner. The developed algorithms for the laser vision system are capable of providing either raw pointcloud data sets from each laser or with additional processing high level information like distance and relative orientation of the target with respect to the ROV can be recovered. An automatic calibration procedure along with the accompanying hardware that was developed, is described in this paper, to reduce the calibration overhead required by regular maintenance operations as is typical for underwater vehicles operating in sea-water. A set of experimental results in controlled laboratory environment as well as at offshore aquaculture installations demonstrate the performance of the system.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuct10"><b>TuCT10</b></a></td>
               <td class="r">#206~208</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuct10" title="Click to go to the Program at a Glance"><b>Legged Robots 1</b></a></td>
               <td class="r">Regular session</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#123193" title="Click to go to the Author Index">Lin, Pei-Chun</a></td><td class="r">National Taiwan Univ</td></tr>
<tr><td>Co-Chair: <a href="IROS16_AuthorIndexMedia.html#130410" title="Click to go to the Author Index">Kottege, Navinda</a></td><td class="r">CSIRO</td></tr>
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct10_01">16:55-17:10, Paper TuCT10.1</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0180.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('180'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Proprioceptive Control of an Over-Actuated Hexapod Robot in Unstructured Terrain</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#195453" title="Click to go to the Author Index">Bjelonic, Marko</a></td><td class="r">Tech. Univ. Darmstadt</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#130410" title="Click to go to the Author Index">Kottege, Navinda</a></td><td class="r">CSIRO</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#153085" title="Click to go to the Author Index">Beckerle, Philipp</a></td><td class="r">Tech. Univ. Darmstadt</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab180" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/0180.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Legged_Robots" title="Click to go to the Keyword Index">Legged Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Compliance_and_Impedance_Control" title="Click to go to the Keyword Index">Compliance and Impedance Control</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> Legged robots such as hexapods have the potential to traverse unstructured terrain. This paper introduces a novel hexapod robot (Weaver) using a hierarchical controller, with the ability to efficiently traverse uneven and inclined terrain. The robot has five joints per leg and 30 degrees of freedom overall. The two redundant joints improve the locomotion of the robot by controlling the body pose and the leg orientation with respect to the ground. The impedance controller in Cartesian space reacts to unstructured terrain and thus achieves self-stabilizing behavior without prior profiling of the terrain through exteroceptive sensing. Instead of adding force sensors, the force at the foot tip is calculated by processing the current signals of the actuators. This work experimentally evaluates Weaver with the proposed controller and demonstrates that it can effectively traverse challenging terrains and high gradient slopes, reduce angular movements of the body by more than 55% and reduce the cost of transport (up to 50% on uneven terrain and by 85% on a slope with 20 degrees). The controller also enables Weaver to walk up inclines of up to 30 degrees, and remain statically stable on inclines up to 50 degrees. Furthermore, we present a new metric for legged robot stability performance along with a method for proprioceptive terrain characterization.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct10_02">17:10-17:25, Paper TuCT10.2</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0216.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('216'); return false" title="Click to show or hide the keywords and abstract">Generation of Underactuated Bipedal Gait Completing in One Step</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#103082" title="Click to go to the Author Index">Asano, Fumihiko</a></td><td class="r">Japan Advanced Inst. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#194561" title="Click to go to the Author Index">Zheng, Yanqiu</a></td><td class="r">Japan Advanced Inst. of Science and Tech</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#158883" title="Click to go to the Author Index">Xiao, Xuan</a></td><td class="r">Tsinghua Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab216" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Legged_Robots" title="Click to go to the Keyword Index">Legged Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_Control" title="Click to go to the Keyword Index">Motion Control</a>, <a href="IROS16_KeywordIndexMedia.html#Underactuated_Robots" title="Click to go to the Keyword Index">Underactuated Robots</a></span><br>
                           <strong>Abstract:</strong> This paper proposes a novel method for generating an underactuated bipedal gait that completes in one step. First, we introduce an underactuated biped robot model that has a circular torso as a reaction wheel. Second, we consider an input-output linearization to formulate an output-following control law for achieving collisionless limit cycle walking. We then mathematically analyze the stability of the zero dynamics and investigate the fundamental gait properties through numerical simulations. Furthermore, we discuss an extension to generation of a few-steps walking motion aimed at safe and quick passage from a safety island to the next.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct10_03">17:25-17:40, Paper TuCT10.3</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/0803.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('803'); return false" title="Click to show or hide the keywords and abstract">Model-Based Bounding in a Quadruped Robot with Waist Actuation</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#191869" title="Click to go to the Author Index">Chen, Chung-Li</a></td><td class="r">National Taiwan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#179820" title="Click to go to the Author Index">Wang, Tso-Kang</a></td><td class="r">National Taiwan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#164734" title="Click to go to the Author Index">Hu, Chia-Jui</a></td><td class="r">Department of Mechanical Engineering, National Taiwan Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#123193" title="Click to go to the Author Index">Lin, Pei-Chun</a></td><td class="r">National Taiwan Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab803" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Legged_Robots" title="Click to go to the Keyword Index">Legged Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Dynamics" title="Click to go to the Keyword Index">Dynamics</a>, <a href="IROS16_KeywordIndexMedia.html#Motion_and_Trajectory_Generation" title="Click to go to the Keyword Index">Motion and Trajectory Generation</a></span><br>
                           <strong>Abstract:</strong> We report on the development of a dynamic gait in a robot based on a sagittal-plane and reduced-order model that fits the natural dynamics of the robot. The proposed model, referred to as the two-rolling-legs model with waist actuation (TRLW), consists of two rolling and compliant legs and two rigid bodies connected by an active waist joint. The numerical fixed-point analyses of the model suggest that with adequate touchdown conditions and waist motion, dynamic and periodic bounding and pronking behaviors of the model are achievable. Several identified fixed points of the model, or passive dynamics of the model, serve as the control guidance to initiate bounding and pronking on a newly-developed quadruped robot. The experimental results show that though several un-modeled factors cause behavioral discrepancies between the TRLW model and the robot, the robot can still initiate its dynamic behaviors by directly mapping the leg trajectories of the model onto those of the robot and via a simple pre-set joint position control strategy, without any other state feedback. This work confirms that the dynamic behavior of the robot can be initiated by using the passive dynamics of the reduced-order model.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct10_04">17:40-17:55, Paper TuCT10.4</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1075.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1075'); return false" title="Click to show or hide the keywords and abstract"><img src="images/att.png" style="border: 0; margin: 0px 4px 0px 0px" alt=""></>Task-Based Limb Optimization for Legged Robots</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#180081" title="Click to go to the Author Index">Ha, Sehoon</a></td><td class="r">Disney Res</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#147028" title="Click to go to the Author Index">Coros, Stelian</a></td><td class="r">Disney Res. Zurich</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#140093" title="Click to go to the Author Index">Alspach, Alexander</a></td><td class="r">Disney Res</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#150747" title="Click to go to the Author Index">Kim, Joohyung</a></td><td class="r">Disney Res</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#104339" title="Click to go to the Author Index">Yamane, Katsu</a></td><td class="r">Disney</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1075" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           <strong>Attachments:</strong> <span style=""><a href="./files/1075.VI.mp4" title="Click to open">Video Attachment</a></span><br>

                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Legged_Robots" title="Click to go to the Keyword Index">Legged Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Animation_and_Simulation" title="Click to go to the Keyword Index">Animation and Simulation</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a></span><br>
                           <strong>Abstract:</strong> The design of legged robots is often inspired by animals evolved to excel at different tasks. However, while mimicking morphological features seen in nature can be very powerful, robots may need to perform motor tasks that their living counterparts do not. In the absence of designs that can be mimicked, an alternative is to resort to mathematical models that allow the relationship between a robot's form and function to be explored. In this paper, we propose such a model to co-design the motion and leg configurations of a robot such that a measure of performance is optimized. <p>The framework begins by planning trajectories for a simplified model consisting of the center of mass and feet. The framework then optimizes the length of each leg link while solving for associated full-body motions. Our model was successfully used to find optimized designs for legged robots performing tasks that include jumping, walking, and climbing up a step. Although our results are preliminary and our analysis makes a number of simplifying assumptions, our findings indicate that the cost function, the sumof squared joint torques over the duration of a task, varies substantially as the design parameters change.
                        </div>
                     </td>
                  </tr>
               
<tr style="line-height: 0.2em"><td colspan="2">&nbsp;</td></tr>
<tr class="pHdr"><td valign="bottom"><a name="tuct10_05">17:55-18:10, Paper TuCT10.5</a></td><td class="r">&nbsp;</td></tr>
<tr><td colspan="2"><span class="pTtl"><a href="./files/1227.pdf" title="Click to open the pdf file"><img src="images/pdf.png" height="12" border="0" alt=""></></a><a href="" onclick="viewAbstract('1227'); return false" title="Click to show or hide the keywords and abstract">Adaptive Locomotion by Two Types of Legged Robots with an Actuator Network System</a></span></td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#177897" title="Click to go to the Author Index">Ryu, Hideyuki</a></td><td class="r">Osaka Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#146524" title="Click to go to the Author Index">Nakata, Yoshihiro</a></td><td class="r">Graduate School of Engineering Science, Osaka Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#117967" title="Click to go to the Author Index">Nakamura, Yutaka</a></td><td class="r">Osaka Univ</td></tr>
<tr><td><a href="IROS16_AuthorIndexMedia.html#101596" title="Click to go to the Author Index">Ishiguro, Hiroshi</a></td><td class="r">Osaka Univ</td></tr>

                  <tr>
                     <td colspan="2" style="padding: 0px">
                        <div id="Ab1227" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
                           
                           <span style="line-height: 2em"><strong>Keywords:</strong> <a href="IROS16_KeywordIndexMedia.html#Legged_Robots" title="Click to go to the Keyword Index">Legged Robots</a>, <a href="IROS16_KeywordIndexMedia.html#Mechanism_Design" title="Click to go to the Keyword Index">Mechanism Design</a>, <a href="IROS16_KeywordIndexMedia.html#Biologically_Inspired_Robots" title="Click to go to the Keyword Index">Biologically-Inspired Robots</a></span><br>
                           <strong>Abstract:</strong> Locomotion on the rough and variable ground surfaces is crucial for robots to complete various tasks. Recent advancement in numerical computation allow such locomotive robots to manipulate in real environments using a model-based control framework. This approach is successful if the precise model it is obtained. However, it is not always feasible to use the precise model because there are various factors related to both the robot and its surroundings. In this paper, we report the locomotion experiments with 2 types of legged robot that can change its behavior by switching connection patterns among hydraulic cylinders mounted on the robot’s legs. An actuator network system (ANS) is used to switch mutual interconnection of cylinders. The switching connection allows the robots to realize not only adaptive locomotion to the environment but also operation of the traveling direction.
                        </div>
                     </td>
                  </tr>
               
</table><table class="trk">

             <tr><td colspan="2">&nbsp;</td></tr>
              <tr class="sHdr">
               <td><a name="tuft11"><b>TuFT11</b></a></td>
               <td class="r">#301</td>
              </tr>
             
             <tr class="sHdr">
               <td nowrap><a href="IROS16_ProgramAtAGlanceMedia.html#tuft11" title="Click to go to the Program at a Glance"><b>Futurist Forum</b></a></td>
               <td class="r">Forum</td>
             </tr>
            
<tr><td>Chair: <a href="IROS16_AuthorIndexMedia.html#178414" title="Click to go to the Author Index">Boesl, Dominik B. O.</a></td><td class="r">Tech. Univ. München</td></tr>

</table>
</div>

<p>&nbsp;<br>&nbsp;</p><p>&nbsp;<br>&nbsp;</p>


</div>

</body>

</html>
